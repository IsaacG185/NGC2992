\function{add_color_palette}
\synopsis{Add a color palette}
\usage{add_color_palette (String_Type name, UInt_Type[] colors);
\altusage{add_color_palette (String_Type name, Ref_Type generator);}}
\description
  Add an array of hex values representing a color palette to the
  system.

  Alternatively, a function can be passed with this signature
  \code{UInt_Type[] palette = generator(UInt_Type n);}
  where \code{n} is the number of colors returned. It is okay to
  return fewer colors than \code{n}, but never more.
\seealso{get_color_palette}
\done

\function{add_hist}
\synopsis{add histograms}
\usage{Struct_Type specs = add_hist([hist1,hist2,...]);}
\usage{Struct_Type specs = add_hist(hist1,hist2,...);}
\description
    Add histogram structures with fields bin_lo, bin_hi, value, and
    err. The function just assumes that the histograms are on the
    same grid. It copies the grid from the very first structure in
    the array, then adds the values of the subsequent structures.
    Uncertainties (err) are added in quadrature. 
\seealso{scale_hist, shift_hist, stretch_hist}
\done

\function{add_plot_unit}
\synopsis{Create a new X-unit,Y-unit pair for isis_fancy_plots package}
\usage{add_plot_unit(String_Type, String_Type [; ]);}
\qualifiers{
\qualifier{SEE BELOW}{}
}
\description

 add_plot_unit(xunit, yunit; xscale=val, yscale=val, is_energy=val,
                             xlabel=str, ylabel=str, pgfont=str);

 Create a new X-unit and Y-unit pair for plot_counts, plot_data, plot_unfold,
 and plot_fit_model.  (yunit and yscale only affect plot_unfold; plot_counts
 and plot_data are only sensitive to the choice of xunit.) Set is_energy=1 if
 the xunit scales as keV (otherwise it defaults to scaling as Angstroms). xscale
 is the scaling of keV/xunit or A/xunit.  yscale scales *all* of the y-axes in
 the plot_unfold plots, so it must be set appropriately to achieve a desired
 effect.  xunit and yunit will be treated as lower case regardless of input.
 Note that existing X-units will not be overwritten; their already defined
 values will be used regardless of input.  A set of default axis labels will
 be produced, with "ylabel" substituting for:

       "Photons cm^{-2} s^{-1}"   (the default)

 However, these labels can be rewritten using the new_plot_labels command.

 To get the scalings correct, remember that plot_unfold(...;power=2) is always
 photons/s/cm^2 in the absence of scalings, power=0 is proportional to flux
 for wavelength based X-units, while power=3 is proportional to flux for energy
 based units.  For these latter two cases, the yscale needs to account for
 divisions/multiplications of the scaled X-unit.

 Examples:

 To produce mJy vs. THz plots, use:

    add_plot_unit("thz", "mjy"; xscale=Const_eV/Const_h*1.e12,
                  yscale=Const_h*1.e26, xlabel="THz", ylabel="mJy/THz");

 Const_eV/Const_h*1.e12 ~ 4.14e-3 is keV per Terra-Hz
 plot_unfold(...;power=2) produces photons/s/cm^2 =>
    (photons*xunit)/s/cm^2/xunit  (xunit scaling cancels out)

 We wish to convert this to mJy:

    (photons*xunit/s/cm^2/xunit) =
    [photons*(1.e-26 ergs)*Hz/(1.e-26 ergs)]/s/cm^2/Hz =
    Hz/(1.e-26 ergs)*mJy

 hence multiplying by yscale=(1.e-26 ergs)/Hz = 1.e26*Const_h
 produces the desired scaling for plot_unfold.

 To produce BTU/Acre/s vs. keV plots, use:

 plot_unfold(...;power=3) for X-unit=keV produces keV/s/cm^2,
 which we wish to convert to BTU/Acre/s, hence xscale=1 and:

    erg_p_btu = 1.055056e10                    % ISO ergs/BTU
    cmsq_p_a = 4.0468564224e7                  % cm^2 per international acre
    yscale = cmsq_p_a*Const_eV*1.e3/erg_p_btu  % cm^2/Acre*BTU/keV

 To achieve this same Y-unit vs. other energy based X-units,
 multiply yscale by keV/xunit, i.e., the value of xscale.
\seealso{fancy_plot_unit, set_plot_labels, new_plot_labels}
\done

\function{ad_create_colormap}
\synopsis{creates a redshift-colormap, where yellow is exactly at
where(img==1)}
\usage{g_br = ad_create_colormap(img,alp,gmax);}
\done

\function{ad_init_raw}
\synopsis{subroutine, which reads the data and the header of a
      raw-redshift image (used by ad_make_image) }
\usage{(data,nre,ng,a,theta0grad) = ad_init_raw(filename);}
\done

\function{ad_init_raw_timing}
\synopsis{subroutine, which reads the data and the header of a
      raw-redshift image FOR TIMING (used by ad_make_image) }
\usage{(data,nre,ng,a,theta0grad) = ad_init_raw_timing(filename);}
\done

\function{ad_interpol_image}
\synopsis{interpolates the image maximal sz_max times beyond rmin}
\usage{image = ad_interpol_image(image,alp_lo,bet_lo,sz_max,rmin);}
\seealso{ad_make_image, ad_init_raw, ad_read_image}
\done

\function{ad_make_image}
\synopsis{creates a redshift image from the raw data by sorting with
respect to alpha and beta}
\usage{(img, alp_lo, alp_hi, bet_lo, bet_hi) = ad_make_image(filename);}
\qualifiers{
\qualifier{sho_minmax}{show the extrem values of alpha and beta?}
\qualifier{timing}{read a Brod-Timing-FITS-Table}
\qualifier{field}{specify which values are plotted}
\qualifier{disk}{kuse intrinsic x and y instead of alpha and beta (only working for timing yet)}
}
\done

\function{ad_read_image}
\synopsis{reads the image and the alpha-beta-grid from the FITS-file
created by ad_make_image}
\usage{ad_read_image(filename);}
\seealso{ad_make_image, ad_init_raw}
\done

\function{ad_write_image}
\synopsis{subroutine, which reads the data and the header of a
      raw-redshift image (ad_make_image) and writes it into a
      fits-image with the filename "image_*"}
\usage{(img,alp,bet) = ad_write_image(filename,size [alp_extr,bet_extr]);}
\done

\function{aglc}
\synopsis{Bin a light curve from ACIS grating events.}
\usage{lc = aglc(tgevt, expno_ref, tbin, wmin, wmax, tg, orders[, bkg]);}
\altusage{lc = aglc(   evt_struct,    tbin, wmin, wmax, tg, orders[, bkg]);}

\description
    Given an event file name (\code{tgevt}) and an exposure reference file
    (unfiltered events or ``stat1'') name (\code{expno_ref}), bin a light curve
    for the specified time bin in seconds (\code{tbin}), wavelength ranges
    specified by \code{wmin}, \code{wmax}, in Angstroms.  If \code{wmin} and \code{wmax} are arrays,
    then they must be the same length and represent low-high pairs of
    wavelength regions.  The argument, \code{tg}, is a scalar or array of
    grating names, and should be one or more of \code{"HEG"} and \code{"MEG"}, or \code{"LEG"}
    (case-insensitive); only the first character is necessary.  The
    argument \code{orders} should be an array of integers specifying the
    diffraction orders to bin (excluding zero).  If the last argument is
    present, then events are binned from the background region instead of
    the source region.

    In the second form, an event structure as returned by
    \code{aglc_read_events} is used instead of two file names.  This is
    more efficient for multiple calls, since it avoids multiple file
    reads.

    The two files required in the first form are the grating coordinates
    event file, probably filtered of bad events; i.e., typically the file
    from which a spectrum would be binned ("Level 1.5" or "Level 2").
    The second file, the exposure reference file, should be unfiltered
    events or the ``stat1'' file.  It is used to count the exposed frames
    to determine the exposure; any event - cosmic ray, bad pixel, photon
    - will suffice to mark a frame.  Use of the ``stat1'' file is more
    efficient, since it has only one entry for each unique frame.

    Standard binning regions are applied for source and background
    events.   They may be changed with \code{aglc_set_regs}.

    The return value is a structure of the form\n
       \code{time = Double_Type[]}        % event time, bin center, since timezero.\n
       \code{time_min = Double_Type[]}    % lower edge of time bin, seconds since timezero.\n
       \code{time_max = Double_Type[]}    % upper edge of time bin, seconds since timezero.\n
       \code{counts = UInteger_Type[]}    % counts per time bin.\n
       \code{count_rate = Double_Type[]}  % Counts per second ( counts / exposure ).\n
       \code{stat_err = Double_Type[]}    % Statistical error ( sqrt(counts) ).\n
       \code{exposure = Double_Type[]}    % Exposure per bin, averaged over all chips included, in seconds.\n
       \code{timezero = Double_Type}      % reference time, in seconds since MJDREF.\n
       \code{mjdref = Integer_Type}       % Modified Julian Day reference point of Chandra data (from event header)\n
       \code{revidx1a = Array_Type[]}     % reverse index array for the grating events.\n
       \code{revidx = Array_Type[]}       % reverse index array for the exposure reference events.\n
    The times are in seconds since timezero.  Timezero is the number of
    seconds since 1998.0, which is also the reference MJD.

    The \code{count_rate} is per second, and is equivalent to counts/exposure.
    The exposure is computed properly for event selections spanning
    multiple CCDs with possibly different GTI tables.

    The reverse indices, \code{revidx1a}, are returned by the \code{histogram}
    function; for each bin of the light curve they point to
    the items in the event list which have been binned.  This facilitates
    subsequent operations on events in specific bins, especially when
    selected on non-time criteria. (see \code{aglc_filter}).

    \code{revidx} is like \code{revidx1a}, but for the exposure record event list.
\example
     Compute the light curve for the sum of Ne X 12A and O VIII 19A:\n
     \code{wlo = [12.1, 18.8];}\n
     \code{whi = [12.2, 19.1];}\n
     \code{g = ["H", "M"];}\n
     \code{o = [-1,1];}\n
     \code{c = aglc( "evt2.fits", "evt1.fits", 1000, wlo, whi, g, o );}\n
     \code{hplot(c.time_min, c.time_max, c.counts);}
\seealso{aglc_read_events, aglc_filter, aglc_set_regs}
\done

\function{aglc_get_ephem}
\synopsis{Retrieve a stored ephemeris, the last one used.}
\usage{(jd0, pd) = aglc_get_ephem;}
\description
    jd0 will be the Julian day of zero phase.
    pd  will be the period in days.
\seealso{aglc_pr_ephem, aglc_phased}
\done

\function{aglc_get_regs}
\synopsis{Retrieve stored source and background regions
    (cross-dispersion limits), and associated BACKSCAL values.}
\usage{r = aglc_get_regs;}
\description
    Retrieves a structure containing the stored values used for binning
    light curves for source or background.
\seealso{aglc_set_regs, aglc_reset_regs}
\done

\function{aglc_get_trange}
\synopsis{Retrieve the time filter which will be applied in phase-binning.}
\usage{(tmin, tmax) = aglc_get_trange;}
\description
    tmin,tmax are relative times in seconds, since the start of the
    observation.
\seealso{aglc_set_trange, aglc_pr_trange;}
\done

\function{aglc_jd_to_rotnum}
\synopsis{Convert Julian days to rotation number.}
\usage{rotation_number = aglc_jd_to_rotnum( t_jd, hjd_epoch, period_days );}
\description
     \code{rotation_number} = number of full cycles plus fractional phase, given an ephemeris.\n
     \code{t_jd}        = times, in Julian day numbers.\n
     \code{hjd_epoch}   = epoch of zero-phase, in HJD\n
     \code{period_days} = period, in days.
\seealso{aglc_tobs_to_jd, aglc_jd_to_phase, aglc_tobs_to_phase, aglc_phased, aglc}
\done

\function{aglc_phased}
\synopsis{Bin a phased light curve.}
\usage{pc = aglc_phased(tgevt, expno_ref, phase_info, ephem, wmin, wmax, tg, orders[, bkg]);
\altusage{pc = aglc_phased(   evt_struct   , phase_info, ephem, wmin, wmax, tg, orders[, bkg]);}
}
\description
    Arguments are similar to those in \code{aglc}, with the exception of
    \code{phase_info} and \code{ephem}.  For other arguments, see \code{aglc}.

    \code{phase_info}  = a 3-element array giving the phase minimum, maximum,
                  and bin size desired for the resultant phased light
                  curve.
    \code{ephem} = a two-element array giving the JD of zero phase (NOT MJD!)
            and the period in days.
 
    Definitions for Phase Curve Structure Fields
    The phase curve fields are the same as for the light curve, with the exception
    of phase replacing time and the addition of the ephemeris. The new fields are:\n
    1. \code{phase}: Value of phase at the bin center.\n
    2. \code{phase_min}: Value of phase at the low edge of the bin.\n
    3. \code{phase_max}: Value of phase at the high edge of the bin.\n
    4. \code{epoch}: Epoch of the ephemeris, in Julian Days (NOT MJD!).\n
    5. period: Period of the ephemeris, in days.
\seealso{aglc, aglc_filter, aglc_read_events}
\done

\function{aglc_pr_ephem}
\synopsis{Print the stored ephemeris used in the last phase-curve binning.}
\usage{aglc_pr_ephem}
\description
    Retrieves the ephemeris stored by aglc_phased and prints to terminal.
\seealso{aglc_get_ephem, aglc_phased}
\done

\function{aglc_pr_trange}
\synopsis{Print to the terminal the current time filter.}
\usage{aglc_pr_trange;}
\description
    Prints the min and max time filter used in binning phase curves.
\seealso{aglc_set_trange, aglc_get_trange;}
\done

\function{aglc_read_events}
\synopsis{Read subset of events from grating file and from exposure file into a structure.}
\usage{Struct_Type s = aglc_read_events(String_Type tgevt, expno_ref);}
\description
    \code{tgevt} should be a Level 1.5 (or 2) grating events file name.
    \code{expno_ref} should be a Level 1 ``stat'' or events (unfiltered) file name.
    The returned value is a structure.  The detailed contents of the
    structure are given below in the example.  While the exposure can
    be computed from the EXPNO column of the unfiltered event file,
    use of the `stat1' file is more efficient.

    Definitions for Event Structure Fields:\n
    - \code{fevt_1a}: File name for grating events.\n
    - \code{fevt_exp}: File name for exposure reference (unfiltered events or ``stat1'' file)\n
    - \code{expno[]}: Exposure number column from exposure reference file.\n
    - \code{ccd}: CCD_ID column from exposure reference file.\n
    - \code{expno_1a }: Exposure number column from grating event file.\n
    - \code{ccd_1a}: CCD_ID column from grating event file.\n
    - \code{tgpart}: TG_PART column from grating event file. (value is 1 for HEG, 2 for MEG, 0 for zero order, 99 for background)\n
    - \code{order}: TG_M column from grating events file, giving the diffraction order.\n
    - \code{tgd}: TG_D column from grating events file. This is the cross-dispersion coordinate in degrees.\n
    - \code{wave}: TG_LAM column from grating events file, giving the wavelength of each event.\n
    - \code{timedel}: TIMEDEL keyword from grating event file, needed for scaling exposure numbers to time. (See the ahelp on time.)\n
    - \code{timepixr}: TIMEPIXR keyword from event file. (See the ahelp on time.)\n
    - \code{frame_exp}: the exposure time per frame (EXPNO). (See the ahelp on time.)\n
    - \code{mjdref}: Modified Julian Day reference time for Chandra observations, from the event file header.\n
    - \code{tstart}: TSTART, from the event file header, is the observaton start time in seconds since MJDREF.\n
    - \code{status}: Event status column, from the grating events file. (See the definition of ACIS status bits.)\n
    - \code{time}: The TIME column from the exposure reference event file.\n
    - \code{cycle}: unused.\n
\seealso{aglc_filter, aglc, aglc_phased}
\done

\function{aglc_set_regs}
\synopsis{Set the spatial binning regions (cross-dispersion) for the source
    and backgrounds on both sides, and compute the background scaling factors.}
\usage{aglc_set_regs( s_min, s_max, b_lo_min, b_lo_max, b_hi_min, b_hi_max );}
\description
   Limits are given in cross-dispersion coordinates, tg_d, in units of degrees.\n
   \code{s_min} = source minumum tg_d.\n
   The center of the source region is defined as tg_d = 0.\n
   \code{s_max} = source region maximum tg_d.\n
   \code{b_lo_min} = background minimum, on the "low" side (tg_d < 0).\n
   \code{b_lo_max} = background maxmimum, on the "low" side (tg_d < 0).\n
   \code{b_hi_min} = background minimum, on the "high" side (tg_d < 0).\n
   \code{b_hi_max} = background maxmimum, on the "high" side (tg_d < 0).

   The limits follow the constraint that
      \code{b_lo_min < b_lo_max <= s_min < s_max <= b_hi_min < b_hi_max}.

   Default values are the same as the default spectral extraction
   regions of tgextract:\n
      \code{s_min    = -6.6e-04}  [ deg ]\n
      \code{s_max    =  6.6e-04}  [ deg ]\n
      \code{b_lo_min = -6.0e-03}  [ deg ]\n
      \code{b_lo_max =  s_min}    [ deg ]\n
      \code{b_hi_min =  s_max}    [ deg ]\n
      \code{b_hi_max =  6.0e-03}  [ deg ]\n
\seealso{aglc_reset_regs, aglc_get_regs}
\done

\function{aglc_set_status_filter}
\synopsis{Set a flag to specify if the grating events should be filtered on STATUS=0.}
\usage{aglc_set_status_filter( flag );}
\description
    By default, the status filter is zero, meaning all good
    events.  Different status bit fields qualify attributes of "bad"
    pixels.  If, for some reason, the grating events have not been
    filtered on status, but should be, then setting this flag causes
    all events with non-zero status to be ignored.

    If flag=1, then ignore events with non-zero status.
    If flag=0, then accept all events.
\seealso{aglc, aglc_phased}
\done

\function{aglc_set_trange}
\synopsis{Set a simple time range filter to be applied to a light curve
    before extracting a phase-binned curve. This is useful for
    excluding flares, which seem to predominantly occur at the
    beginning or end of an observation.}
\usage{aglc_set_trange( tmin, tmax );}
\description
    \code{tmin, tmax} are relative times in seconds, since the start of the
    observation.  The interval specifies the times of interest to be included
    in phase binning. \code{tmax==NULL} means to the end of the observation.
\example
    \code{aglc_set_trange( 0, 2000 );   %} first 2ks will be included in phase binning.\n
     \code{aglc_set_trange( 5000, NULL);  %} from 5ks to the end will be selected.
\seealso{aglc_get_trange, aglc_pr_trange}
\done

\function{aglc_tobs_to_jd}
\synopsis{Convert Chandra obervation times, given in seconds since MJDREF, to Julian day.}
\usage{jd = aglc_tobs_to_jd( t_obs, mjdref );}
\description
    \code{t_obs} is a scalar or array of Chandra times in seconds since \code{mjdref}.
    \code{mjdref} is the Modified Julian Day reference, found in Chandra
    headers, or in the aglc event structure.
\seealso{aglc_jd_to_rotnum, aglc_jd_to_phase, aglc_tobs_to_phase, aglc_read_events}
\done

\function{aglc_write_curve}
\synopsis{Write a light or phase curve to a FITS bintable file}
\usage{aglc_write_curve( outfile, hdr_ref_file, curve_struct [, history_string ] );" );}
\description
    \code{outfile} = name of output file ( \code{String_Type}).\n
    \code{hdr_ref_file} =  Reference file for copying of header info ( \code{String_Type}).\n
    \code{curve_struct} =  The light or phase curve structure, as created by \code{aglc} or  \code{aglc_phased}.\n
    \code{history_string} = arbitrary note (\code{String_Type}).
\seealso{aglc, aglc_phased}
\done

\function{air_to_vacuum}
\synopsis{Convert air wavelengths to vacuum wavelengths}
\usage{Double_Type l_v[] = air_to_vacuum(Double_Type l_a[])}
\description
    Convert air wavelengths l_a (in Angstroem) to vacuum wavelengths l_v (in Angstroem)
    for dry air at 15 degrees Celsius, 101.325 Pa, and with 450 parts per million CO2
    content. Valid in the wavelength range between 2300 to 17000 Angstroem.
\notes
    According to Equation 1 in Ciddor 1996, Applied Optics, 35, 1566:
       1e8*(l_v-l_a)/l_a = n-1 = k1/(k0-l_v^(-2)) + k3/(k2-l_v^(-2))
    => l_a = l_v / ( k1*1e-8/(k0-l_v^(-2)) + k3*1e-8/(k2-l_v^(-2)) + 1 )
       with k0 = 238.0185 * 1e-8 / Angstroem^2,
            k1 = 5792105  * 1e-8 / Angstroem^2,
            k2 = 57.362   * 1e-8 / Angstroem^2,
            k3 = 167917   * 1e-8 / Angstroem^2.
    This function is not analytically solvable for l_v. To zeroth order,
    however, l_v ~ l_a so that
       l_v ~ l_a * ( k1*1e-8/(k0-1/l_a^2) + k3*1e-8/(k2-1/l_a^2) + 1 ).
\qualifiers{
\qualifier{verbose [=1]}{: Set to 0 to suppress warnings.}
}
\example
    l_v = [2000,4000,8000,16000];
    print(l_v - air_to_vacuum(vacuum_to_air(l_v)));
\seealso{vacuum_to_air}
\done

\function{Aitoff_projection}
\synopsis{computes an Aitoff projection}
\usage{(Double_Type x, y) = Aitoff_projection(Double_Type l, b);}
\qualifiers{
\qualifier{deg}{\code{l} and \code{b} are in degrees, not in radian}
\qualifier{normalized}{\code{x} and \code{y} are normalized (by \code{PI/2})
                 such that \code{abs(x) <= 2} and \code{abs(y) <= 1}.}
}
\description
    NOTE: for astronomical purposes you probably want to use the
    equal area Hammer-Aitoff projection rather than an Aitoff-projection.
    Erroneously many astronomers call the Hammer-Aitoff projection an
    Aitoff projection

    \code{x = 2 * cos(b) * sin(l/2) / sinc(alpha);}\n
    \code{y = sin(b) / sinc(alpha);}\n
    where
      \code{cos(alpha) = cos(b) * cos(l/2)}\n
    and
      \code{sinc(alpha) = sin(alpha)/alpha}
\seealso{Hammer_projection, Lambert_Equal_Area_projection}
\done

\function{alpha_ff}
\synopsis{Calculate the absorption coefficient for free-free absorption}
\usage{Double_Type alpha = alpha_ff(nu,T);}

\qualifiers{
\qualifier{Z}{average nuclear charge (default=1)}
\qualifier{ne}{electron particle density (cm^-3; default: 1e10)}
\qualifier{np}{proton particle density (cm^-3; default: 1e10)}
}
\description
    This function returns the absorption coefficient for free-free radiation
    (bremsstrahlung). The frequency, nu, is measured in Hz and can be an array,
    the temperature T is measured in K.

    For the moment this function assumes the Gauntt factor equals unity.

    Note: per Kirchhoff's law the total bremsstrahlung spectrum from
    a slab of size R is
    B_nu(nu,T)*(1.-exp(-R*alpha_ff(nu,T)))

\seealso{j_ff}
\done

\function{angle2string}
\synopsis{convert an angle to a string for pretty printing}
\usage{string=angle2string(angle;qualifiers)}
\qualifiers{
   \qualifier{hoursign}{angle is a hour angle, always include sign}
   \qualifier{declination}{angle is a declination, always include sign}
   \qualifier{latitude}{angle is a latitude, always include sign}
   \qualifier{hours}{display hours, not degrees}
   \qualifier{deg}{input angle is in degrees (default: radians)}
   \qualifier{separator}{string to insert between the degrees/hours,
        minutes, and after the seconds. If a scalar: insert between
        degrees and minutes only [e.g., ":"]. If an array: insert
        the three array elements between the output numbers (e.g.,
        ["h","m","s"]). Default is ["d","m","s"] and ["h","m","s"]
        depending on whether the hours-qualifier is set or not.}
   \qualifier{blank}{do not display leading zeros}
   \qualifier{secfmt}{sprintf format for the seconds (default:
           %04.1f, i.e., a precision of 0.1 sec)}
   \qualifier{mas}{display to a precision of milliseconds
        corresponds to secfmt="%06.3f"}
   \qualifier{muas}{display to a precision of microseconds
        corresponds to secfmt="%08.5f"}
}
\description
  This function is used to produce well formatted strings out of
  angular quantities which are given in radians or degrees.
  (the default is radian).

  Angles can be displayed either in degrees or in hours, 
  various separators can be used to separate the hms-terms,
  and the routine can be told to always display a sign (e.g.,
  for declination- or latitude-like quantities), or not.

  This routine is array safe.

\seealso{hms2deg,dms2deg,generate_iauname}
\done

\function{angle_to_rad}
\synopsis{converts an angle in degrees or h:m:s format into radian}
\usage{Double_Type angle_to_rad(Double_Type x);}
\qualifiers{
\qualifier{unit}{[\code{="deg"}] unit of \code{x} (\code{"deg"}/\code{"hms"}/\code{"rad"})}
}
\description
    \code{x} can be a scalar value or (unless \code{unit="rad"}) an array of the form\n
    - \code{[deg, arcmin]} (respectively \code{[hour, min]} for \code{unit="hms"})\n
    - \code{[deg, arcmin, arcsec]} (respectively \code{[hour, min, sec]} for \code{unit="hms"})\n
    - \code{[sign, deg, arcmin, arcsec]} (respectively \code{[sign, hour, min, sec]}).\n
    The latter representation is needed for \code{-1 < deg}/\code{hour < 0}.
\seealso{hms2deg,dms2deg}
\done

\function{angular_separation}
\usage{ sep=angular_separation(ra1,dec1,ra2,dec2);}
\synopsis{calculates the angular distance between two points on a sphere}
\qualifiers{
\qualifier{deg}{if set, the input coordinates and output are in degrees (default: radian)}
\qualifier{radian}{if set, the input coordinates and output are in radian (the default))}
}
\description
 This routine calculates the angular separation between points on the sky
 with coordinates ra1/dec1 and ra2/dec2.

 The function is equivalent to greatcircle_distance but is compatible in terms
 of its qualifiers with the other coordinate system routines. It also returns
 the angular separation in the units of the input parameters rather than
 always in rad and, by using the haversine formula, is less prone to round
 off errors for small angular separations.

 This function is array safe (for either ra1/dec1 or ra2/dec2).

\seealso{hms2deg,dms2deg,angle2string,position_angle}
\done

\function{ansi_escape_code}
\synopsis{transforms a text format, e.g., a color into its corresponding ANSI code}
\usage{String_Type ansi_escape_code(String_Type format[, String_Type text]);}
\qualifiers{
  \qualifier{list}{print a list of available keywords}
}
\description
  Transforms the given (human readable) format string into
  a sequence of ANSI escape codes. The format string consists
  of keywords, e.g., colors separated by semicolons. A list
  of supported keywords is printed setting the 'list'
  qualifiers.

  The cursor movement keywords 'up', 'down', 'forwards', and
  'back' allow an optional preceding number, which specifies
  the amount of the movement.

  In order to reset a previous format you may use the 'reset'
  keyword. In case an optional text is provided, this text
  and the reset ANSI code is appended to the returned string
  automatically.
\example
  ansi_escape_code("red;blink", "ALERT");
  ansi_escape_code("3up"); % move three lines up
  ansi_escape_code("blue"); % blue color *from now on*
  ansi_escape_code("reset"); % turn off the format again
\done

\function{apj_size}
\synopsis{Set the pgplot output size to something suitable for ApJ single column (isis_fancy_plots package)}
\usage{apj_size;}
\description

   Use as:
   isis> id = open_print("fig1.ps/vcps"); apj_size; nice_width;
   isis> plot(x,y);
   isis> close_print(id,"gv");
\seealso{sov, keynote_size, nice_width, open_print, close_print, pg_color, pg_info}
\done

\function{append_chain}
\synopsis{Append a chain fits file to another chain fits file}
\usage{append_chain(String_Type chainfile1, String_Type chainfile2, String_Type chainoutfile)}
\qualifiers{
\qualifier{verbose}{show progress}
}
\description
       This function takes two chains stored in chainfile1 and chainfile2 written with the write_chain function
       (or by emcee itself) and appends the second one to the first one, again writing a fits file.
       This function does not make use of read_chain or write_chain, it's consisting only of
       fits routines. The function itself does perform some sanity and safety checks (same data, same fit_fun)
       but you have to make sure that you use the correct chains in the correct order.
\seealso{combine_chain, read_chain, write_chain, emcee}
\done

\function{append_struct_arrays}
\synopsis{appends a structure's fields to another structures's fields}
\usage{append_struct_arrays(Struct_Type &s, Struct_Type additional_s[]);}
\description
    \code{s} and \code{additional_s} have to be structures with the same fields.
    \code{s}, which is changed by this function, has to be passed by reference.
    For every \code{field},\n
       \code{s.field = [s.field, additional_s.field];}\n
    The following two statements are equivalent:\n
       \code{append_struct_arrays(&s, additional_s);}\n
       \code{s = merge_struct_arrays( [s, additional_s] );}
\seealso{merge_struct_arrays}
\done

\function{aproposer}
\synopsis{recall object names and the documentation satisfying a regular expression}
\usage{aproposer("s")}
\description
  This is an extended version of S-Lang's 'apropos' function,
  which may be used to get a list of all defined objects
  whose name matches the regular expression "s". In addition,
  the SYNOPSIS of all functions described in the documentation
  files is checked on matches as well. Finally, the output is
  formatted such that the matching substring and the object's
  name are printed in different colors (see below for format
  options).

  In order to use this function instead of the intrinsic 'apropos'
  function, you can put
    alias("aproposer", "apropos");
  into, e.g., your .isisrc file.

  Further variables can be defined within the .isisrc file for
  more control options:
    APROPOS_ENABLE_SYNOPSIS - 0 = turn off the synopsis search
    APROPOS_FORMAT_NAME     - format string for the object's name
    APROPOS_FORMAT_MATCH    - format string for the substring match
    APROPOS_LINE_WIDTH      - number of columns of the terminal,
        which is needed to format the output (default: output of
        `stty size` or 80 if the first attempt fails)
  Read the documentation of 'ansi_escape_code' for details about
  the format string.
\seealso{apropos, .apropos, help, who,
    get_doc_files, ansi_escape_code}
\done

\function{ARFconstruct}
\synopsis{Constructs an ARF in keV and cm^2-units from mirror area, filters, filter supports, detector material and additional factor information.}
\usage{variable arf=ARFconstruct(e_lo, e_hi, mirrorareafile, filterfiles, supportfiles, open_fraction, detectorlayerfiles, factor);}
\description
    An ARF is built from a mirror area, filter-,
    support grid-, detector layer transmission
    information. The transmission files-parameters
    have to be file names of transmission tables 
    in the format:

    TEXT
    TEXT
    energy[0] [ev] transmission[0]
    energy[1] [ev] transmission[1]
    ...             ...

    These arrays are interpolated to the input energy 
    grid (e_lo, e_hi).
    The open_fraction is an array with the same lenght
    as the number of filter support files and gives the
    fraction of the support grid that is open for X-rays.
    The mirror area file has to be in the format:

    TEXT
    energy[0] [kev] area[0] [cm^2]
    energy[1] [kev] area[1] [cm^2]
    ...             ...

    The factor is an additional factor multiplied to the
    ARF.
    
    The return value is a structure
    
    arf=struct{
           energ_lo=e_lo,
           energ_hi=e_hi,
           specresp
           };


    To construct an ARF without mirrors included, it can be done by
    setting mirrorareafile=NULL
\done

\function{ARFfromMirrorArea}
\synopsis{Sets the ARF values to the interpolated values of the given mirror area array.}
\usage{ARFfromMirrorArea(arf, mirrorarea);}
\seealso{ARFreadMirrorArea}
\done

\function{ARFmultiplyDetQE}
\synopsis{Multiplies the ARF values with the total quantum efficiency, corresponding to the total transmission of the given detector layers.}
\usage{ARFmultiplyDetQE(arf, detectorLayers);}
\seealso{ARFfromMirrorArea, ARFmultiplyFilter, ARFmultiplySupport, ARFmultiplyFactor}
\done

\function{ARFmultiplyFactor}
\synopsis{Multiplies the ARF values with the given factor.}
\usage{ARFmultiplyFactor(arf, factor);}
\seealso{ARFfromMirrorArea, ARFmultiplyFilter, ARFmultiplySupport, ARFmultiplyDetQE}
\done

\function{ARFmultiplyFilter}
\synopsis{Multiplies the ARF values with the interpolated transmission of the given filter.}
\usage{ARFmultiplyFilter(arf, filter);}
\seealso{ARFfromMirrorArea, ARFmultiplySupport, ARFmultiplyFactor, ARFmultiplyDetQE}
\done

\function{ARFmultiplySupport}
\synopsis{Multiplies the ARF values with the interpolated transmission of the given support material, taking the open fraction into account.}
\usage{ARFmultiplySupport(arf, support, openFraction);}
\seealso{ARFfromMirrorArea, ARFmultiplyFilter, ARFmultiplyFactor, ARFmultiplyDetQE}
\done

\function{ARFreadHenkeTransm}
\synopsis{Reads a standard henke table ASCII-file and returns it as a structure. Energy (first column) must be given in eV and is converted into keV. Transmission (second column) between [0.;1.]. First 2 rows are assumed to be comments.}
\usage{variable henketable=ARFreadHenkeTransm(henkename);}
\seealso{ARFmultiplyFilter, ARFmultiplySupport, ARFmultiplyFactor, ARFmultiplyDetQE, ARFreadMirrorArea}
\done

\function{ARFreadMirrorArea}
\synopsis{Reads the mirror area ASCII-file and returns it as a structure.}
\usage{variable mirrorarea=ARFreadMirrorArea(mirrorname);}
\description
    
    The mirror area file has to be in the format:

    TEXT
    energy[0] [kev] area[0] [cm^2]
    energy[1] [kev] area[1] [cm^2]
\seealso{ARFfromMirrorArea}
\done

\function{array2image}
\synopsis{Converts three x, y, z arrays into a 2D array}
\usage{image[,] = array2image(x,y,z)}
\qualifiers{
\qualifier{nbinx}{number of bins along x-axis}
\qualifier{nbiny}{number of bins along y-axis}
\qualifier{xgrid}{1D array containing grid of values for x-axis. Ignores nbinx.}
\qualifier{ygrid}{1D array containing grid of values for y-axis. Ignores nbiny.}
\qualifier{func}{reference to a single-parameter S-Lang function used to determine pixel values from z-values (see below).}
\qualifier{func_quals}{struct containing additional qualifiers to be passed to the function defined in func}
\qualifier{include_inf}{include points where x, y, or z are inf or -inf}
\qualifier{include_nan}{Note that this means the returned 2D array is of the form
   im[y,x]. By default this ignores any points with infinite or NaN values
   (this can be turned off with the include_inf and include_nan switches).}
}
\description
   The x-y grid is determined automatically from the ranges of the data, or
   can be defined by the "xgrid" and "ygrid" qualifiers. If "xgrid" or "ygrid"
   is a reference to a local variable, the automatically-determined grids will
   be stored in these variables. This function uses histogram2d() to bin the
   data and figure out which points fall into which bins, so see that
   function's documentation for how the grids are handled.

   Notes regarding coordinate grids: 
   * histogram2d() uses the last bin as an "overflow" bin, so user-supplied
     binning should have their last bin be SMALLER than the largest x- or
     y-value (otherwise you will have a column or a row with no points
     included).
   * The grids define the EDGES of the bins used by histogram2d(). I think
     (although I am not sure) that plot_image() and plot_contour() use any
     supplied coordinate grid to define the CENTERS of the pixels. Check the
     documentation of PGPLOT's PGIMAG routine to be sure.
   * Finally, I have no idea what histogram2d() does when it gets a
     pathologically-designed grid (e.g., bins with zero width, overlapping
     bins, or bins with lower bounds larger than their upper bounds). Please
     only use monotonically-increasing bins.

   The value of each pixel is, by default, the average of all points which
   fall into that bin. This can be changed by passing a reference to a
   function via the "func" qualifier. 

   The "func" qualifier must be a reference to an S-Lang function which takes
   a single array as input and outputs a single value, e.g., mean(), sum(), or
   length(). The input to this function is an array containing all z-values
   that fall into the current pixel. However, this function also receives a
   set of qualifiers. By default, this gives @func access to the data, the
   coordinate grids, and the list of points in the current pixel via the
   following qualifiers:
   qualifier("x"), qualifier("y"), qualifier("z"): x, y, and z data coordinates
   qualifier("xgrid"), qualifier("ygrid"): x and y coordinate grids
   qualifier("ndx"): array of data indices in the current pixel

   Extra qualifiers can be supplied via the func_quals qualifier, which
   should be set to a structure containing any additional qualifiers you need.
   This will be merged with the above set of qualifiers using struct_combine()
   with user-defined qualifiers taking precedence over the defaults in cases
   where the names are the same.

   As an example, to set each pixel's value to the innner product of the
   z-values and some other array of numbers (which we'll call "z2"), one could
   define:
   \code{
   define arr2im_dotprod(zValues) {
     variable z2 = qualifier("extra_array");
     variable ndx = qualifier("ndx");
     return inner_product(zValues,z2[ndx])[0];
   }
   }
   
   and then after reading in or defining x, y, z, and z2 arrays, do:
   \code{
   variable im = array2image(x,y,z;func=&arr2im_dotprod,func_quals=struct{extra_array=z2});
   }

   By default this function removes infinite and NaN values from the x, y, and
   z arrays before doing anything; the include_inf and include_nan switches
   disable this behavior. Note that including infinite and/or NaN values will
   cause problems for the default grids and pixel value assignment, so
   include_inf and include_nan are best used with user-defined functions and
   grids.

   This script is under development; please contact Paul Hemphill
   (pbh@space.mit.edu) regarding bugs or missing features (please do not
   report missing bugs; any bugs that are not present are missing
   intentionally).

   \seealso{histogram2d, plot_image}
 
\done

\function{array2matrix}
\synopsis{Transforms an Array_Type into a matrix}
\usage{Any_Type[l,...] = array2matrix(Array_Type[l] array);}
\qualifiers{
\qualifier{filler[=-_NaN]:}{Filler for empty array entries}
}
\description
    Similar to array_flatten, this function flattens the
    array into a single one. In contrast to array_flatten,
    the array_shape of the individual entries is conserved.
    If the array_shape of the individual array entries is
    not the same, the maximal dimension is determined and
    empty entries are being filled with the value given
    with the filler qualifier.
\example
    variable arr = Array_Type[3];
    arr[0] = _reshape( [1:3*4*5], [3,4,5] );
    arr[1] = _reshape( [1:3*4], [3,4] );
    arr[2] = _reshape( [1:2*3*4], [2,3,4] );
    
    variable mat = array2matrix(arr);
    print(mat);
\seealso{array_flatten}
\done

\function{array_copy}
\synopsis{makes a copy of a nested array}
\usage{Array_Type copy = array_copy(Array_Type[] array);}
\description
    \code{array_copy} copies an \code{Array_Type[]} with all its
    entries, which can be of any Type! If an entry is a Struct_Type
    \code{array_copy} calls \code{struct_copy} and if an entrie is
    an \code{Array_Type[]}, \code{array_copy} calls itself.
\example
    s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
    s[0].a[[0]]=[0:9];
    copy = array_copy(s); copy[0].a[[0]] = ["modified"];
    print(s[0].a);
\seealso{struct_copy}
\done

\function{array_density}
\synopsis{calculates the 'intensity' of an array, i.e. the invers of
the spacing}
\usage{array_density(bin_lo, bin_hi);}
\done

\function{array_fit_gauss}
\synopsis{performs a gaussian fit on given x- or xy-data}
\usage{Struct_Type array_fit_gauss(x [,y] [,dy] [,c] [,s] [,a] [,o]);}
\qualifiers{
\qualifier{frz}{boolean array determining which parameters (c,s,a,o) are frozen}
\qualifier{keep}{keeps the data and fit function (be careful, needs to be deleted for the next fit}
\qualifier{plot}{plot the given data and oplot the fit}
\qualifier{oplot}{only oplot the fit}
}
\description
   Tries to fit the given data by a gaussian and an offset. If the y-data is
   omitted, the given x-data is interpreted as y-data. The x-values are then
   generated as indices of the y-data.
   The uncertainties of the y-data can be passed to the fit algorithm by the
   `dy' parameter. If not given the errors are calculated assuming Poisson
   statistics.
   The four remaining parameters are the starting values:
     c - center position
     s - sigma
     a - area
     o - offset in y-direction
   They may also be given by an array as parameter `c' in the order shown
   above. The same order is used for the freeze status qualifier `frz'.
   Performing the fit is done with the actual choosen fit method. The result
   is returned as a structure of the form
     Struct_Type { center, sigma, area, offset, chisqr }
   where chisqr is the reduced chi-square value of the best fit.
\done

\function{array_flatten}
\synopsis{flattens an array of Array_Type[] into a single array}
\usage{Any_Type array_flatten(Array_Type[] array);}
\description
    The given 'array' itself contains arrays of any
    data type. This function flattens the 'array'
    such that all internal arrays are merged.

    Also the array can have entries of any data type,
    they must be of the same type! Otherwise the function
    will throw an error.
    To avoid this split the array, e.g., with

       arr1 = arr[where(_typeof(arr)==Double_Type)];

    or if there are NULL entries,

       arr1 = arr[wherenot( arr == NULL )];

   and use arr1 as argument for array_flatten!
    
\example
    variable arr = Array_Type[3];
    arr[0] = [1,2,3,4];
    arr[1] = [5,6];
    arr[2] = [7,8,9];
    
    variable arrF = array_flatten(arr);
    % returns an Integer_Type[9] containing
    % [1,2,3,4,5,6,7,8,9]
\done

\function{array_insert}
\synopsis{Inserts an array into another array}
\usage{Any_Type[] = array_insert(Any_Type[] A, Any_Type[] IA, Integer_Type i);}
\description
    Inserts the array or single value IA into the array A at the index
    position i. A and IA have to be of the same DataType_Type!
    Basically it is:

         [ A[[:i-1]], IA , A[[i:]] ]
    
\example
    variable A  = [1:10];
    variable IA = [2,1];
    variable NA = array_insert( A, IA, 3 );
    print(NA);
\done

\function{array_map_index}
\synopsis{Call a function on a subset of arrays}
\usage{Array_Type array_map_index (Int_Type[] start, Int_Type[] stop,
                              DataType_Type type, Ref_Type func, args, ...);}
\altusage{Array_Type array_map_index (Int_Type[] index, DataType_Type type,
                              Ref_Type func, args, ...);}
\altusage{Array_Type array_map_index (Ref_Type start, Ref_Type stop,
                              DataType_Type type, Ref_Type func, args, ...);}
\altusage{Array_Type array_map_index (Ref_Type index, DataType_Type type,
                              Ref_Type func, args, ...);}
\description
  The \code{array_map_index} function is somewhat the opposite of the
  \code{array_map}. As \code{array_map} allows it to apply a function to every
  array element individually, \code{array_map_index} can be used to apply a
  function (taking an array) on one or more subsamples of the target array.

  Case 1: If called with \code{start} and \code{stop} the function \code{func} is
  applied to \code{args[start[i]:stop[i]]} for each \code{i}.

  Case 2: If called with only one index array \code{index} the function is
  applied to args[index]. This is the same as \code{func(args[index]);}.

  IMPORTANT: array_map_index applys \code{func} to an array!

  Case 3: If called with \code{&start} and \code{&stop} the function behaves the
  same as for case 1, but this is usefull when you want to nest \code{array_map}
  calls.

  Case 4: If called with \code{index} the it behaves the same as for case 2 but,
  again, this might be usefull for \code{array_map} nesting.

  NOTE: This function can hide a lot of important operations so it should be
  avoided for implementing algorithms. However, it is very useful for fast
  scripting and interactive (shell) operations.

\example
  % Case 1: apply mean() to a subset of data
  data = rand_gauss(0.1, 100) % generate 100 normal distributed random numbers
  means = array_map_index([0,5,25], [99,94,74], &mean, data);
  print(means);
  % this outputs the same as
  print(mean(data)); print(mean(data[[5:94]])); print(mean(data[[25:74]]));

  % Case 2: apply mean() to specified subset
  data = rand_gauss(0.1, 100) % 100 random numbers
  idx = where(data < 0) % select only negative data
  neg_mean = array_map_index(idx, Double_Type, &mean, data);
  print(neg_mean);
  % this results in the same as
  print(mean(data[n]));

  % Case 3: apply mean() to array of arrays
  data = Array_Type[3];
  data[0] = rand_gauss(0.1, 100);
  data[1] = rand_gauss(0.01, 100);
  data[2] = rand_gauss(1, 100);
  start = [0,5,25];
  stop = [99,94,74];
  means = array_map(Array_Type, &array_map_index, &start, &stop,
                    Double_Type, &mean, data);
  % returns the mean as in example for case 1 but applied to all sub arrays
  % note that you have to declare the start and stop arrays explicitly because
  % &[5:94] is not a valid expression

  % Case 4: Same as case 3 but with only one index array

  % Use array_map_index to call a function n times
  rn = array_map_index([1:5], Array_Type, &rand_gauss, 0.1, 100);
  % gives 5 arrays of 100 random numbers each

\seealso{array_map, array_map_qualifiers}
\done

\function{array_map_qualifiers}
\synopsis{Apply a function to each element of an array (also if given as qualifier)}
\usage{Array_Type array_map (DataType_Type type, Ref_Type func, args, ... );}
\altusage{(Array_Type, ...) array_map (DataType_Type type, ..., Ref_Type func, args, ... );}
\description
   The 'array_map_qualifiers' function is an extension of the 'array_map'
   function and supports all its features.
   This function also works for qualifiers! First of all qualifiers given to
   the 'array_map_qualifiers' are passed onto the called function 'func'.

   Furthermore if the qualifier(s) itself is an array, it will be mapped too!
   This is especially helpfull when used for calls of integration function
   such as 'qromb' as these require parameters to be given as qualifiers!

   This functionality can be turned off with the "noqualmap"
   qualifier, which is necessary for, e.g., epochfolding routines (and much
   faster!)

   NOTE that this function works as 'array_map', i.e., the mapping will apply
   for the first argument which is an array! As the qualifiers are passed after
   the arguments, an argument array will supersede a qualifier array as long as
   they do not have the same length.

\example
   define t(x){ return qualifier("a",0)*x;};

   %% Mapping function using an argument array:
     print(array_map_qualifiers( Double_Type, &t, [1,2,3,4] ; a=1 ));

   %% Mapping function using a qualifier array:
     print(array_map_qualifiers( Double_Type, &t, 1 ; a=[1,2,3,4] ));

   %% Mapping function argument array superseding qualifier array:
     print(array_map_qualifiers( Double_Type, &t, [3,4,5,6] ; a=[1,2] ));
     
   %% Mapping function using argument array AND qualifier array:
     print(array_map_qualifiers( Double_Type, &t, [3,4,5,6] ; a=[1,2,3,4] ));

   %% Map function using a qualifier array application on 'qromb' integrater
     print(array_map_qualifiers( Double_Type, &qromb, &t, 0, 1 ; a=[0:2] ));

\seealso{array_map, struct_of_arrays_2_struct_array}
\done

\function{array_permutation_matrix}
\synopsis{returns all possible permutations of the given arrays}
\usage{Array_Type array_permutation_matrix(Array_Type[] or List_Type arrays);}
\description
    Calculates the index arrays of all possible permutations of the
    elements in the given arrays (see the below example). The input
    has to be either an array of arrays (Array_Type) or a list of
    arrays (List_Type). The returned matrix (J,K) is an array of
    integer arrays. The element (J,K) is the index of the K^th input
    array, which corresponds to the J^th permutation.    
\example
    variable arr = Array_Type[3];
    arr[0] = [10,20,30,40];
    arr[1] = [50,60];
    arr[2] = [70,80,90];
    
    variable matrix = array_permutation_matrix(arr);
    % Returns an Array_Type[24] corresponding to 4*2*3 = 24 possible
    % permutations. Each element is an Integer_Type[3] containing the
    % indices for each of the 3 input arrays:
    % matrix[0]  = [0,0,0] -> [10,50,70]
    % matrix[1]  = [1,0,0] -> [20,50,70]
    % matrix[2]  = [2,0,0] -> [30,50,70]
    % matrix[3]  = [3,0,0] -> [40,50,70]
    % matrix[4]  = [0,1,0] -> [10,60,70]
    % matrix[5]  = [1,1,0] -> [20,60,70]
    % ...
    % matrix[23] = [3,1,2] -> [40,60,90]
\done

\function{array_permute}
\synopsis{generates a random permutation of [0 : n-1]}
\usage{Integer_Type[] random_array(Array_Type a)
\altusage{Integer_Type[] random_array(Integer_Type n)}
}
\description
    The return value is an array of indices.
    To randomize an array \code{a}, use \code{a[array_permute(a)]}.
\seealso{array_sort}
\done

\function{array_remove}
\synopsis{removes one element from an array}
\usage{Array_Type a_ = array_remove(Array_Type a, Integer_Type i);}
\description
    \code{a_} contains all elements of \code{a} excecpt of the one indexed by \code{i}.
\done

\function{array_split_at_extrema}
\synopsis{Splits an array into monotone chucks}
\usage{ Array_Type[] I = array_split_at_extrema( array );}
\done

\function{array_unique}
\synopsis{takes elements of an array only once}
\usage{Array_Type array_unique(Array_Type a)}
\description
   Duplicated array elements are removed from the array.
   This function is basically
      a[unique(a)]
   where unique gives the indicies of the unique array
   elements.
\seealso{unique}
\done

\function{array_zip}
\synopsis{"zip" two or more arrays together}
\usage{Any_Type array_zip(Any_Type[] a, b, ...);}
\description
    Returns an array of tuples (or higher orders), where each tuple i
    contains the i-th element from each of the given arrays. The
    returned array is truncated to the length of the shortest
    array. Thus, the number of tuples is equal to this smallest
    length.
\example
    a = [1:10];
    b = [11:20];
    c = array_zip(a,b);
    % c = [1,11,2,12,3,13,4,14,...,10,20];
\done

\function{arrtimes}
\synopsis{fit function for modelling arrival times}
\usage{arrtimes(id)}
\description
    This model can bes used in arrival time fits. The
    pulse arrival times are calculated using the
    'pulse_time' function, including orbital motion
    and up to the third derivative of the pulse
    period. The observed pulse arrival time t_obs(n)
    is calculated by

      t_obs(n) = t0 + A*n + B*n^2 + C*n^3 + D*n^4
                 + z(t_emit(n))/c
                 
    where A to D are coefficients depending on the
    pulse period and its higher derivatives, z/c is
    the Doppler Shift due to orbital motion (see
    'BinaryPos' for details) and t_emit(n) is the
    time, when the nth pulse is emitted in the bary-
    centre of the binary. This time is found solving
    the above equation:
    
      t_emit(n) = t_obs(n) - z/c
      
    However, this can not be solved since the
    observed arrival times include the Doppler
    Shift. Hence the emission time must be found
    iteratively, starting at t_emit(n) = t_obs(n).
    The found pulse numbers are stored in a reference
    variable, if set by 'define_atime'.
    The model parameters are
      ppuls - pulse period (s)
      pdot  - first derivative (s/s)
      p2dot - second derivative (s/s^2)
      p3dot - third derivative (s/s^3)
      dphi  - constant additive phase shift
      porb  - orbital period (d)
      torb0 - time of periastron passage (MJD)
      asini - projected semi major axis (lts)
      ecc   - eccentricity
      omega - angle of periastron (degrees)
      pporb - change of orbital period (d/d)
    The reference time of the pulse ephemeris,
    called tpuls0 in 'atime_get_ephemeris' for
    example, is fix and set by 'define_atime'.
    If you know what you are doing you can
    change this time using 'atime_set_ephemeris'.
\seealso{pulse_time, atime_set_ephemeris, define_atime, BinaryPos}
\done

\function{AS}
\synopsis{Evaluate equations of motion, total energy, or circular velocity derived from a revised
    Allen & Santillan potential}
\usage{AS(Double_Types t, m[6,n]; qualifiers)}
\qualifiers{
\qualifier{coords}{[\code{="cyl"}] Use cylindrical ("cyl") or cartesian ("cart") coordinates.}
\qualifier{eomecd}{[\code{="eom"}] Return equations of motion ("eom"), total energy ("energy"),
      circular velocity ("circ"), or Sun-Galactic center distance ("sgcd").}
\qualifier{Mb}{[\code{=409}] Mass of bulge in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{Md}{[\code{=2856}] Mass of disc in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{Mh}{[\code{=1018}] Mass scale factor of halo in Galactic mass units,
               see Irrgang et al. 2013.}
\qualifier{bb}{[\code{=0.23}] Bulge scale length, see Irrgang et al. 2013.}
\qualifier{ad}{[\code{=4.22}] Disc scale length, see Irrgang et al. 2013.}
\qualifier{bd}{[\code{=0.292}] Disc scale length, see Irrgang et al. 2013.}
\qualifier{ah}{[\code{=2.562}] Halo scale length, see Irrgang et al. 2013.}
\qualifier{exponent}{[\code{=2}] Exponent in the halo mass distribution, see Irrgang et al. 2013.}
\qualifier{cutoff}{[\code{=200}] Halo cutoff, see Irrgang et al. 2013.}
}
\description
    Evaluate the equations of motion, the total energy, or the circular velocity at time 't'
    derived from the revised Galactic gravitational potential by Allen & Santillan (see Model I
    in Irrgang et al., 2013, A&A, 549, A137) using either cylindrical coordinates (r [kpc],
    phi [rad], z [kpc]) and their canonical momenta vr [kpc/Myr], Lz [kpc^2/Myr], vz [kpc/Myr])
    or cartesian coordinates (x [kpc], y [kpc], z [kpc], vx [kpc/Myr], vy [kpc/Myr], vz [kpc/Myr]),
    see qualifier 'coords'.  Conservation of angular momentum Lz is implemented in the equations
    of motion for cylindrical coordinates only. The total energy E_total [kpc^2/Myr^2] is not
    used to integrate the equations of motion although being a conserved quantity, too. Therefore,
    conservation of energy, i.e., of E_total, is a measure for the precision of the numerical
    methods applied.

    For computing orbits with n different initial conditions, the input parameter m is
    a [6,n]-matrix with (qualifier("coords")=="cyl")   or (qualifier("coords")=="cart")
       m[0,*] = r;                                        m[0,*] = x;
       m[1,*] = phi;                                      m[1,*] = y;
       m[2,*] = z;                                        m[2,*] = z;
       m[3,*] = vr;                                       m[3,*] = vx;
       m[4,*] = Lz;                                       m[4,*] = vy;
       m[5,*] = vz;                                       m[5,*] = vz;
    If the qualifier 'eomecd' is set to "eom", the function returns a [6,n]-matrix delta with
       delta[0,*] = vr;                                   delta[0,*] = vx;
       delta[1,*] = Lz/r^2; % = vphi                      delta[1,*] = vy;
       delta[2,*] = vz;                                   delta[2,*] = vz;
       delta[3,*] = -d/dr (Potential(r,z) + Lz^2/r^2);    delta[3,*] = -d/dx Potential(x,y,z);
       delta[4,*] = 0; % -d/dphi Potential(r,z)           delta[4,*] = -d/dy Potential(x,y,z);
       delta[5,*] = -d/dz Potential(r,z);                 delta[5,*] = -d/dz Potential(x,y,z);
    If the qualifier 'eomecd' is set to "energy", the function returns a [1,n]-array storing
    the total energy for each orbit:
       E_total(r,z,vr,vz,Lz) = Double_Type[n] = 0.5*(vr^2+vz^2+Lz^2/r^2) + Potential(r,z)
    or
       E_total(x,y,z,vx,vy,vz) = Double_Type[n] = 0.5*(vx^2+vy^2+vz^2) + Potential(x,y,z)
    If the qualifier 'eomecd' is set to "circ", the function returns a [1,n]-array storing
    the circular velocity for each orbit:
       v_circ(r,z) = Double_Type[n] = sqrt( r * d/dr Potential(r,z) )
    If the qualifier 'eomecd' is set to "sgcd", the function returns the Sun-Galactic center
    distance found to fit best to this potential.
\example
    delta = AS(0, m);
    energy = AS(0, m; eomecd="energy");
    v_circ = AS(0, m; eomecd="circ");
    sgcd = AS(; eomecd="sgcd");
\seealso{orbit_calculator, MN_NFW, MN_TF, plummer_MW}
\done

\function{ascii_read_table}
\synopsis{reads an ASCII table from a file into a structure}
\usage{Struct_Type table = ascii_read_table(String_Type filename, String_Type formats[]);
\altusage{Struct_Type table = ascii_read_table(String_Type filename, List_Type infos[]); with  infos[i] = { formats[i], columns[i] };}
\altusage{(table, keys) = ascii_read_table(String_Type filename, List_Type infos[]);\n
    % with  infos[i] = { formats[i], columns[i], units[i] };}
}
\description
    The data format of the columns has to be specified as for sscanf; i.e.,
    %s for strings, %d for decimal integers, %F for double precision floats, ...
    Lines starting with a comment string ("#" by default; see below) are ignored.
    The return value is a structure containing the table.
    Therefore, the column names have to respect the conventions
    for struct-field names (no special characters as "-"...).
    If the column name is "", this column is skipped.
    If a unit is given, the function ascii_read_table has a second return value,
    namely a keys-structure which can be used for \code{fits_write_binary_table}.
\qualifiers{
\qualifier{comment}{string which indicates comments [default = "#"]}
\qualifier{startline}{Only lines after this number are considered.}
\qualifier{endline}{The file is not read after this line number.}
\qualifier{verbose}{}
}
\examples
    \code{variable tab1 = ascii_read_table(filename, [{"%s"}, {"%F"}, {"%F"}]);}\n
    \code{%} reads \code{String_Type tab1.col1}, \code{Double_Type tab1.col2}, \code{Double_Type tab1.col3} from \code{filename}

    \code{variable tab2 = ascii_read_table(filename, [{"%s","A"}, {"%F","B"}, {"%F","C"}]);}\n
    \code{%} reads \code{String_Type tab2.A}, \code{Double_Type tab2.B}, \code{Double_Type tab2.C}

    \code{variable tab3 = ascii_read_table(filename, [{"%s","A"}, {"%F",""}, {"%F","C"}]);}\n
    \code{%} reads \code{String_Type tab3.A}, skips one \code{Double_Type} column and reads \code{Double_Type tab3.C}

    \code{variable tab4, keys;}\n
    \code{(tab4, keys) = ascii_read_table(filename, [{"%s","A"}, {"%F"}, {"%F","C","u"}]);}\n
    \code{%} reads \code{String_Type tab4.A} and \code{Double_Type tab4.C} as before. With\n
    \code{fits_write_binary_table(FITSfilename, "tab4", tab4, keys);}\n
    \code{%} the unit "u" is assigned to the (second) column C.
\seealso{sscanf, fits_read_table, fits_write_binary_table, readcol}
\done

\function{__assemblePanstarrsUrl}
\synopsis{Assemble a URL for the PanSTARRS image cutout server}
\usage{String_Type __assemblePanstarrsUrl(String_Type position, Integer_Type size)}
\description
    position is either a set of coordinates of the form "ra+/-dec"
    (e.g. "124.45+34") or a string which is used by the cutout server itself
    to perform a simbad query.
    size is the pixel number along the edge of the square image.
    The qualifiers for the function are the set of options for filter, filetype,
    and auxiliary data given in __panstarrsFilters, __panstarrsFiletypes, and
    __panstarrsAuxiliary
\done

\function{associate_arrays}
\synopsis{computes the smallest pairwise difference between elements of two arrays and returns the indices}
\usage{Struct_Type result = associate_arrays(Double_Type[n1] A, Double_Type[n2] B);}
\description
    For each element \code{A[i]} this function searches the element \code{B[j]}
    with the smallest difference to \code{A[i]}. It returns the indices \code{j}
    as \code{result.index[i]} and the corresponding difference as \code{result.diff[i]}.
\qualifiers{
\qualifier{nr_closest}{ [=1]: set to N, to obtain the N closest elements.
                       In this case additionally the index of the second, ..., N-th closest element is
                       returned as \code{result.index2[*,0], ..., result.index2[*,N-2]},
                       the same holds for \code{result.diff2}.}
}
\seealso{array_sort}
\done

\function{associate_coords}
\synopsis{computes the smallest pairwise angular separation between arrays of positions and returns the indices}
\usage{Struct_Type result = associate_coords(Double_Type[n1] RA1, Double_Type[n1] DEC1),Double_Type[n2] RA2, Double_Type[n2] DEC2);}
\qualifiers{
\qualifier{nr_closest}{[=1]      set to N, to obtain the N closest elements.
                            In this case additionally the index of the second, ..., N-th closest element is
                            returned as \code{result.index2[*,0], ..., result.index2[*,N-2]},
                            the same holds for \code{result.sep2}.}
\qualifier{unit}{[\code{="deg"}]        unit of the angular coordinates}
\qualifier{alpha1_unit}{[\code{="deg"}] unit of the RA1}
\qualifier{alpha2_unit}{[\code{="deg"}] unit of the RA2}
\qualifier{delta1_unit}{[\code{="deg"}] unit of the DEC1}
\qualifier{delta2_unit}{[\code{="deg"}] unit of the DEC2}
}
\description
    For each position \code{(RA1[i], DEC1[i])} this function searches the element \code{(RA2[j],DEC2[j])}
    with the smallest difference to \code{(RA1[i],DEC[i])}. It returns the indices \code{j}
    as \code{result.index[i]} and the corresponding separation in rad as \code{result.sep[i]}.
    The units of each coordinate can be set with the qualifier \code{unit}, but also independently,
    as for the function \code{greatcircle_distance}.
\seealso{greatcircle_distance, associate_arrays}
\done

\function{assoc_copy}
\synopsis{makes a copy of an associated array}
\usage{Assoc_Type copy = assoc_copy(Assoc_Type list);}
\description
    \code{assoc_copy} returns a properly dereferenced copy of an
    arbitrarily nested \code{Assoc_Type} with all its entries. 
    Depending on the Data_Type of the entries, \code{assoc_copy}
    iteratively calls the according sub-function
    (\code{struct_copy}, \code{array_copy} or \code{list_copy}).
    If the Data_Type of one of its entries is not
    one of previously mentioned ones, \code{assoc_copy} returns
    @(Data_Type) or respectively @(entry).
\example
     s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
     s[0].a[[0]]=[0:9];
     l = [{ array_copy(s), 1., [1:10] }, {"abs"}];
     copy = COPY(l); copy[0][0][0].a[[0]] = ["modified"];
     print(l[0][0][0].a);
\seealso{COPY, struct_copy, array_copy, list_copy}
\done

\function{assoc_struct_combine}
\synopsis{associate structure fields, combine structures and fill entries}
\usage{Struct_Type \code{comb} = assoc_struct_combine(Struct_Type \code{s1}, Struct_Type \code{s2}, String_Type \code{f1}, String_Type \code{f2});}
\qualifiers{
\qualifier{double_fill [=_NaN]}{: filler for missing entries in Double_Type fields of \code{s2}}
\qualifier{float_fill [=_NaN]}{:  filler for missing entries in Float_Type fields of \code{s2}}
\qualifier{integer_fill [=_0]}{:  filler for missing entries in Integer_Type fields of \code{s2}}
\qualifier{string_fill [=""]}{:   filler for missing entries in String_Type fields of \code{s2}}
\qualifier{flag_field_name  [="both_struct_flag"]}{: additional field name for \code{comb}}
}
\description
    This function combines two structures. It uses the field with name \code{f1} of the
    structure \code{s1} and that with name \code{f2} of \code{s2} to associate the entries with the function
    get_intersection. The fields of the returned structure \code{comb} have the length of the
    field \code{f1}. The entries in these fields are filled according to the qualifiers, if
    there is no association found for this entry.
    Fields of a type for which no fill qualifier is given are filled with NULL and
    returned as a list. Fillers can be defined via qualifiers for all variable types.
    The suffix _fill is required, i.e., for example the qualifier to define a filler
    for Complex_Type is \code{complex_fill}.
    WARNING: This function fails for more dimensional fields. If there are fields with
    the same name in both structures, the field of \code{s2} is used, as it is done by
    struct_combine. The field \code{f2} is not included.
    The function adds a field (name can be indentified via qualifier \code{flag_field_name}),
    which indicates if the entry was found in both structures (1) or if it exists
    only in \code{f1} (0).
    
\examples
    a = struct{source = ["1", "2", "3"], flux=[0.1, 2.5, 0.7]};
    b = struct{source = ["3", "2"], redshift=[0.65, 0.03], opt_id = ["Quasar","BL Lac"]};
    c = assoc_struct_combine (a,b,"source", "source");
    print_struct(c);

    % without filling fields, but using just common fields this corresponds to:
    (i1,i2) = get_intersection(a.source,b.source);
    struct_filter(a,i1); 
    struct_filter(b,i2);
    d = struct_combine(a,b);
    print_struct(d);
\seealso{get_intersection, struct_combine, struct_filter}
\done

\function{aStar}
\synopsis{pathfinding algorithm A*}
\usage{Struct_Type aStar(Double_Type[][] graph, Integer_Type startX, startY, endX, endY);}
\qualifiers{
    \qualifier{max}{find the most expensive path instead of the cheapest one}
    \qualifier{meanCost}{costs are normalized by the best path length to any point
               in the graph, resulting in a mean cost on a path}
    \qualifier{estimate}{reference to a function, which estimates the cost between
               two nodes in the graph (parameters: x0, y0, x1, y1). By
               default, the distance between both nodes is used}
    \qualifier{plot}{plots the graph and the working progress at each step. Nodes
               in the open list are shown in red, in the closed list in
               blue, nodes of infinite cost in green and the best path,
               finally, in black. The function sleeps 0.01 seconds after
               each step or the value given with this qualifier}
}
\description
    Pathfinding algorithm with high performance and accuracy by
      Hart, Nilsson & Raphael, "A Formal Basis for the Heuristic
      Determination of Minimum Cost Paths", IEEE Transactions on
      Systems Science and Cybernetics SSC4 (2), 100107, 1968
                               
    The A* algorithm (called A Star) finds the shortest (or in general
    cheapest) path between to nodes in a graph. The algorithm looks for
    the best way following the lowest known heuristic cost. This cost
    has to be estimated at each new discovered node and is, by default,
    the straight-line distance between both points. The runtime and
    accuracy of the search strongly depends on the choice of this
    estimation.

    The 'graph' has to be given as 2d-array defining the cost at each
    point (node). Infinite costs (_Inf) are allowed and corresponds to
    insuperable borders. The starting node and goal node habe to be
    given as array indices 'startX', 'startY' and 'endX', 'endY',
    respectively.

    The returned structure contains
      x    - x-indices of the best path
      y    - y-indices of the best path
      cost - summed costs along the best path
\done

\function{atime_calc_beta}
\synopsis{converts a pulse phaseshift into arrival times}
\usage{(Double_Type[] arrtimes, errors) = atime_calc(Struct_Type atime, eph[, orb]);
 or (Double_Type[] arrtimes, errors) = atime_calc(Double_Type[] t0, phi, error, Struct_Type eph[, orb]);}
\description
    Takes the structure determined by 'atime_det'
    or an array of times, phase shifts and their
    errors and returns the pulse arrival times
      t = t0 + phi * p(t0) [+ z(t0)/c]
    To calculate the pulse period at each time
    a pulse ephemeris structure is needed (see
    'check_pulseperiod_orbit_struct'). If an
    optional structure containing orbital
    parameters is given, the Doppler shift delay
    is included as well.
\seealso{atime_det, check_pulseperiod_orbit_struct, pulseperiod}
\done

\function{atime_dataind}
\synopsis{returns the datasets, which contain arrival times}
\usage{Integer_Type[] atime_dataind();}
\description
    Loops over all defined datasets and checks, which
    are containing arrival times. Therefore the function
    'atime_metavalid' is used. If no dataset is found,
    the function returns NULL.
\seealso{atime_metavalid, define_atime}
\done

\function{atime_det}
\synopsis{Determines the arrival times from a lightcurve using a pulse pattern}
\usage{Struct_Type atime_det(Struct_Type lc, Struct_Type[] pattern, Double_Type t0, Struct_Type ephemeris[, Struct_Type orbit]);
 or Struct_Type atime_det(String_Type lc, Struct_Type[] pattern, Double_Type t0, Double_Type[] period[, Struct_Type orbit]);}
\qualifiers{
    \qualifier{time}{name of the time field in the FITS-file,
             see fits_read_lc for details}
    \qualifier{indiv}{determine individual pulses. If no value
             is assigned, all arrival times are re-
             turned. Otherwise a given integer sets
             the number of pulses to average}
    \qualifier{movem}{if 'indiv' is greater one, so it is averaged
             over a number of arrival times, a
             moving mean is used to get all individual
             pulses. Note that the pulses are then not
             statistically independent!}
    \qualifier{ccfint}{interpolates the cross correlation of the
             pulse pattern and the actual analyzed
             pulse to increase the accuracy. Only works
             well for clear signals! The number of
             interpolated bins between each original
             bin is set to the assigned number
             (default = 4)}
    \qualifier{varerr}{estimates the error by a local standard
             deviation. Therefore the variation of the
             given number of arrival times is used
             (default = 10)}
    \qualifier{mcerr}{estimates the error of each arrival time
             by performing Monte Carlo simulations. If
             an integer is assigned it sets the number
             of runs (default = 10000). Has a higher
             priority than 'varerr'}
    \qualifier{mcgaus}{fit a gaussian to the Monte Carlo distri-
             bution, if used for error estimation}
    \qualifier{getpat}{if multiple patterns are given, the index
             of the used one is stored in this variable,
             which has to be given as a reference (@var)}
    \qualifier{match}{reference to a variable (&var) where the
             matching pulses are saved as structures
             similar to the reference profile}
    \qualifier{ccflim}{matches below the given cross-correlation
             values are skipped (default = 0), allowed
             range is -1 to 1.}
    \qualifier{chatty}{boolean value for output messages
             (default = 1). If set to 2, also echo
             result of Monte Carlo error estimation}
    \qualifier{debug}{plot the cross correlation and the matching
             pulse, echo the found phase shift and
             sleep the given seconds (default = 1) or,
             if set to 'user' wait until a key is
             pressed}
}
\description
    Using one or more pulse pattern the arrival times
    of pulses in a lightcurve are determined using
    phase connection. The pattern must be given as a
    structure with the fields
      Double_Type bin_lo
      Double Type bin_hi
      Double_Type value
      Double_Type error,
    as, for example, returned by the 'epfold' function.
    The lightcurve can be passed by a structure with
    the fields 'time', 'rate', 'error' and 'fracexp',
    or by the filename to a FITS-file.
    To transform the phase shift to time, the pulse
    period at the actual position in the lightcurve is
    needed. For a first guess this may be a constant
    value (in seconds) or an array of two elements
    defining a range of periods (min/max values, in
    seconds). In the latter case the pulse period is
    searched by using the 'getVarPeriod' function and
    additional qualifiers are passed. However, the
    correct way is to pass the pulse ephemeris and
    additional orbital parameters to calculate the
    pulse poriod more precisely. The corresponding
    structures are described in the 'check_pulseperiod_orbit_struct'
    function. The values may be found by a fit to the
    arrival times using a constant pulse period. This
    leads to slightly different results, hence the
    determination of the arrival times and their
    analysis is an iterative procedure.
    If the 'indiv' qualifier is omitted the given
    lightcurve is folded and the returned arrival
    time corresponds to the phase shift of the given
    pattern to the resulting profile.
    If the method for estimating the uncertainty of
    the arrival times is not specified by qualifiers,
    the default uncertainty is set to one phase bin.
    The number of bins are derived from the given
    pulse pattern.
    The returned structure has the following fields:
    arrtimes - array of determined arrival times (MJD)
    error    - their uncertainties (days)
    arrnum   - relative pulse number of each arrival
               time to a specific pulse. Dramatically
               increases the speed of a fit.
    numref   - index of the reference pulse, should be
               zero. Is updated if arrival times are
               merged.
    reft0    - the given reference time t0
\seealso{atime_merge, save_atime, define_atime, arrtimes, pfold}
\done

\function{atime_det_beta}
\synopsis{Determines the arrival times from a lightcurve using a pulse pattern}
\usage{Struct_Type atime_det_beta(Struct_Type lc, Struct_Type[] pattern, Double_Type t0[, Double_Type period]);
 or Struct_Type atime_det_beta(String_Type lc, Struct_Type[] pattern, Double_Type t0[, Double_Type period]);}
\qualifiers{
    \qualifier{time}{name of the time field in the FITS-file,
             see fits_read_lc for details}
    \qualifier{indiv}{determine individual pulses. If no value
             is assigned, all arrival times are re-
             turned. Otherwise a given integer sets
             the number of pulses to average}
    \qualifier{movem}{if 'indiv' is greater one, so it is aver-
             aged over a number of arrival times, a
             moving mean is used to get all individual
             pulses. Note that the pulses are then not
             statistically independent!}
    \qualifier{ccfint}{interpolates the cross correlation of the
             pulse pattern and the actual analyzed
             pulse to increase the accuracy. Only works
             well for clear signals! The number of
             interpolated bins between each original
             bin is set to the assigned number
             (default = 4)}
    \qualifier{varerr}{estimates the error by a local standard
             deviation. Therefore the variation of the
             given number of arrival times is used
             (default = 10)}
    \qualifier{mcerr}{estimates the error of each arrival time
             by performing Monte Carlo simulations. If
             an integer is assigned it sets the number
             of runs (default = 10000). Has a higher
             priority than 'varerr'}
    \qualifier{mcgaus}{fit a gaussian to the Monte Carlo distribution, if
             used for error estimation}
    \qualifier{ccflim}{matches below the given cross-correlation
             values are skipped (default = 0), allowed
             range is -1 to 1.}
    \qualifier{numdif}{maximum allowed deviation from expected
             pulse number (default = 0.5 phases)}
    \qualifier{match}{reference to a variable (&var) where the
             matching pulses are saved as structures
             similar to the reference profile}
    \qualifier{slope}{reference to a variable (&var) where the
             slope of the lightcurve is saved, which
             is calculated by an interpolation of
             the mean count rate in the single pulses}
    \qualifier{skipsl}{do not take the slope of the lightcurve
             into account}
    \qualifier{mmnorm}{by default the pulse pattern and the
             pulses are renormalized by
               f = (f - mean(f)) / sdev(f)
             If this qualifier is set, the min/max-
             normalizaton is used instead
               f = (f - max(f)) / (max(f) - min(f))}
    \qualifier{chatty}{boolean value for output messages
             (default = 1). If set to 2, also echo
             result of Monte Carlo error estimation}
    \qualifier{debug}{plot the cross correlation and the matching
             pulse, echo the found phase shift and
             sleep the given seconds (default = 1) or,
             if set to 'user' wait until a key is
             pressed}
}
\description
    Using one or more pulse pattern the arrival times
    of pulses in a lightcurve are determined using
    phase connection. The patterns must be given as a
    structure with the fields
      Double_Type bin_lo
      Double Type bin_hi
      Double_Type value
      Double_Type error,
    as, for example, returned by the 'epfold' function.
    If multiple patterns are given, the best matching
    one is used to determine the arrival times.
    The lightcurve can be passed by a structure with
    the fields 'time', 'rate', 'error' and 'fracexp',
    or by the filename to a FITS-file.
    To determine individual pulses as enabled by the
    'indiv' qualifier an approximate pulse period must
    be given (in seconds) to cut the lightcurve into
    segments were the pulses are searches. Instead of
    the pulse period an array of two elements defining
    a range of periods (min/max values, in seconds)
    may also be given. In that case the 'getVarPeriod'
    function is used to determine the pulse period and
    additional qualifiers are passed.
    If the 'indiv' qualifier is omitted the given
    lightcurve is folded and the returned phase shift
    corresponds to the phase shift of the used pattern
    to the resulting profile.
    If the method for estimating the uncertainty of
    the phase shifts is not specified by qualifiers,
    the default uncertainty is set to one phase bin.
    The number of bins are derived from the given
    pulse pattern.
    The returned structure has the following fields:
    t0       - array of first time bins of the pulses
               determined from the lightcurve (MJD)
    phi      - array of phase shifts of the pulses
               with respect to the used pattern
    error    - uncertainties of 'phi'
    arrnum   - relative pulse number of each arrival
               time to a specific pulse. Dramatically
               increases the speed of a fit.
    numref   - index of the reference pulse, should be
               zero. Is updated if arrival times are
               merged.
    reft0    - the given reference time t0
    p0       - the given pulse period
    usedpat  - index of the used pulse pattern in case
               of multiple ones given
    The arrival times can be calculated by
      t = t0 + phi * p0
    as a first guess, since p0 is a function of time.
    This fact is handled properly by the fit function
    'arrtimes'. The function 'atime_calc' calculates
    the arrival times according to the above equation
    with respect to a given pulse ephemeris and
    orbital  parameters.
\seealso{atime_calc, atime_merge, save_atime, define_atime, arrtimes, pfold}
\done

\function{atime_get_ephemeris}
\synopsis{retrieves the actual model parameters of a given
    dataset of pulse arrival times}
\usage{(Struct_Type, Struct_Type) atime_get_ephemeris(Integer_Type idx);}
\description
    Returnes two structures containing the model parameters
    of a dataset, which contains pulse arrival times. The
    first structure describes the pulse ephemeris:
      ppuls  - pulse period (s)
      pdot   - first derivative (s/s)
      p2dot  - second derivative (s/s^2)
      p3dot  - third derivative (s/s^3)
      tpuls0 - reference time (MJD)
    The orbital parameters are stored in the second
    structure:
      porb   - orbital period (d)
      torb0  - time of periastron passage (MJD)
      asini  - projected semi major axis (lts)
      ecc    - eccentricity
      omega  - angle of periastron (degrees)
    If porb or asini is zero, the returned orbital
    structure will be set to NULL.
\seealso{arrtimes, check_pulseperiod_orbit_struct}
\done

\function{atime_get_t0}
\synopsis{returns the reference time of the arrival times}
\usage{Double_Type atime_get_t0(Integer_Type[] dataid);}
\description
    This function returns the emitting time of the pulse
    of number 0 as used in the polynomial to calculate
    pulse arrival times.
\seealso{arrtimes, pulse_time}
\done

\function{atime_load_ephemeris}
\synopsis{returns previously ssaved pulse ephemeris and orbital parameters}
\usage{atime_load_ephemeris(Integer_Type idx, String_Type filename);
 or (Struct_Type, Struct_Type) atime_load_ephemeris(String_Type filename);}
\description
    The pulse ephemeris and orbital parameters, which
    have been previously saved by using the function
    'atime_save_ephemeris', are either assigned to the
    given dataset 'idx' or returned as structures.
    The first one defines the pulse ephemeris and the
    second one the orbit.
    The following commands are equal:
      (eph, orb) = atime_load_ephemeris(filename);
      (eph, orb, ) = evalfile(filename);
\seealso{atime_save_ephemeris, atime_get_ephemeris, evalfile}
\done

\function{atime_merge}
\synopsis{merges two or more structures of arrival times into one}
\usage{Struct_Type atime_merge(Struct_Type atime1, Struct_Type atime2);
 or Struct_Type atime_merge(Struct_Type[] atime);}
\qualifiers{
    \qualifier{nosort}{do not sort the merged arrival times}
}
\description
    Either both given structures or an array of
    structures containing arrival times are merged.
    The fields of the structures must be equal to
    the ones described in 'atime_det'.
    The relative pulse numbers and their references
    are updated to the indices of the new array of
    arrival times. In addition the arrival times are
    sorted by time unless the 'nosort' qualifier is
    given.
\seealso{atime_det}
\done

\function{atime_merge_beta}
\synopsis{merges two or more structures of arrival times into one}
\usage{Struct_Type atime_merge(Struct_Type atime1, Struct_Type atime2);
 or Struct_Type atime_merge(Struct_Type[] atime);}
\qualifiers{
    \qualifier{nosort}{do not sort the merged arrival times}
}
\description
    Either both given structures or an array of
    structures containing arrival times are merged.
    The fields of the structures must be equal to
    the ones described in 'atime_det'.
    The relative pulse numbers and their references
    are updated to the indices of the new array of
    arrival times. In addition the arrival times are
    sorted by time unless the 'nosort' qualifier is
    given.
\seealso{atime_det}
\done

\function{atime_metavalid}
\synopsis{checks if a given dataset of arrival times contains valid metadata}
\usage{Integer_Type[] atime_metavalid(Integer_Type index);}
\description
    If a dataset of arrival times is defined using the
    'define_atime' function, there are also a lot of
    addiotional needed informations defined in the
    metadata. These are used by fitting the data by the
    'arrtimes' fit function. This function can be used
    to find all defined data containing arrival times,
    which is implemented in 'atime_dataind'.
\seealso{define_atime, arrtimes, get_dataset_metadata, atime_dataind}
\done

\function{atime_save_ephemeris}
\synopsis{saves pulse ephemeris and orbital parameters into as an S-lang script}
\usage{atime_save_ephemeris(Integer_Type idx, String_Type filename);
 or atime_save_ephemeris(Struct_Type eph[, Struct_Type orb], String_Type filename)}
\description
    The fitted pulse ephemeris and orbital paramters
    of the given dataset or the structures themself
    are saved into a file. This file is an S-lang
    script and can be called directly or loaded by
    using 'atime_load_ephemeris'.
\seealso{atime_load_ephemeris, atime_get_ephemeris}
\done

\function{atime_set_ephemeris}
\synopsis{sets the actual model parameters of a given
    dataset of pulse arrival times}
\usage{atime_set_ephemeris(Integer_Type[] idx, Struct_Type eph[, Struct_Type orb]);}
\altusage{atime_set_ephemeris(Integer_Type[] idx, Double_Type eph[, Struct_Type orb]);}
\qualifiers{
    \qualifier{sett0}{also set the reference time of the pulse
               ephemeris, which is used internally}
}
\description
    The given structure(s) containing the pulse ephemeris
    and the orbital parameters are used to set the model
    parameters of dataset 'idx'. Instead of the pulse
    ephemeris the pulse period may be given only. Other-
    wise the structure has to contain:
      ppuls  - pulse period (s)
      pdot   - first derivative (s/s)
      p2dot  - second derivative (s/s^2)
      p3dot  - third derivative (s/s^3)
      tpuls0 - reference time (MJD)
    The orbital parameters are stored in the second
    structure:
      porb   - orbital period (d)
      torb0  - time of periastron passage (MJD)
      asini  - projected semi major axis (lts)
      ecc    - eccentricity
      omega  - angle of periastron (degrees)
    By default the reference time of the pulse ephemeris
    is NOT set, because this should be fixed and set to
    the same time as used for the pulse pattern. This
    time therefore defines the arrival time of the pulse
    of number n=0. Only change this time if you know what
    effects will result (see arrtimes function)!    
\seealso{arrtimes, atime_get_ephemeris}
\done

\function{atime_shiftref}
\synopsis{switches the shift of the reference pulse number}
\usage{atime_shiftref(Integer_Type[] dataid[, Integer_Type boolean]);}
\description
    While fitting pulse arrival times a reference pulse
    number may be used. The reference pulse is the
    first one of the dataset, but while fitting it is
    by default shifted to the pulse nearest to the
    fixed reference time 'tpuls0' of the pulse ephe-
    meris. With this function this shift can be turned
    off (boolean=0) or on (boolean=1, default).
\seealso{arrtimes, atime_useref, atime_det}
\done

\function{atime_sim}
\synopsis{simulates pulse arrival times}
\usage{Struct_Type atime_sim([Integer_Type n,] Double_Type tmin, tmax, Struct_Type eph[, orb]);}
\qualifiers{
    \qualifier{scramble}{1 sigma noise of the simulated arrival
               times in seconds (default = 0)}
    \qualifier{dn}{the increment of the arrival times if
               they are not randomly generated
               (default = 1)}
}
\description
    Simulates the orbit and the pulsation of a pulsar
    and returns the pulse arrival times in the time
    range given by 'tmin' and 'tmax'. If the number
    of arrival times 'n' is given, then n randomly
    selected arrival times are returned. The returned
    structure is equal to the atime_det function.
    The uncertainties of the simulated arrival times
    are calculated using the 'scramble' value. If no
    value is given, the uncertainties are set to 0.1%
    of the pulse period to avoid ISIS from treating
    the arrival times as counts, resulting in poisson
    errors.
    The structures describing the pulse ephemeris and
    the orbit must fullfil the conditions described
    in the function check_pulseperiod_orbit_struct.

 NOTE
    Due to speed reasons the returned arrival times
    might be beyond the given time interval or not
    all pulses in this interval are returned.
\seealso{atime_det, check_pulseperiod_orbit_struct, arrtimes}
\done

\function{atime_sim_orbitimpact}
\synopsis{determines the impact of uncertainties of orbital
  parameters on the pulse ephemeris}
\usage{atime_sim_orbitimpact(Double_Type tmin, tmax, Struct_Type eph, orb, dorb);}
\qualifiers{
    \qualifier{tpd}{number of simulated arrival times per day
            (default: 10)}
    \qualifier{pars}{parameters of the pulse ephemeris which
            are used to fit the simulated data
            (default: ["ppuls", "pdot", "p2dot",
                       "p3dot"])}
    \qualifier{range}{allowed fitting ranges of the parameters
            given by the 'pars' qualifier in accordant
            units. The ranges have to be given as an
            array of alternate min/max values for each
            parameter in 'pars'}
}
\description
    An uncertainty of an orbital parameter may lead
    to a wrong pulse ephemeris after a successful
    fit of pulse arrival times. This function deter-
    mines the impact of uncertainties on the pulse
    ephemeris by simulating arrival times in the
    range 'tmin' and 'tmax' and fitting them within
    the given orbital uncertainties. The returned
    structure itself contains structures for each
    orbital parameter, which contain the fitted
    pulse ephemeris and therefore the impact of this
    parameter. The highest phase shift left in the
    residuals is also returned for each orbital
    parameter.
    The given orbital and pulse ephemeris structure
    must fullfil the conditions described in the
    check_pulseperiod_orbit_struct function. The uncertainties of
    the parameters are itself given by a structure,
    which may either contain double types for the
    errors or arrays with two elements for the lower
    and upper errors.
\seealso{atime_sim, check_pulseperiod_orbit_struct, arrtimes}
\done

\function{atime_sim_residuals}
\synopsis{simulates arrival times to calculate how the
  residuals depend on parameter variations}
\usage{atime_sim_residuals(Double_Type tmin, tmax, Struct_Type eph, orb, String_Type psfile);}
\qualifiers{
    \qualifier{n}{number of variations for each parameter
           and sign (-> 2*n variatons, default: 3)}
    \qualifier{tpd}{number of simulated arrival times per day
           (default: 10)}
    \qualifier{mres}{maximum allowed residuals (=yrange) in
           units of pulse phase (default: 0.45),
           must be in the range of 0.0-0.45}
    \qualifier{minr}{smallest allowed relative variation of a
           parameter, used to determine the overall
           variation of this parameter to produce
           nice plots (default: 1e-10)}
    \qualifier{pars}{only simulate this given parameters
           (default: all except tpuls0 and pporb)}
}
\description
    This function determines the dependency of the
    residuals on the given orbit and pulse ephemeris.
    Therefore pulse arrival times are simulated and
    the fit parameters are variied in the way, that
    the shape of the resulting residuals can be seen
    well. The resulting plots are stored in a Post-
    Script file specified by the filename 'psfile'.
    The labels show the absolute variation of the
    the parameter.
    The simulated times will be in the range of
    'tmin' to 'tmax' (in MJD). The orbital parameter
    and the pulse ephemeris structure must be in the
    same form described in the function
    check_pulseperiod_orbit_struct.
\seealso{atime_sim, check_pulseperiod_orbit_struct, arrtimes}
\done

\function{atime_useref}
\synopsis{switches the use of a reference pulse number}
\usage{atime_useref(Integer_Type[] dataid[, Integer_Type boolean]);}
\description
    While fitting pulse arrival times the pulse number
    of each pulse must be determined. By using the
    atime_det function to determine the arrival times
    each pulse gets a pulse number relative to the
    first pulse found in the input lightcurve. Once
    the pulse number of this reference pulse is deter-
    mined during a fit, the numbers of all other pulses
    are also set. Using this function the use of rela-
    tive pulse numbers during a fit can be turned off
    (boolean=0) or on (boolean=1, default).
\seealso{arrtimes, atime_det, atime_shiftref}
\done

\function{atime_xinclude}
\synopsis{Specifies the datasets, which should be taken into account in the fit}
\usage{atime_xinclude(Integer_Type[] index);}
\description
    During the fit only the given datasets containing
    arrival times are taken into account. The remain-
    ing ones are noticed such that no bins are used.
    Hence the fit function and parameters are not
    affected by changed dataset indices.
\seealso{xnotice_atime, define_atime}
\done

\function{atom_name}
\synopsis{DEPRECATED}
\usage{String_Type atom_name(Integer_Type Z)}
\description
 This function returns the symbol for atoms with proton number Z.
 This function is DEPRECATED, please use element_symbol.
\seealso{element_symbol}
\done

\function{attitude_lissajous_pattern}
\synopsis{creates an Attitude File for a Lissajous pattern}
\usage{attitude_lissajous_pattern(prefix,expos,ra,dec)}
\done

\function{AVLTree}
\synopsis{Creates a new balanced binary search tree}
\usage{Struct_Type=AVLTree();}
\altusage{Struct_Type=AVLTree(cmpfunction);}
\description
A binary search tree is an object that permits fast
searches and ordered operations on ordinal data types (i.e.,
data that can be sorted).

An AVLTree is a balanced binary search tree, that is, a search
tree with a structure that ensures that searches will always
be close to O(log(N)), where N is the number of elements in the
tree.

The interface of AVLTree is identical to that of BinarySearchTree,
see there for a description of the member functions.

\seealso{BinarySearchTree}
\done

\function{AzEl_from_RAdec}
\synopsis{computes horizontal (azimut, elevation) coordinates from a point's (RA, dec) at a time MJD}
\usage{(az, el) = AzEl_from_RAdec(RA, dec, MJD,  longw, lat);}
\description
    This function is deprecated. Please use equatorial2horizon instead.
\seealso{equatorial2horizon,horizon2equatorial}
\done

\function{backshift}
\synopsis{determines the shift of an array to a reference one}
\usage{Struct_Type backshift(Double_Type[] in, Double_Type[] ref[, Double_Type[] error]);}
 \altusage{Double_Type[] backshift(Double_Type[] in, Double_Type[] ref[, Double_Type[] error]; retarr);}
\qualifiers{
    \qualifier{retarr}{function returns the unshifted array instead
             of the structure defined below}
}
\description
    The function 'shift' rotates an array by a given number
    of indices. The function described here tries to get
    this number back: an input array 'in' is assumed to be
    a shifted version of the reference array 'ref'. The
    shift is then determined by using the fit-function
    'unshift', also taking optional 'error's into acocunt.
    The returned structure, if the 'retarr'-qualifier is
    not set, is defined as follows:
      shift         - shift in number of indices
      shift_conf    - its [lower, upper] confidence limits
      relative      - relative shift between 0 and 1
      relative_conf - its [lower, upper] confidence limits
      redChiSqr     - reduced chi-square value of the fit
\seealso{unshift, shift_intpol, shift}
\done

\function{barycen}
\synopsis{barycenter an event list, FITS extension, or FITS file}
\usage{barytime=barycen(eventlist);}
\altusage{barycen(String_Type fitsfile);}
\altusage{barycen(FITS_File_Type fptr);}
\qualifiers{
\qualifier{debug}{if set, print debugging information}
\qualifier{ephemeris}{pointer to or name of a FITS ephemeris file (default: DE430)}
\qualifier{orbit}{mandatory, structure containing the orbit information, in
                    geocentric ICRS coordinates and velocities in tags
                    x,y,z,vx,vy,vz in units of m and m/s, and time in tag mjd}
\qualifier{deltat}{if set and the routine barycenters an event list or times, return the
                     difference between the barycentered time and the normal time}
\qualifier{ra}{right ascension (ICRS) of the source [rad]}
\qualifier{dec}{declination (ICRS) of the source [rad]}
\qualifier{deg}{if set, ra/dec are in degrees}
}
\description
 This routine barycenters an event list, a FITS extension that is compatible
 with this routine, or all FITS extensions in a FITS file that are compatible
 with this routine.

 Caveat emptor: Right now the function assumes that all times are in TT and
 coordinates are in the ICRS system, i.e., it does not know that certain
 JPL ephemerides are NOT in this coordinate system.

\seealso{jpl_eph}

\done

\function{bayesian_blocks}
\synopsis{find bayesian blocks in the given data}
\usage{Struct_Type bayesian_blocks(Struct_Type[] data);}
\qualifiers{
    \qualifier{fp_rate}{value of the false positive rate, which is
                the probability that a found change point
                with the current value of ncp_prior is
                actually not significant (default: .01)
                WARNING: This has no influence on data of     
                type 3, where the default ncp_prior, fp_rate
                is 0.05, and it is not possible to change this
                currently.}
    \qualifier{ncp_prior}{controls the tolerance level used to find
                the change points. The value has to be an
                array of estimates for the order of number
                of blocks for each given dataset. The
                probability P that the final number of
                blocks N_blocks is then given by
                  log(P) ~ ncp_prior * N_blocks
                Thus, ncp_prior has to be decreased to
                increase the final number of blocks.
                The default value is determined by a
                formula derived from simulations, given by
                  ncp_prior = 4 - log(73.53*fp_rate*N^-0.478) 
                for data modes 1 and 2 and 
                  ncp_prior = 1.32 + 0.577*log(N) 
                for data mode 3, where N is the number of
                data points.}
    \qualifier{do_iter}{number of iterations on ncp_prior
                (default: 0)}
    \qualifier{dt_min}{events within the given time range are
                considered to be simultaneous and a new
                combined event at the average time is added
                (default: 0)}
    \qualifier{num_skip}{maximal number of consecutive simultaneous
                events (time difference < dt_min)}
    \qualifier{gti}{structure containing the GTIs for the given
                events (data mode 1) with the fields 'start'
                and 'stop'. The blocks are searched for each
                GTI individually, thus the function returns
                an array structures! If GTIs are given only
                ONE input dataset is allowed (at the moment).}
    \qualifier{gti_mindt}{all GTIs shorter than the given duration are
                ignored (i.e. no blocks are searched)
                (default: 0)}
}
\description
    This function is an S-lang implementation of the
    'find_blocks_mult' MatLab-program described in
      Scargle et al., 2013, ApJ 764, 167
    which source code is available on the arXiv.

    WARNING: The full functionality has not been tested
    yet, thus please use with caution!

    Depending on the fields of the input 'data' structure,
    the best Bayesian block representation is determined.
    The available data modes are:
    1: Event data
      Double_Type[] tt        - time tags of events
    2: Binned data (data without specific uncertainties,
       e.g., counts vs. time)
      Double_Type[] tt        - time tags
      Double_Type[] nn_vec    - binned data at tt
WARNING! Mode 2 possibly has bugs! Use with extreme caution!
    3: Point measurements (data with known uncertainties,
       e.g., radio flux vs. time)
      Double_Type[] tt        - time tags
      List_Type[]   cell_data - cell data at tt defined as
                     { Double_Type[] data, uncertainties }

    The output structure contains
      change_points - time INDICES of the change points
      tt_all        - combined time tags of all datasets
      ncp_prior_vec - ncp_prior value of each dataset
      data_matrix   - see Fig. 2 of Scargle et al. 2013
      last          - index of the last change point
                      over tt_all
      best          - fitness (=statistics) over tt_all
      index_vec     - dataset index over tt_all
      data_mode_vec - data mode of each dataset

   The algorithm can handle multiple datasets, which are
   combined internally to one dataset (tt_all). Zero
   blocks are handled as well.

   NOTE: in case of events (data mode 1) it is important
         to provide the GTIs via the corresponing qualifier
         as well. Otherwise the block statistics are
         probably wrong and thus the total block
         represenation!

   To finally retrieve the block represenatation of the
   input data, the 'get_blocks_data' function can be used.
\seealso{get_blocks_data}
\done

\function{bayesian_blocks_cperror}
\synopsis{calculates the probability distribution
    of the change point positions}
\usage{Struct_Type[] bayesian_blocks(Struct_Type blocks);}
\description
    After
      Scargle et al., 2013, ApJ 764, 167
    the uncertainty of the position of a change point in
    a bayesian block representation can be estimated by
    calculating the fitness of a block with variing
    change point position, while all other change points
    are kept fix. Although inter-change-point dependencies
    are neglected in this approach, it can be used to
    derive proper uncertainties as long as the resulting
    distributions do not overlap.

    This function is an implementation of this procedure
    based on the MatLab-programs 'figure_cp_error' and
    'cp_prob' by the authors above.

    The input 'blocks' have to be calculated previously
    by the 'bayesian_blocks' function. The returned
    structure array contains the probability distribution
    'prob' (integral = 1.) over the time-tags 'tt' for
    each change point (i.e., the array index corresponds
    to the change point index). From each individual
    distribution an uncertainty for the location of the
    change point can be estimated via, e.g., its width.
\seealso{bayesian_blocks}
\done

\function{bbootstrap}
\synopsis{1-parameter bootstrap calculation of function 'funct'}
\usage{Array_Type bbootstrap(Array_Type data, Integer_Type num_samp
                             [, funct = &mean, datatype = Double_Type]);}
\qualifiers{
    \qualifier{slow}{the balanced bootstrap is performed. This is
            more accurate, especially for long-tailed
            distributions, but is also much slower and
            requires more memory.}
}
\description
    This function has been ported from the IDL-routine
    bbootstrap.pro by H.T. Freudenreich, HSTX, 2/95

    This program randomly selects values to fill sets of
    the same size as 'data'. Then calculates the 'funct'
    of each, which is an optional function reference with
    the &mean as default. It does this 'num_samp' times.
    The 'datatype' returned by 'funct' has to be provided
    to initialize the array returned by this function. 
    If the slow-qualifier is set, the balanced bootstrap
    method is used to obtain the samples, hence the name
    'bbootstrap'. This method requires, however, more
    virtual memory, and patience.

    The user should choose 'num_samp' large enough to get
    a good distribution. The sigma of that distribution
    is then the standard deviation of the mean of the
    returned 'funct'-vector.
    For example, if input 'data' is normally distributed
    with a standard deviation of 1.0, the standard
    deviation of the returned 'funct'-vector (by default
    the mean) will be ~1.0/sqrt(N-1), where N is the
    number of values in 'data'.

    WARNING: at least 5 points must be input.
             The more, the better.
\done

\function{behr}
\synopsis{calculates fractional difference hardness ratio thorugh bayesian estimation}
\usage{Double_Type (HR , HR_err_max , HR_err_min) =  behr(Double_Type soft_count, Double_Type hard_count);}
\qualifiers{
  \qualifier{scale_s}{Scaling factor for soft_count;  soft_count = scale_s*soft_count  (default = 1)}
  \qualifier{scale_h}{Scaling factor for hard_count;  hard_count = scale_h*hard_count  (default = 1)}
  \qualifier{back_s}{background counts in the soft band}
  \qualifier{back_h}{background counts in the hard band}
  \qualifier{bkg_scale_s}{Scaling factor for source background to observed background counts in the soft band.
             Expected background in source region is ~ back_s/bkg_scale_s.}
  \qualifier{bkg_scale_h}{Scaling factor for source background to observed background counts in the hard band.
             Expected background in source region is ~ back_h/bkg_scale_h.}
  \qualifier{aprox_n}{= run aproximation which keeps powers of bkg_scale_h^i*bkg_scale_s^j, with i+j <= n}
  \qualifier{significance}{significance of calculated uncertainties in percent (default = 68 per cent)}
  \qualifier{fulloutput}{if present function will additionally return the Grid used (2000 color values ranging from -0.999 to 0.999), and the
             normalized probability distribution}
}
\description
  This function gives a bayesian estimation for the so-called fractional difference hardness ratio according to (H-S)/(H+S),
  assuming that soft_count and hard_count are the *intrinsic* counts.
  Optionally, scaling factors for the indiviudal channels can be set.
  If given, background counts are taken into account presuming a separate measurement of a Poisson background.
  If the background counts are given a scaling factor for the source background has to be provided as well.
  An approximative approach can be used as well, by only keeping up to the n-th power in the expansion in terms of
  the scaling factor of the source background.

  The hardness ratio estimation and the respecitive uncertainties are returned by default. 
  Use qualifier if also the grid used (2000 color values ranging from -0.999 to 0.999), and the
  normalized probability distribution are desired.

  Based on Park et al., 2006, ApJ, 652, 610. Based on original isis-code by Mike Nowak.

\seealso{hardnessratio;}
\done

\function{bezier}
\synopsis{calulates a Bezier curve from given supporting points}
\usage{Vector_Type bez = bezier(List_Type points, Integer_Type nbins);
}
\description
   Bezier calculates the Bezier curve between the first and last
   [x,y] entry of 'points', using the other entries of 'points' as
   supporting points. 'points' must be a List_Type, with each
   element containing the [x,y] values of the corresponding point.
   'nbins' gives the numbers of points used to calculate each part
   of the curve, as well as the length of the returned curve.
   
\qualifiers{
\qualifier{allbez}{if set, all intermediate Bezier curves are
          returned as a List, containing all the Vector_Type Bezier curves.}
\qualifier{conline}{if set, the initial lines connecting the
          points are also returned, i.e., the return value becomes (Vector_Type
          conlines, Vector_Type bez).}
}
   
\seealso{xfig_plot_allbezier, bezNp}
\done

\function{bezNp}
\synopsis{calulates a Bezier curve between vectors}
\usage{Vector_Type bez = bezNp(Vector_Type vectors);
}
\description
   The Bezier curve between each pair of the arrays in 'vectors'
   will be calculated. The 'vectors' arrays needs to be at least two
   entries.
   Returned value 'bez' will be one entry shorter than 'vectors'.
   Main working horse for the bezier function.
   
\seealso{bezier}
\done

\function{bgsubHEXTElc}
\synopsis{calculates a background subtracted HEXTE lightcurve}
\usage{Struct_Type lc = bgsubHEXTElc(Struct_Type srclc, Struct_Type bglc);
\altusage{Struct_Type lc = bgsubHEXTElc(Struct_Type srclc, Struct_Type bglc1, Struct_Type bglc2);}
}
\description
    HEXTE rocking -> Background is not measured simultaneously with source data.
\done

\function{BinaryCor}
\synopsis{Removes the influence of the doublestar motion for circular or eliptical orbits.}
\usage{Array_Type t = BinaryCor(Array_Type OR Double_Type time), time in MJD}
\qualifiers{
\qualifier{asini}{Projected semi-major axis [lt-secs], Mandatory}
\qualifier{porb}{Orbital period at the epoch [days], Mandatory}
\qualifier{eccentricity}{Eccentricity, (0<=e<1)}
\qualifier{omega}{Longitude of periastron [degrees], mandatory}
\qualifier{t0}{epoch for mean longitude of 0 degrees (periastron, MJD)}
\qualifier{t90}{epoch for mean longitude of 90 degrees (MJD)}
\qualifier{pporb}{rate of change of the orbital period (s/s) (default 0)}
\qualifier{limit}{absolute precision of the correction of the  computation (days, default: 1D-6)}
\qualifier{maxiter}{stop Newton-Raphson iteration after maxiter steps if limit is not reached (default: 20)}
}
\description
    (transcribed from IDL-programm BinaryCor.pro)

    For each time, the z-position of the emitting object is computed
    and the time is adjusted accordingly. This is iterated until
    convergence is reached (usually only one iteration is necessary,
    even in high elliptic cases).

    Follows equations from Hilditch's book and has also been
    checked against fasebin/axBary. All codes give identical results,
    (to better than 1d-7s) as checked by a Monte Carlo search using
    1d7 different orbits.

    qualifiers t90 and t0 have to be in days and in the same time
    system as time (e.g. JD or MJD)

    Circular orbits:
         * if time of lower conjunction Tlow is known, set
           t0=Tlow and omega=0
         * if time of ascending node is known, Tasc, set
           t90=Tasc and omega=0
         * if time of mid eclipse is known, Tecl, set
           t0=Tecl-0.25*porb and omega=0
\done

\function{binarydigits}
\synopsis{retrieves a number's dual representaion}
\usage{String_Type binarydigits(Integer_Type x)}
\qualifiers{
\qualifier{n}{[\code{=16}] number of bits}
}
\done

\function{BinaryPos}
\synopsis{computes the position of a star in a binary system}
\usage{Double_Type[] BinaryPos(Double_Type[] time)}
\qualifiers{
\qualifier{porb}{Orbital period at the epoch [same units as time argument], mandatory}
\qualifier{eccentricity}{Eccentricity, (0<=e<1)}
\qualifier{omega}{Longitude of periastron [degrees], mandatory}
\qualifier{asini}{Projected semi-major axis (a sin i), mandatory for spectroscopic binaries, 
                    give it in light seconds (a sini/c) if you want to compute a time correction}
\qualifier{semi}{semimajor axis, mandatory for visual binaries}
\qualifier{bigomega}{Longitude of the ascending node [degrees], mandatory for visual binaries}
\qualifier{incl}{inclination [degrees], mandatory for visual binaries}
\qualifier{t0}{epoch for mean longitude of 0 degrees (periastron, MJD)}
\qualifier{t90}{epoch for mean longitude of 90 degrees (MJD)}
\qualifier{pporb}{rate of change of the orbital period (s/s) (default 0)}
}
\description
    (transcribed from IDL-programm BinaryPos.pro)

    For spectroscopic binaries, the z-position of the emitting object
    is computed (negative: closer to Earth). In case of visual binaries
    the output is a 3d array containing the xyz position of the object
    in a right handed coordinate system where the z axis points away from
    Earth, the x axis points towards the North Celestial Pole, and where
    the y-axis increases towards East.

    Follows equations from Hilditch's book.

    Qualifiers t90 and t0 have to be in days and in the same time
    system as time (e.g. JD or MJD)

    Circular orbits:
         * if time of lower conjunction Tlow is known, set
           t0=Tlow and omega=0
         * if time of ascending node is known, Tasc, set
           t90=Tasc and omega=0
         * if time of mid eclipse is known, Tecl, set
           t0=Tecl-0.25*porb and omega=0
\done

\function{BinarySearchTree}
\synopsis{Creates a new binary search tree}
\usage{Struct_Type=BinarySearchTree();}
\altusage{Struct_Type=BinarySearchTree(cmpfunction);}
\description
A binary search tree is an object that permits fast
searches and ordered operations on ordinal data types (i.e.,
data that can be sorted). 

The tree consists of nodes which are structs containing
the data (or "key") in tag "data" and references to nodes with
keys with values less than data in child[0] and to keys with
values larger than data in child[1]. In general, users of the
data structure should not worry about the structure of the
nodes and use the functions described below for all tree
operations.

Searches in a binary search tree can be fast (O(log N)) if the
search tree is balanced, that is, if there is a similar number
of nodes below each node (if that makes sense...). This is typically
the case if random (unsorted) data are inserted into the tree, and
very much not the case if ordered data are inserted. In this case
the search degenerates to O(N). The isisscripts provide a special
binary search tree called AVLTree that has the same accessor functions
as BinarySearchTree but ensures that the tree is height-balanced (at some
small additional cost for key insertions and deletions).

The BinarySearchTree and the AVLTree provide the following functions:
  insert(key) - insert a data element key into the binary search tree
  delete(key) - remove a data element key from the binary search tree
  exists(key) - return true if the search tree contains key
  search(key) - return the node containing the key (or NULL if
                they key does not exist)
  min         - return the minimum key of the search tree (or a subtree), or
                the node containing the minimum key if the node qualifier
                is set.
  max         - return the maximum key of the search tree (or a subtree), or
                the node containing the maximum key if the node qualifier
                is set.
  successor   - return the successor node to a node (i.e., the node
                containing the next largest data value in the tree).
  predecessor - return the predecessor node to a node (i.e., the node
                containing the next smaller data value in the tree).
  print       - print the structure of the tree
  traversal(&func;quals) - traverse the tree, i.e., execute func for all
                keys of the tree. The function func is called with each
                node N (i.e., the data are found in N.data) and all
                qualifiers given to traversal. The order in which the
                nodes are traversed is defined by the following qualifiers:
                preorder: operate on the node, then on the left children,
                          then on the right children
                postorder: operate first on the left children, then on the
                          right children, then on the node
                inorder (the default): operate on the left children, then
                          on the node, then on the right children
                Note that it is permitted that func modifies the tree (e.g.,
                insert new elements, delete tree elements and so on; it is
                best practice to use a qualifier that specifies the tree to
                let func know about it). Depending on the traversal order
                the modified elements may or may not be operated on as part
                of the traversal.


The default setup of the binary search tree works on all data types
where the operators <, ==, and > have been defined. For other data
types, initialize the tree with a reference to a comparison function
of the style define key_compare(k1,k2) where k1 and k2 are of the
data type contained in the search tree and which returns a negative
number if k1<k2, 0 if k1==k2, and a positive number if k1>k2 (this
is the same definition as the one used by s-lang's sort function or
by the strcmp function).

\example
   % sort some numbers
   % (obviously a better way would be array_sort...)

   define arrapp(N) {
     % append key of N to list
     variable list=qualifier("list");
     list_append(list,N.data);
   }
    
   variable arr=[1,4,6,3,5,7,8,2,9];
   variable t=BinarySearchTree();
   variable i;
   foreach i (arr) {
     t.insert(i);
   }
    
   % print the tree
   t.print();

   % delete element with key 8
   t.delete(8);

   variable ll={};
   t.traversal(&arrapp;list=ll);
   foreach i (ll) {
     print(i);
   }
   % print the minimum and maximum values
   ()=printf("min: %S - max: %S\n",t.min(),t.max());

\example
   % setup a tree for strings
   variable arr=["A","X","D","B","C"];
   variable t=AVLTree(&strcmp);
   variable i;
   foreach i (arr) {
      t.insert(i);
   }
   t.print();
   

\seealso{AVLTree,sort}
\done

\function{binary_search}
\synopsis{searches a value in a sorted list}
\usage{Integer_Type ndx=binary_search(Array_Type arr, Data_Type val);}
\qualifiers{
\qualifier{interval}{do an interval search, return
   ndx such that arr[ndx]<=val<add[ndx+1], or 0 if val<=arr[0] or
   length(arr) if add[-1]<=val, see description. }
}
\description
  The function performs a binary search for the value val in the array
  arr. All data types where the comparison operators work are allowed
  (i.e., mainly all numerical types).

  Values in arr MUST be unique and sorted. This is not checked.
  The binary search is much faster than, e.g., a call to
  where, since the where function is O(n), while binary_search
  scales is O(lb(N)) where lb is the logarithm to base 2.

  If no qualifiers are set, the function returns the index of the array
  element if val is found. The function returns -1 if the value is not found.

  If the interval qualifier is set, then the function will do an interval
  search such that arr[ndx]<=value<arr[ndx]. The function will return
  0 if value<=arr[0] and length(arr) if value>arr[-1]. This can be
  used to search the index where one would insert a number into
  an array.
\done

\function{bin_average}
\synopsis{computes averages in histogram bins}
\usage{Struct_Type bin_average(phi, rate, phi_lo[, phi_hi])}
\qualifiers{
\qualifier{err}{error on rate}
\qualifier{quantiles}{array of quantiles to calculate for the distribution}
\qualifier{quartiles}{calculate the .25, .5 and .75 quantiles}
}
\description
    The fields of the returned structure are:\n
    - \code{bin_lo} and \code{bin_hi}, defining the bins\n
    - \code{value}: the average\n
    - \code{err}: the standard error of the mean\n
    - \code{n}: the number of points in each bin\n
    and, if the \code{err} qualifier specifies an error array:\n
    - \code{weighted_average}: an error-weighted average
\done

\function{bknpowerlaw_xyfit}
\synopsis{linear xy fit function to be used with xyfit_fun}
\usage{xyfit_fun ("bknpowerlaw");}
\description
    This function is not meant to be called directly!
    
    Calling \code{xyfit_fun ("bknpowerlaw");} sets up a powerlaw fit
    function for xy-data. It has the form \code{y = norm*x^{-index}}
\seealso{xyfit_fun, define_xydata, plot_xyfit, linear_regression}
\done

\function{blocks_between_gaps}
\synopsis{retrieves blocks in an array whose elements may contain gaps}
\usage{Integer_Type blocks[] = blocks_between_gaps(Double_Type a[], Double_Type gap);}
\description
    The array a has to contain monotonically increasing values.
    The return value will be an integer array of the same length,
    whose elements start with 0 and increase by 1 whenever the difference
    of sequential elements of the array a is larger than gap.
\seealso{split_struct, split_lc_at_gaps}
\done

\function{BoundingBox}
\synopsis{retrieves the Bounding Box of a postscript file}
\usage{Double_Type (x1, y1, x2, y2) = BoundingBox(String_Type psfile);
\altusage{Double_Type (w, h) = BoundingBox(String_Type psfile; size);}
}
\description
    \code{x1, y1, x2, y2} are the values from the last line in \code{psfile}
    of the form \code{"%%BoundingBox: x1 y1 x2 y2"}.
    \code{x1, y1, x2, y2} are \code{NULL} if no such line is found.
\qualifiers{
\qualifier{first}{Take the first matching %%BoundingBox line instead of the last.}
\qualifier{size}{If this qualifier is set, \code{w = x2-x1} and \code{h = y2-y1} are returned.}
}
\done

\function{B_lambda}
\synopsis{Calculates the spectral density for black body radiation (wavelength space)}
\usage{Double_Type Blambda = B_lambda(lambda,T);}

\qualifiers{
\qualifier{keV}{T argument is kT in keV}
\qualifier{A}{Wavelength is given in Angstroms}
\qualifier{Angstrom}{Wavelength is given in Angstroms}
}
\description
    This function returns the spectral energy density of a black body,
    in units erg/cm^2/s/cm/sr 
    Either lambda or T can be an array.
    
\done

\function{B_nu}
\synopsis{Calculates the spectral density for black body radiation (frequency space)}
\usage{Double_Type Bnu = B_nu(nu,T);}

\qualifiers{
\qualifier{keV}{1st argument is energy in keV,
                  T argument is kT in keV}
\qualifier{MHz}{Frequency is given in MHz}
\qualifier{GHz}{Frequency is given in GHz}
}
\description
    This function returns the spectral energy density of a black body,
    in units erg/cm^2/s/Hz/sr 
    Either nu or T can be an array.
    
\done

\function{cart2cyl}
\synopsis{Convert Cartesian to cylindrical coordinates}
\usage{cart2cyl(Double_Types x[], y[], z[], vx[], vy[], vz[]);}
\description
    Convert Cartesian (x,y,z,vx,vy,vz) to cylindrical (r,phi,z,vr,vphi,vz)
    coordinates with phi rotating counter-clockwise from the positive x-axis.
\example
    (r,phi,z,vr,vphi,vz) = cart2cyl(1,1,1,2,3,4);
    (r,phi,z,vr,vphi,vz) = cart2cyl([1,0],[1,0],[1,0],[2,0],[3,0],[4,0]);
    (r,phi,z,vr,vphi,vz) = cart2cyl( cyl2cart(1,PI/2,1,2,3,4) );
\seealso{cart2sphere, cyl2cart}
\done

\function{cart2sphere}
\synopsis{Convert Cartesian to spherical coordinates}
\usage{cart2sphere(Double_Types x[], y[], z[], vx[], vy[], vz[]);}
\description
    Convert Cartesian (x,y,z,vx,vy,vz) to spherical (r,phi,theta,vr,vphi,vtheta)
    coordinates with polar angle phi rotating counter-clockwise from the positive
    x-axis and azimuth angle theta being zero for the positive z-axis.
\example
    (r,phi,theta,vr,vphi,vtheta) = cart2sphere(0,0,0,1,2,3);
    (r,phi,theta,vr,vphi,vtheta) = cart2sphere(0,0,1,1,2,3);
    (r,phi,theta,vr,vphi,vtheta) = cart2sphere(1,1,1,3,3,3);
    (r,phi,theta,vr,vphi,vtheta) = cart2sphere(1,0,0,0,2,1);
    (r,phi,theta,vr,vphi,vtheta) = cart2sphere([0,1],[0,0],[0,0],[1,0],[2,2],[3,1]);
\seealso{cart2cyl}
\done

\function{CCF_1d}
\synopsis{calculates the cross correlation function of two (1d) arrays}
\usage{Double_Type CCF = CCF_1d(a1, a2, Integer_Type dx);
\altusage{Array_Type CCF = CCF_1d(a1, a2);}
}
\qualifiers{
\qualifier{notperiodic}{arrays are not extended periodically.}
}
\description
    The arrays \code{a1} and \code{a2} have to have the same length \code{n}.
    They can be passed directly or as references to arrays.

    By default (unless the \code{notperiodic} qualifier is given), both arrays
    are considered to be extended with periodic boundary conditions.
    When using \code{notperiodic}, chose \code{|dx| < n-1}, since the larger \code{|dx|},
    the smaller are the parts of the arrays that can actually be compared.

    This function computes the correlation of \code{a1[x]} and \code{a2[x-dx]}:
       \code{CCF = sum_x { (a1[x]-<a1>)/|a1| * (a2[x-dx]-<a2>)/|a2| }}
    For periodic boundary conditions, the entire array is considered,
    else the index \code{x} takes only values such that \code{x} and \code{x-dx} are contained.
    The norm \code{|a| = sqrt{ sum_x a[x]^2 }} ensures that for any array \code{a},
       \code{CCF_1d(a, a, 0) = +1}   and   \code{CCF_1d(a, -a, 0) = -1}.\n
    \n
    If only the two arrays \code{a1} and \code{a2} are given, the function returns
    an array with all possible correlations between them, i.e.:
    \code{[ CCF_1d(a1, a2, 0), CCF_1d(a1, a2, 1), ..., CCF_1d(a1, a2, n-1) ]}
    This is most reasonable for periodic boundary conditions.
\notes
    The function is vectorized on \code{dx}, i.e., can handle an array \code{dx}.
\example
    variable a1 = Double_Type[n];  % some data
    variable a2 = Double_Type[n];  % some other data

    variable dx = [-n/2 : n/2];
    variable CCF_np = CCF_1d(a1, a2, dx; notperiodic);

    variable CCF_p = CCF_1d(a1, a2);  % This is equivalent to:
    variable CCF_p = array_map(Double_Type, &CCF_1d, &a1, &a2, [0:n-1]);
\seealso{CCF_2d}
\done

\function{CCF_2d}
\synopsis{calculates the cross correlation function of two 2d-arrays (e.g., images)}
\usage{Double_Type CCF = CCF_2d(img1, img2, Integer_Type dx, Integer_Type dy);}
\description
     The 2d-arrays img1 and img2 from which the CCF is to be computed can
     either be passed directly or by a reference to the arrays.
     This function computes the correlation of \code{img1[y, x]} and \code{img2[y-dy, x-dx]},
     where the indices (x,y) take all values such that (x,y) and (x-dx,y-dy) are contained:
        \code{CCF = sum_{x,y} { (img1[y,x]-<img1>)/|img1| * (img2[y-dy,x-dx]-<img2>)/|img2| }}
     The norm \code{|img| = sqrt{ sum_{x,y} img[y,x]^2 }} ensures that \code{CCF_2d(img, +-img, 0, 0) = +-1}.
     for any image \code{img}.\n
     If \code{dx} or \code{dy} are arrays, \code{CCF} will be a 2d array.
\qualifiers{
\qualifier{verbose}{turns on verbosity}
\qualifier{savebest}{=&best: if \code{dx} or \code{dy} are arrays, the best combination will be saved in \code{best}.}
}
\seealso{CCF_1d}
\done

\function{cel2gal}
\synopsis{Transform celestial to Cartesian Galactic coordinates}
\usage{cel2gal(Double_Types ah[], am[], as[], dd[], dm[], ds[], dist[], vrad[], pma_cos_d[], pmd[]; qualifiers)}
\description
    Transform celestial coordinates (right ascension [h, m, s], declination [deg, arcmin, arcsec],
    distance [kpc], radial velocity [km/s], proper motion in right ascension times cosine of declination
    [mas/yr], proper motion in declination [mas/yr]) to right-handed, Cartesian Galactic coordinates
    (x [kpc], y [kpc], z [kpc], vx [km/s], vy [km/s], vz [km/s]) with the Galactic center at the origin,
    the Sun on the negative x-axis and the z-axis pointing to the north Galactic pole implying clockwise
    Galactic rotation when seen from the half space with positive z. See function 'RD2rad' for detailed
    information on how to properly enter right ascension and declination.
\qualifiers{
\qualifier{parallax}{Set this qualifier to use parallaxes [mas] instead of distances [kpc].}
\qualifier{SunGCDist}{Sun-Galactic center distance [kpc];
      default: 8.4 (see Model I in Irrgang et al., 2013, A&A, 549, A137)}
\qualifier{vxs}{Sun's x-velocity component [km/s] relative to the local standard of rest;
      default: 11.1 (Schoenrich, Binney & Dehnen, 2010: MNRAS 403, 1829)}
\qualifier{vys}{Sun's y-velocity component [km/s] relative to the local standard of rest;
      default: 12.24 (Schoenrich, Binney & Dehnen, 2010: MNRAS 403, 1829)}
\qualifier{vzs}{Sun's z-velocity component [km/s] relative to the local standard of rest;
      default: 7.25 (Schoenrich, Binney & Dehnen, 2010: MNRAS 403, 1829)}
\qualifier{vlsr}{Local standard of rest velocity [km/s];
      default: 242 (see Model I in Irrgang et al., 2013, A&A, 549, A137)}
\qualifier{GC_NGP}{Celestial coordinates of the Galactic center and the north Galactic pole in the
      format [ah_GC, am_GC, as_GC, dd_GC, dm_GC, ds_GC, ah_NGP, am_NGP, as_NGP, dd_NGP, dm_NGP, ds_NGP];
      default: [17, 45, 37.224, -28, 56, 10.23, 12, 51, 26.282, 27, 07, 42.01] (J2000, Reid & Brunthaler, 2004, ApJ, 616, 872)}
}
\example
    (x, y, z, vx, vy, vz) = cel2gal(0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
    % -> Sun's position and velocity
    (x, y, z, vx, vy, vz) = cel2gal(17, 45, 37.224, -28, 56, 10.23, 8.4, -11.1, -3.171, -5.544);
    % -> Galactic center's position and velocity
    (x, y, z, vx, vy, vz) = cel2gal([0,17], [0,45], [0,37.224], [0,-28], [0,56], [0,10.23], [0,8.4], [0,-11.1], [0,-3.171], [0,-5.544]);
    % -> position and velocity of Sun and Galactic center stored in the same array
    (x, y, z, vx, vy, vz) = cel2gal(17, 45, 37.224, -28, 56, 10.23, 8.4, -11.1, -3.171, -5.544;
    GC_NGP=[17, 45, 37.1991, -28, 56, 10.221, 12, 51, 26.2755, 27, 7, 41.704]);
    % -> use non-standard coordinates for Galactic center and north Galactic pole
    (x, y, z, vx, vy, vz) = cel2gal( gal2cel(0, 0, 0, 0, 0, 0) );
    % compute vr and vphi:
    vr = (x*vx+y*vy)/sqrt(x^2+y^2);
    vphi = (y*vx-x*vy)/sqrt(x^2+y^2);
\seealso{RD2rad, gal2cel}
\done

\function{cerf}
\synopsis{Complex error function}
\usage{Complex_Type[] = cerf(Complex_Type[]);}
\description
    Compute complex error function. Only useful in a region
    with abs(z)<10.
\seealso{Faddeeva, cerfc}
\done

\function{cerfc}
\synopsis{Complex error function complement}
\usage{Complex_Type[] = cerfc(Complex_Type[]);}
\description
    Compute complex error function complement. Only useful in a region
    with abs(z)<10.
\seealso{Faddeeva, cerf}
\done

\function{Chandra_display_mask}
\synopsis{shows the content of a Chandra mask file (msk1.fits)}
\usage{Chandra_display_mask([String_Type filename]);}
\description
    \code{filename} can be a globbing expression.
    If it is omitted, \code{acis*msk1.fits*} is assumed.

    The mask file contains the valid part of the CCD,
    i.e., the portion for which events can be telemetered.
\done

\function{change_plot_options}
\synopsis{changes the currently used plot options}
\usage{Struct_Type change_plot_options([plot_opt]; [default,] opt=value, opt2=value2, ...);}
\description
    The current plot options will be used as a starting point unless
    an explicit argument \code{plot_opt} is specified, which is used in this case.

    These plot options are then modified by the \code{opt} qualifiers (\code{opt} can be
    any fieldname of the structure returned by \code{get_plot_options}, i.e.,
    \code{xmin}, \code{xmax}, \code{ymin}, \code{ymax}, \code{xlabel}, \code{ylabel}, \code{tlabel}, \code{xopt}, \code{yopt}, \code{logx}, \code{logy},
    \code{color}, \code{start_color}, \code{x_unit}, \code{line_style}, \code{start_line_style}, \code{line_width},
    \code{frame_line_width}, \code{point_style}, \code{connect_points}, \code{char_height}, \code{point_size},
    \code{ebar_term_length}, \code{use_errorbars}, \code{use_bin_density},
    \code{ovp_xmin}, \code{ovp_xmax}, \code{ovp_ymin}, \code{ovp_ymax}),
    unless the qualifier \code{default} is specified. In this latter case,
    default plot options are set regardless of the starting plot options
    and the other qualifiers.

    The return value is the \code{get_plot_options} structure before \code{change_plot_options}
    was called. It can be used to reset the plot options after a change was applied.
\example
    \code{variable plot_options = change_plot_options(; line_style=2);}\n
    \code{plot(lineX, lineY);}\n
    \code{set_plot_options(plot_options);}\n
\seealso{get_plot_options, set_plot_options}
\done

\function{chebyshev_lagrange_weights}
\synopsis{Get Lagrange weights for the chebyshev_nodes}
\usage{Double_Type[] chebyshev_lagrange_weights(Int_Type n);}
\qualifiers{
  \qualifier{second}{if given, return weights of second order polynomal.}
}
\description
  Given an integer \code{n} this function returns the weights for the
  barycentric Lagrange polynomial for nodes placed the the Chebyshev nodes.

  In case the \code{second} qualifier is given, the second kind Chebyshev
  polynomial defines the weights, else the first kind.
\seealso{chebyshev_nodes,lagrange_weights,lagrange_poly}
\done

\function{chebyshev_nodes}
\synopsis{Get nodes of Chebyshev polynomial}
\usage{Double_Type[] nodes = chebyshev_nodes(Int_Type n);}
\qualifiers{
  \qualifier{second}{if given, return nodes of second order polynomal.}
  \qualifier{min}{[=-1] rescale to min}
  \qualifier{max}{[=1] rescale to max}
}
\description
  Given an integer \code{n} this function returns the nodes
  of the Chebyshev polynomial of the first kind of order \code{n}.

  Or, if the \code{second} qualifier is given, the corresponding nodes
  of the second kind polynomial.

  The nodes are distributed between -1 and 1 per default. If the \code{min}
  and \code{max} qualifiers are given the nodes are rescaled accordingly.
\seealso{chebyshev_lagrange_weights}
\done

\function{chebyshev_poly}
\synopsis{Returns the Chebyshev polynomial T(n,x) of the first kind of order n}
\usage{Double_Type[] chebyshev_poly(Double_Type[] x, n)}
\description
    Evaluates the Chebyshev Polynomial of the first kind of order n
    via the recurrence relation
    
    T(0,x) = 1
    T(1,x) = x
    T(n,x) = 2*x*T(n-1,x) - T(n-2,x)
\example
    x = [-1.:1.:#100];
    chebyshev_poly(x,n); % returns 5th order Chebyshev polynomial evaluated
                         % on [-1,1)
\seealso{cheby_sum}
\done

\function{cheby_sum}
\synopsis{Returns a weighted sum of Chebyshev polynomials up to a given order}
\usage{Double_Type[] cheby_sum(Double_Type[] [a0,a1,...aN], Double_Type[] x)}
\description
    Return the value of the expression

    a0*T(0,x) + a1*T(1,x) + a2*T(2,x) + ... + aN*T(N,x)

    Where T are Chebyshev polynomials of the first kind of order 1...N.

    This is array-safe in x and implemented to be much more efficient than
    explicitly using multiple calls of chebyshev_poly()!
\example
    x = [-1.:1.:#100];
    cheby_sum(x,[0, 1., 2.]); % returns chebyshev_poly(x,1)
                              %    + 2.*chebyshev_poly(x,2)
                         % on [-1,1)
\seealso{chebyshev_poly}
\done

\function{checkrwlc [ff]}
\usage{Struct_Type checkrwlc(time, value, error, bin_lo, bin_hi)
\altusage{Struct_Type checkrwlc (time, value, bin_lo, bin_hi) % in which case resu.erro will be zero}
}
\description
    This script implements the alogrithm proposed by M. de Kool et al. (1993)
    to check for random walk in a pulse period evolution.
    In principle, everything containing time and value information
    can be checked for random walk with this script (i.e. a lightcurve).
    The output of the script is a structure containing the fields\n
      - bin_lo = lower delta t bin\n
      - bin_hi = upper delta t bin\n
      - value  = delta omega value\n
      - erro = uncertainties calculated from the given uncertainties of the values\n
      - dist  = not normalized delta omega value\n
      - len = normalization facotr for delta omega (value = dist/len)

    Care should be taken that, when using for period evolution, the period and the
    time array should be given in the same units.

    The code might not be perfectly optimized
    and is getting slow for large arrays of time and value (>500).

    To estimate the true errors of the result, a Monte Carlo simulation should be used.
\qualifiers{
\qualifier{grp_r}{[Double]: grouping parameter as defined in Eq. 5, de Kool (1993) (default 0.1)}
\qualifier{verbose}{[Boolean]: print progress in form of current working timebin (default: false)}
}
\seealso{M. de Kool and U. Anzer, 1993, MNRAS, 262, 726}
\done

\function{check_pulseperiod_orbit_struct}
\synopsis{checks if the pulse period- and/or orbit-structure are valid}
\usage{Integer_Type check_pulseperiod_orbit_struct(Struct_Type structure);
 or Struct_Type check_pulseperiod_orbit_struct(Double_Type pulseperiod);}
\description
    This function performs a checks whether the given
    structure matches one of the following definitions
    of a pulse period- or orbital parameter structure:

    1) pulseperiod struct { % given as period over time
        Double_Type[] time, % in MJD
        Double_Type[] period % in seconds
       }

    2) pulseperiod struct { % given as taylor coefficients
        Double_Type t0, % reference time in MJD
        Double_Type p0[, % pulse period at t0 in seconds
        Double_Type pdot[, % first derivative in s/s
        Double_Type p2dot[, % second derivative in s/s^2
        ...
        Double_Type pNdot]]] % higher orders
       }

    3) orbit struct {
        Double_Type tau or t90, % time of periastron passage
                    % or mean longitude of 90 degrees in MJD
        Double_Type porb, % orbital period in days
        Double_Type asini, % projected semi-major axis in lt-s
        Double_Type ecc[, % eccentricity
        Double_Type omega] % longitude of periastron in degrees
                                          % required if ecc > 0     
       }

    On a successful match the number of the definition
    (1-3) is returned, 0 otherwise.
    Note that additional field names are allowed.
    
    If an input pulse period (as a number) is given
    instead a structure, a new pulse period structure
    (type 2) is returned with t0 set to 0 and the
    pulse period as given.

    Note that units cannot checked by this function, but
    are a suggestion. Read the help of any function using
    the same definition on their required units.
\seealso{pulseperiod}
\done

\function{cholesky_decomposition}
\synopsis{Decompose a matrix into the product of a lower triangular matrix and its transpose}
\usage{Double_Type cd[n,n] = cholesky_decomposition(Double_Type m[n,n])}
\description
    The Cholesky decomposition is a decomposition of a symmetric, positive-definite
    matrix into the (matrix) product of a lower triangular matrix and its transpose:
      m = cd # transpose(cd).
    The CholeskyBanachiewicz algorithm is used here to carry out the decomposition.
\notes
    The Cholesky decomposition can be used to generate correlated variables that obey
    a given covariance matrix. See the example section for details.
\example
    m = Double_Type[3,3]; m[0,*] = [ 9,-3,-6]; m[1,*] = [-3,10, 5]; m[2,*] = [-6, 5, 6];
    print(m);                       % symmetric, positive-definite matrix
    cd = cholesky_decomposition(m);
    print(cd);                      % lower triangular matrix
    print(cd#transpose(cd));        % matrix product of 'cd' and its transpose

    % From three uncorrelated normal variables 'g', generate three correlated normal
    % variables 'x' that obey the covariance matrix 'm':

    g = Double_Type[3,100000];
    g[0,*] = grand(100000); g[1,*] = grand(100000); g[2,*] = grand(100000);
    (cov_mat, cor_mat) = covariance_correlation_matrix(g);
    print(cov_mat); % the covariance matrix of 'g' shows that there are no correlations

    x = cd#g;
    (cov_mat, cor_mat) = covariance_correlation_matrix(x);
    print(cov_mat); % the covariance matrix of 'x' is indeed (almost) 'm'
\seealso{covariance_correlation_matrix}
\done

\function{circle_from_points}
\synopsis{find the best fitting circle for a set of points}
\usage{pars=circle_from_points(Array_Type x,Array_Type y);
}
\description
    Given arrays of x/y coordinates of points which lie approximately on
    a circle, this function returns the center coordinate and radius of
    the circle. This is a numerically unstable procedure, but the
    routine generally returns parameters which are good enough to start
    a more advanced fitting routine.
    The function is an implementation of ideas described by L. Maisonobe
    in a document entitled "Finding the circle that best fits a set of
    points" (see http://www.spaceroots.org/downloads.html): For each
    tuple of three points of  coordinates it finds the center
    and radius, which is possible in an exact manner, assuming that
    the points are not on a straight line. This is done for all or a
    subset of the points, and the resulting radius and center coordinates
    are then averaged.
    The function returns a structure with the tags xc and yc (x- and y-
    coordinate of the center of the circle) and radius (radius of the circle),
    or NULL if no solution could be found (points are on a straight line).

 \qualifiers{
    \qualifier{eps}{if the determinant of the three point solution is less than
                      than this number, the points lie on a straight line}    
    \qualifier{maxiter}{average at most maxiter tuples of three points}
}
\done

\function{clear_all}
\synopsis{Deletes data & corresponding ARFs and RMFs}
\usage{clear_all();}
\qualifiers{
\qualifier{noprompt}{avoid the prompt in scripts}
}
\description
    Deletes all data, the corresponding ARFs and RMFS, as well
    as the intrinsic correction factors used for Fermi spectra.
    This function removes the indicated spectra from the internal
    list; it does not affect the disk files containing the spectra.


\seealso{delete_data}
\done

\function{clip_points_polygon}
\synopsis{Clip points against a polygon}
\usage{(xc,yc)=clip_points_polygon(x,y,xp,yp)}
\altusage{clipped=cohen_sutherland(points,poly)}
\qualifiers{
 \qualifier{evenodd}{use the even-odd method to determine }
 \qualifier{crossing}{use the crossing number method}
 \qualifier{winding}{use the winding number method (the default)}
}

\description
 This function clips points defined by (x,y), where x and y can be arrays,
 against a closed polygon defined by the arrays (xp, yp) and returns the
 clipped points (xc,yc) where xc, yc are arrays (which can be empty
 if all points are outside of the polygon). Here, a closed polygon
 means that xp[0]==xp[-1] and yp[0]==yp[-1].

 Alternatively, the points and polygon can be defined as structs,
 where points=struct{ x=[], y=[] } and where the polygon is defined
 as poly=struct{x=xp,y=yp}

 The qualifiers define what to consider the "inside" of the polygon.

\seealso{clip_points_rectangle, greiner_hormann, point_in_polygon}

\done

\function{clip_points_rectangle}
\synopsis{Clip points against a rectangle}
\usage{(xc,yc)=clip_points_rectangle(x,y,xmin,ymin,xmax,ymax)}
\altusage{clipped=clip_points_rectangle(points,box)}
\description
This function clips the points defined by (x,y), where x and y can be arrays,
against a rectangle defined by the corner points (xmin,ymin) and (xmax,ymax) and 
returns the clipped points (xc,yc) where xc, yc are arrays (which can be empty
if all points are outside of the rectangle).

Alternatively, the points and clipping rectangle can be defined as structs,
where points=struct{ x=[], y=[] } and where the clipping rectangle is defined
either as box=struct{x=[xmin,xmax],y=[ymin,ymax]}
or as box=struct{xmin=xmin,ymin=ymin,xmax=xmax,ymax=ymax}.


\seealso{clip_points_polygon, clip_polyline_rectangle, cohen_sutherland, greiner_hormann}

\done

\function{clip_polyline_polygon}
\usage{clipped=clip_polyline_polygon(line,polygon)}
\qualifiers{
\qualifier{evenodd}{use the crossing number method}
\qualifier{crossing}{use the crossing number method}
\qualifier{winding}{use the winding number method (the default)}
}
\description
 This function clips a polyline defined by line=struct{x=[],y=[]},
 where x and y are the points connected by the polyline, against
 a closed polygon=struct{x=[],y=[]}, where again x and y are the points
 connected by the polygon and where x[0]==x[-1] and y[0]==y[-1].

 The function returns a list of polylines that contain the segments
 of the line that are inside of the polygon. This list can be empty if
 there is no overlap.

 For complex polygons with intersecting segments, the qualifier defines
 what constitues the inside of the polygon.

 If you want to clip against a simple rectangle, use clip_polyline_rectangle
 for a faster algorithm.

\seealso{point_in_polygon,clip_polyline_rectangle,clip_points_polygon}
\done

\function{clip_polyline_rectangle}
\synopsis{Clip a polyline against a rectangle}
\usage{clipped=clip_polyline_rectangle(poly,box)}
\description
 This function clips the polyline poly=struct{x=[],y=[]}, where x and y are arrays containing
 the points of the polyline, against a rectangle defined by the corner points (xmin,ymin) and
 (xmax,ymax). It returns a list of structs{x,y}, where each list element contains the points of
 a segment of the polygon that is inside the box.

\seealso{clip_points_polygon, clip_polyline_rectangle, cohen_sutherland, greiner_hormann}

\done

\function{close_plot_ps2eps}
\synopsis{closes a plot calls ps2eps}
\usage{close_plot_ps2eps();}
\seealso{close_plot, close_plot_ps2eps}
\done

\function{close_print}
\synopsis{Wrapper around close_plot to allow system function to call the output file (isis_fancy_plots package)}
\usage{close_print(window_id, String_Type);}
\description

   Use as:
   isis> id = open_print("fig1.ps/vcps"); keynote_size; nice_width;
   isis> plot(x,y);
   isis> close_print(id,"gv");
\seealso{open_print, sov, open_plot, close_plot, apj_size, keynote_size, nice_width, pg_color, pg_info}
\done

\function{cl_save}
\synopsis{computs single-parameter confidence lmits for several parameters, using multiple cores on one machine}
\usage{Struct_Type results = cl_save([Integer_Type pars[]]);}
\qualifiers{
\qualifier{strict}{[=1]: restarts the calculation if a new best fit was found}
\qualifier{saveoutput}{[=1]}
\qualifier{basefilename}{[=<date_time>]}
\qualifier{level}{[=1]: specifies the confidence level. Values of 0, 1, or 2
                 indicate 68%, 90%, or 99% confidence levels respectively.
                 By default, 90% confidence limits are computed.}
\qualifier{tolerance}{convergence criterion for the calculation of the confidence
      limits (see help for the conf command). Default: 1e-3}
\qualifier{cleanup}{will remove all temporary files ending in *.[0-9][0-9]* from the basefilename directory}
}
\description
    This function is a direct copy of fit_pars, however, using
    "conf_loop" to allow multi-core support on one machine. The
    number of slaves is determined by the global variable
    Isis_Slaves.num_slaves.
    The return value \code{results = struct { index, name, value, min, max, conf_min, conf_max, buf_below, buf_above, tex }}
    is a table with the following information for each parameter:\n
    \code{min} and \code{max} are the minimum/maximum values allowed.
    \code{conf_min} and \code{conf_max} are the confidence limits.
    \code{buf_below} (\code{buf_above}) is the fraction of the allowed range \code{[min:max]}
    which separates the lower (upper) confidence limit from \code{min} (\code{max}).
    If one of these buffers is 0, your confidence interval has bounced.
\seealso{fit_pars, pvm_fit_pars, conf}
\done

\function{cohen_sutherland}
\synopsis{Clip a line against a rectangle}
\usage{(xc0,yc0,xc1,yc1)=cohen_sutherland(x0,y0,x1,y1,xmin,ymin,xmax,ymax)}
\altusage{clipped=cohen_sutherland(line,box)}
\description
 This function clips a line defined by the points (x0,y0) and (x1,y1) against
 a box defined by the corner points (xmin,ymin) and (xmax,ymax) and returns
 the clipped line (xc0,yc0) -- (xc1,yc1). The clipped coordinates are set
 to _NaN if the line misses the box.

 The line segment and clipping box can be either defined directly by giving the
 coordinates or as structs, In the latter case, the line segment
 is defined as line= struct{ x=[x0,x1], y=[y0,y1] } and the
 rectangular box is defined either as box=struct{x=[xmin,xmax],y=[ymin,ymax]}
 or as box=struct{xmin=xmin,ymin=ymin,xmax=xmax,ymax=ymax}.

\seealso{clip_points_rectangle,clip_points_polygon, clip_polyline_rectangle,greiner_hormann}

\done

\function{colacal}
\synopsis{Timing Tools: Coherence and Lag Calculation}
\usage{ Struct_Type cola = colacal ([10 required inputs]);}
\description
    Input:
      freq     - Fourier Frequency Array
      cpd      - CPD Array
      noicpd   - Poisson Noise Contribution to CPD
      lopsd    - Non-noise-corrected PSD (low channel)
      hipsd    - Non-noise-corrected PSD (high channel)
      noilopsd - Noise-Contribution to low PSD
      noihipsd - Noise-Contribution to high PSD
      siglopsd - Noise-corrected PSD (low channel)
      sighipsd - Noise-corrected PSD (high channel)
      alln     - number of averaged segments in each frequency bin
    Output: Struct containing
      rawcof   - non-noise-corrected coherence function [Vaughan & Nowak, 1997, ApJ, 474, L43 (Eqn. 2)]
      cof      - noise-corrected coherence function [Vaughan & Nowak, 1997, ApJ, 474, L43 (Eqn. 8, Part 1)]
      errcof   - one-sigma uncertainty of cof [Vaughan & Nowak, 1997, ApJ, 474, L43 (Eqn. 8, Part 2)]
      lag      - time lag [Nowak et al., 1999, ApJ, 510, 874 (Sect. 4)]
      errlag   - one-sigma uncertainty of lag [Nowak et al., 1999, ApJ, 510, 874 (Eqn. 16)]
      sigcpd   - noise-corrected cross-power-density [sigcpd = cpd - noicpd]
\done

\function{collect_conf_files}
\synopsis{Collect multiple confidence limit files from fit_pars}
\usage{df = collect_conf_files(conffiles);}
\description
    This function can be used to collect multiple output files from
    fit_pars and produce an easily readable data structure from it.
    It is important that all spectra are fit with the same model.
    The output, explained based on the example of
    tbnew_simple*powerlaw is as follows: 
    
    struct{ tbnew_simple_nh = struct{value = Double_Type[nfiles],
                                     conf_min = Double_Type[nfiles], 
                                     conf_max = Double_Type[nfiles]},
            powerlaw_norm = struct{value = Double_Type[nfiles],
                                   conf_min = ..., conf_max = ...},
            powerlaw_phoindex = ... }
            
    where nfiles is the number of conffiles given and the entries of
    the arrays value, conf_min, conf_max, etc. are the data fields of the
    confidence limit files created by fit_pars. In addition, the
    output contains the conffile names, the reduced chi^2, an array
    of the parameters, and an array of the data field names.
\example
    variable conffiles = glob("*_conf.fits");
    variable df = collect_conf_files(conffiles);
    variable dummy_time = [0:length(conffiles)-1];
    variable P = tiky_plot_new;
    P.plot(dummy_time, df.powerlaw_phoindex.value, {0,0},
         {df.powerlaw_phoindex.conf_min, df.powerlaw_phoindex.conf_max};
         minmax);
    
\seealso{get_struct_field}
\done

\function{color2hex}
\synopsis{converts color string to hex color.}
\usage{Integer_Type color2hex(String_Type color)}
\seealso{hex2color}
\done

\function{color2rgb}
\synopsis{converts a 24 bit color value to r, g, b, values.}
\usage{Integer_Type r, g, b color2rgb(Int/String_Type col)}
\qualifiers{
  \qualifier{float}{If given, the channels are instead returned in the range (0-1).}
}
\description
  Converts a hex color to RGB values. Can be either 24 bit
  value or string.
\seealso{rgb2color}
\done

\function{color_blackbody}
\synopsis{Computes RGB colors of hot blackbody radiation.}
\usage{Integer_Type color = color_blackbody(Double_Type T);}
\description
    \code{T} is the temperature of the blackbody in Kelvin.
    The return value is its 24 bit RGB value.

    It is computed after the code "RGB VALUES FOR HOT OBJECTS"
    by Dan Bruton (astro@sfasu.edu), which can be found at
       http://www.physics.sfasu.edu/astro/color.html.
\done

\function{color_color_data}
\synopsis{extracts light curves in 3 consecutive bands and the corresponding colors from an event list}
\usage{Struct_Type dat = color_color_data(Struct_Type evts, Double_Type E0, E1, E2, E3);}
\qualifiers{
\qualifier{dt}{time resolution [default: 100]}
\qualifier{field}{field used for selection of the bands [default: "pi"]}
}
\description
    \code{dat.a} = lightcurve in "E0 <= field < E1" band\n
    \code{dat.b} = lightcurve in "E1 <= field < E2" band\n
    \code{dat.c} = lightcurve in "E2 <= field < E3" band\n
    \code{dat.}x\code{_}y = x/y color for x, y = a | b | c
\seealso{histogram}
\done

\function{color_complex}
\synopsis{Converts a complex number to a RGB color}
\usage{Integer_Type color = complex2rgb(Complex_Type z);}
\description
    arg(\code{z}) determines the hue,
    |\code{z}| determines the lightness:
    0<=|\code{z}|<=1 is mapped from black to color,
    1<=|\code{z}|<inf is mapped from color to white.
    The return value is a 24-bit RGB value.
\qualifiers{
\qualifier{s[=1.0]}{saturation (from 0 to 1) of the color}
}
\seealso{hsl2rgb, png_write}
\done

\function{color_desaturate}
\synopsis{Weighted color to gray conversion}
\usage{UInt_Type gray = color_desaturate(UInt_Type[] color);}
\description
    Converts a RGB color, hex encoded, into a weighted gray scale using:

    \code{gray} = (0.3 * R) + (0.59 * G) + (0.11 * B)
    
    The 8 bit gray value(s) are return as UInt_Type[] in the interval [0,255].

\seealso{rgb2hsl, color2rgb}
\done

\function{color_mix}
\synopsis{Mix two rgb colors}
\usage{UInt_Type color = color_mix (UInt_Type c1, UInt_Type c2, Double_Type fraction);}
\description
  This function mixes two colors in rgb space. Given the rgb values in the
  usual encoding in one 24bit integer, the color is mixed according to 
  new color  = c*fraction + c2*(1-fraction)
  The operations are analoguous to the color mixing performed by the xcolor
  package of LaTeX (the operation is similar to the color1!fraction!color2
  syntax). 
  The function returns the integer specifying the new rgb values. \code{fraction}
  can be an array. The colors can also be a hex string.
\done

\function{color_wavelength}
\synopsis{Computes the color values of visual light}
\usage{UInt_Type[] color_wavelength (Double_Type[] lambda);}
\description
    \code{380 <= lambda <= 780} is the wavelength in nm.
    The return value is its 24bit RGB color value.

    The conversion is performed after the code by Dan Bruton,
    see http://www.physics.sfasu.edu/astro/color.html.
\done

\function{combine_chain}
\synopsis{Combine several mcmc chains generated with the same configuration}
\usage{combine_chain(Array_Type chaininfiles, String_Type chainoutfile)}
\qualifiers{
\qualifier{verbose}{show progress}
}
\description
       This function takes mcmc chains stored in files passed as an array of strings in chaininfiles,
	written with the write_chain function (or by emcee itself) and combines them to one chain file,
	again writing a fits file.
       This function does not make use of read_chain or write_chain, it's consisting only of
       fits routines. The function itself does perform some sanity and safety checks (same data, same fit_fun)
       but you have to make sure that you use the correct chains in the correct order.
\seealso{read_chain, write_chain, emcee, append_chain}
\done

\function{compare_modelfits}
\synopsis{to compare modelfit parameters of different epochs}
\usage{compare_modelfits([array of \code{epoch fitsfiles}]);}
\qualifiers{
\qualifier{plottype}{[="overlay"] layout of the comparison plot,
                               by default it shows the overlay of all component positions,
                               alternatives are "distancevsmjd","fluxvsdistance","TBvsdistance"}
\qualifier{fileextent}{[=.eps] output is given by default as .eps-file in working directory}
\qualifier{outputfile}{give outputfile name and directory by hand}
\qualifier{sourcename}{[=default] set the name of the source, by default the name is read from the .fits file,\n
                               set to NULL for not plotting a source name}
\qualifier{ra_mas}{[=[20,-20]] ra range (for "overlay")}
\qualifier{dec_mas}{[=[-20,20]] dec range (for "overlay")}
\qualifier{mjd_min}{[=54101.0] lower limit of time axis (default: 1/1/2007)}
\qualifier{mjd_max}{[=55927.0] lower limit of time axis (default: 1/1/2012)}
\qualifier{distance_min}{[=0] lower limit of distance axis in mas}
\qualifier{distance_max}{[=50] upper limit of distance axis in mas}
\qualifier{flux_min}{[=1e-5] lower limit of flux axis in Jy}
\qualifier{flux_max}{[=2] upper limit of flux axis in Jy}
\qualifier{TB_min}{[=10^5] lower limit of brightness temperature axis in K}
\qualifier{TB_max}{[=10^15] upper limit of brightness temperature axis in K}
\qualifier{linestyle}{[=default] set to 0 if the flux values should not be conntected (or to another value for
                               another line style}
\qualifier{ex_comps}{[=0] set to 1 if components should be excluded and define
                               the corresponding quadrant of the plot via the ex_comps-coordinates}
\qualifier{ex_comps_ra}{[=NULL] quadrant coordinates in mas where components should be excluded}
\qualifier{ex_comps_dec}{[=NULL] define quadrant coordinates in mas where components should be excluded}
}
\description
    This function creates an overlay of a VLBI-clean image and the corresponding\n
    model of Gaussian components which are over-plotted as ellipses.\n
    The required input format are fits-file for both the clean and the modelfit images.
    The format of the output file depends on the suffix of the given\n
    \code{filename}. Possible formats of the output file are PDF, EPS,\n
    PNG, GIF, etc.
    If labelling = 1 is set, the components are labeled with J0,J1,J2,... depending on
    their distance to [0,0].
    The counterjet components (and core) can be excluded by defining the corresponding
    quadrant of the the plot via ex_comps_ra and _dec.
\done

\function{compare_par}
\synopsis{compares models saved in parameter files}
\usage{compare_par([String_Type pattern]);
\altusage{Struct_Type info = compare_par([String_Type pattern]; get_list);}
}
\qualifiers{
\qualifier{fit_fun}{include fit-function}
\qualifier{fit_fun=fitfun}{only include models using the fit-function \code{fitfun}}
\qualifier{get_list}{the information is not printed, but returned}
\qualifier{load_best}{load the best fit parameters}
\qualifier{verbose}{}
}
\description
    It is assumed that the corresponding data sets are already initialized.
\seealso{load_par, eval_counts}
\done

\function{complementary_array}
\synopsis{returns the "difference" of arrays}
\usage{Array_Type C = complementary_array(Array_Type B, A)}
\description
    \code{C = B} \\ \code{A}
\done

\function{Compton_crosssection}
\usage{Double_Type sigma = Compton_crosssection(Double_Type E);}
\description
    \code{E} is the energy in keV.
    \code{sigma} is the total Klein-Nishina crosssection in cm^2:
    \code{sigma = 3/4 sigma[Th] * [  (1+y)/y^3 * { 2y*(1+y)/(1+2y) - log(1+2*y) }  +  log(1+2*y)/(2*y)  +  (1+3y)/(1+2y)^2  ]}
    where \code{y = E/(511 keV)}.
\seealso{Rybicki & Lightman (1979)}
\done

\function{continued_fraction}
\synopsis{computes a continued fraction}
\usage{Double_Type continued_fraction(UInteger_Type a[])}
\description
    \code{                                  1 |      1 |}\n
    \code{continued_fraction(a) = a[0] + ------ + ------ + ...}\n
    \code{                               | a[1]   | a[2]}
\done

\function{continued_fraction_expansion}
\synopsis{expands a floating point number as a continued fraction}
\usage{UInteger_Type[] continued_fraction_expansion(Double_Type x)}
\qualifiers{
\qualifier{verbose}{}
\qualifier{maxlen}{[\code{=64}]}
}
\done

\function{contour_borders}
\synopsis{Returns outer borders of a contour}
\usage{Double_Type[] = contour_borders(fits);}
\qualifiers{
\qualifier{conf_level [2]}{: set confidence level for which you need the borders}
}
\description
   This function takes as input a contour in fits-format saved with
   save_conf and returns the array [par1_min,par1_max,par2_min,par2_max].
   Select the confidence level with the conf_level quelifier (1,2,3).
\example
   contour_borders("contour.fits";);
\seealso{function_name2, function_name3}   
\done

\function{contour_trq}
\synopsis{creates and (with qualifier trq) sends bins^2 jobs to torque (at Remeis cluster) and
        calculates chi_2 values for each point for chosen 2 parameters.}
\usage{contour_trq(String_Type inputPar1, inputPar2, Int_Type bins,
        Double_Type lowLimPar1, upLimPar1,lowLimPar2, upLimPar2,
        String_Type startUpFile [, parFile], outDir, time);}  
\qualifiers{
\qualifier{trq}{Sends the jobs to Remeis torque}
}

\description
    - \code{inputPar1}     input parameter 1 written as in .par file
    - \code{inputPar2}     input parameter 2 written as in .par file
    - \code{bins}          number of chosen bins
    - \code{lowLimPar1}    lower parameter 1 value
    - \code{upLimPar1}     upper parameter 1 value
    - \code{lowLimPar2}    lower parameter 2 value
    - \code{upLimPar2}     upper parameter 2 value
    - \code{startUpFile}   upload file (with data, binning,
                                 noticing etc.)
    - \code{parFile}       best fit parameter file name (optional) 
    - \code{outDir}        output directory for torque files and
                                 calculated output files
    - \code{time}          torque wall time

    In case where values for all bins^2 points are not calculated (for
    example when the torque walltime is shorter than needed), one can run
    function missing_contour_trq in order to get all bins^2 values. 
    
    To plot the result use the function plot_contour_trq.


 EXAMPLE

   contour_trq(`relconv(1).Incl`,`reflionx(1).Xi`,32,25,35,1000,4000,"data.sl",
   "results/bestFit.par","outputInclXi","02:00:00";trq);

   will calculate 32 by 32 values for two parameters and write the
   output files to "outputInclXi" subdirectory. It will do that by
   sending the 32x32 jobs to Remeis torque via qualifier "trq", and
   giving it a walltime of 02:00:00 hours. 
   
\seealso{missing_contour_trq, plot_contour_trq}
\done

\function{convert_units}
\synopsis{convers units of a physical quantity}
\usage{convert_units(PhysicalQuantity_Type q);
\altusage{PhysicalQuantity_Type convert_units(PhysicalQuantity_Type q; copy)}
}
\qualifiers{
\qualifier{leng}{unit of length}
\qualifier{time}{unit of time}
\qualifier{mass}{unit of mass}
\qualifier{curr}{unit of electrical current}
\qualifier{temp}{unit of temperature}
\qualifier{SI}{sets  leng="m", time="s", mass="kg", curr="A", temp="K"}
\qualifier{copy}{does not change unit of q, but returns a copy of q}
}
\description
   The allowed units are specified by the associative arrays
         baseunits_length_in_m, baseunits_time_in_s,
         baseunits_mass_in_kg, baseunits_current_in_A,
     and baseunits_temperature_in_K
   which specify the factor to the corresponding SI units.
\done

\function{coords_in_box}
\synopsis{calculates world coordinates from the relative coordinates in the plot box}
\usage{(Double_Type x, y) = coordY_in_box(Double_Type x_rel, y_rel);}
\description
    Note that the x- and yrange has to be set in advance
    in order to calculate the world coordinates with \code{coords_in_box}.
\seealso{coordX_in_box, coordY_in_box}
\done

\function{coordX_in_box}
\synopsis{calculates a world x-coordinate from the relative x-coordinate in the plot box}
\usage{Double_Type coordY_in_box(Double_Type x_rel)}
\description
    The left boundary of the plot box has \code{x_rel==0},
    and the right boundary has \code{x_rel==1}.
    Logarithmic world-coordinates are properly taken into account.
    Note that the xrange has to be set in advance
    in order to calculate the world x-coordinate with \code{coordX_in_box}.
\seealso{coordY_in_box, coords_in_box}
\done

\function{coordY_in_box}
\synopsis{calculates a world y-coordinate from the relative y-coordinate in the plot box}
\usage{Double_Type coordY_in_box(Double_Type y_rel)}
\description
    The lower boundary of the plot box has \code{y_rel==0},
    and the upper boundary has \code{y_rel==1}.
    Logarithmic world-coordinates are properly taken into account.
    Note that the yrange has to be set in advance
    in order to calculate the world y-coordinate with \code{coordY_in_box}.
\seealso{coordX_in_box, coords_in_box}
\done

\function{COPY}
\synopsis{makes a copy of a nested Struct/Array/List_Type}
   \usage{Struct_Type copy = COPY(Struct_Type copyme);}
\altusage{Array_Type  copy = COPY(Array_Type  copyme);}
\altusage{List_Type   copy = COPY(List_Type   copyme);}
\altusage{Assoc_Type  copy = COPY(Assoc_Type   copyme);}
\description
    \code{COPY} returns a properly dereferenced copy of an
    arbitrarily nested \code{Struct/Array or List_Type} with
    all its entries. 
    Depending on the Data_Type of the initial given
    copyme and the Data_Type of its entries, \code{COPY}
    iteratively calls the  according sub-function
    (\code{struct_copy}, \code{array_copy} or \code{list_copy}).
    If the given Data_Type or one of its entries is not
    one of previously mentioned ones, \code{COPY} returns
    @(Data_Type) or respectively @(entry).
    
    NOTE: * Ref_Type entries will not be dereferenced, which
            allows to copy XFIG-OBJECTS !!!
          * Double_Type[] is an Array_Type

\example
    Array_Type:
     s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
     s[0].a[[0]]=[0:9];
     copy = COPY(s); copy[0].a[[0]] = ["modified"];
     print(s[0].a);

    List_Type:
     s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
     s[0].a[[0]]=[0:9];
     l = [{ array_copy(s), 1., [1:10] }, {"abs"}];
     copy = COPY(l); copy[0][0][0].a[[0]] = ["modified"];
     print(l[0][0][0].a);

    Struct_Type:
     s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
     s[0].a[[0]]=[0:9]; S = struct{ f=s };     
     C = struct_copy(S); C.f[0].a[[0]] = ["modified"];
     print(S.f[0].a);

\seealso{struct_copy, array_copy, list_copy, assoc_copy}
\done

\function{correlation_coefficient}
\synopsis{calculates the [weighted] linear correlation coefficient between two arrays}
\usage{Double_Type rho = correlation_coefficient(Double_Type x[], Double_Type y[] [ , Double_Type w[] ]);}
\description

   The function of the unweighted correlation coefficient reads:

   \code{                     n * sum(x*y)  -  sum(x) * sum(y)          }
   \code{rho  =  -------------------------------------------------------}
   \code{         sqrt(n*sum(x^2)-sum(x)^2) * sqrt(n*sum(y^2)-sum(y)^2) }
   
   w defines the weight of each point. By default it is set to w = 1.
   
\seealso{find_correlations}
\done

\function{cosmo_param}
\synopsis{Derive full set of cosmological density parameters from a partial set}
\usage{Struct_Type P = cosmo_param();}
\qualifiers{
\qualifier{omega_m}{value of the normalized matter density (Omega_M)}
\qualifier{omega_lambda}{value of the normalized cosmological constant (Omega_Lambda)}
\qualifier{omega_k}{value of the the normalized curvature term (Omega_k)}
\qualifier{q0}{value of the deceleration parameter (q0)}
}
\description
    This procedure is called by lumdist to allow the user a choice
    in defining any two of four cosmological density parameters.
    Given any two of the four input parameters this  program will derive
    the remaining two.
    Omega_M      - Normalized matter energy density, non-negative numeric scalar
    Omega_Lambda - Normalized cosmological constant, numeric scalar
    Omega_k      - Normalized curvature parameter, numeric scalar.
                   This is zero for a flat universe
    q0           - Deceleration parameter, numeric scalar = -R*(R'')/(R')^2
                   = 0.5*Omega_m - Omega_lambda
    Here "normalized" means divided by the closure density so that
    Omega_m + Omega_lambda + Omega_k = 1.    For a more
    precise definition see Carroll, Press, & Turner (1992, ArAA, 30, 499).

    If less than two parameters are defined, this procedure sets default
    values of Omega_k=0 (flat space), Omega_lambda = 0.7, Omega_m = 0.3
    and q0 = -0.55

    If more than two parameters are defined upon input (overspecification), 
    then the first two defined parameters in the ordered list Omega_m, 
    Omega_lambda, Omega_k, q0 are used to define the cosmology.
    
    (This procedure is translated from the IDL COSMO_PARAM function.)

\example
    variable default_set = cosmo_param();
    print (default_set);

    variable my_set = cosmo_param(; omega_k = 0.1, omega_lambda = 0.65);
    print (my_set);
\seealso{lumdist}
\done

\function{covariance_correlation_matrix}
\synopsis{Compute the covariance and correlation matrices of a set of variables}
\usage{(Double_Types cov_mat[n,n], cor_mat[n,n]) = covariance_correlation_matrix(Double_Type a[n,N])}
\description
    This function estimates the covariance and correlation matrices of a set of
    random variables. Consider a sample 'a' of 'N' column vectors of length 'n':
    a = Double_Type[n,N]. Each of the 'n' rows of 'a' may represent the realization
    of a random variable. The 'n' times 'n' covariance matrix 'cov_mat' between
    these 'n' variables is
      cov_mat[i,j] = 1/(N-1) * sum( (a[i,*]-mean(a[i,*]))*(a[j,*]-mean(a[j,*])).
    The 'n' times 'n' correlation matrix 'cor_mat' is
      cor_mat[i,j] = cov_mat[i,j]/sigma_i/sigma_j
    where 'sigma_i' is the square root of the variance of 'a_i':
      sigma_i = sqrt( 1/(N-1) * sum( (a[i,*]-mean(a[i,*]))^2 ) ).
\notes
    The Cholesky decomposition of the covariance matrix can be used to generate
    correlated variables that obey a given covariance matrix. See the example
    section for details.
\example
    % From three uncorrelated normal variables 'g', generate three correlated normal
    % variables 'x' that obey the covariance matrix 'm':

    g = Double_Type[3,100000];
    g[0,*] = grand(100000); g[1,*] = grand(100000); g[2,*] = grand(100000);
    (cov_mat, cor_mat) = covariance_correlation_matrix(g);
    print(cov_mat); % the covariance matrix of 'g' shows that there are no correlations
    print(cor_mat);

    m = Double_Type[3,3]; m[0,*] = [ 9,-3,-6]; m[1,*] = [-3,10, 5]; m[2,*] = [-6, 5, 6];
    cd = cholesky_decomposition(m);
    x = cd#g; % matrix product of 'cd' and 'g'
    (cov_mat, cor_mat) = covariance_correlation_matrix(x);
    print(cov_mat); % the covariance matrix of 'x' is indeed (almost) 'm'
    connect_points(0); plot(x[0,*],x[2,*]);
\seealso{cholesky_decomposition}
\done

\function{cov_matrix}
\synopsis{computes a covariance matrix}
\usage{COV = cov_matrix(Struct_Type s);
\altusage{COV = cov_matrix(Double_Type X[][]);}
\altusage{COV = cov_matrix(Double_Type x0[], x1[], ...);}
}
\description
    \code{COV[i,j] =} cov(x_i, x_j) = < (x_i - <x_i>) * (x_j - <x_j>) >
\done

\function{create_basic_simputfile}
\synopsis{creates a basic SIMPUT file}
\usage{Struct_Type str = create_basic_simputfile(filename, RA, Dec, srcFlux, nH, powerLawIndex);}
\description
    This function creates a structure, which contains all necessary
    info for a simple "simputfile" command to create a basic SIMPUT
    file for a given point source. The flux is assumed to be given in
    the 2-10 keV band by default. The file can be directly created by
    adding the qualifier "eval". Otherwise a structure is returned.
    This structure can be further modifed, and then evaluated with
    the function "eval_simputfile".
\qualifiers{
\qualifier{crab}{flux is given in units of Crab}
\qualifier{eval}{directly evalute the returned structure (similar to call eval_simputfile(str); }
\qualifier{emin}{lower limit of the energy band of the given flux in keV}
\qualifier{emax}{lower limit of the energy band of the given flux in keV}
\qualifier{quite}{don't show any output}
}
\examples
    \code{variable str = create_basic_simputfile("velaX-1.fits",135.528583,40.554722,100e-3,1.8,5.0;crab);}
\seealso{eval_simputfile,get_simputfile_struct,set_simputfile_model_grid,set_simputfile_flux}
\done

\function{create_struct_field}
\synopsis{create and set the value associated with a structure field}
\usage{Struct_Type create_struct_field(s, field_name, field_value);}
\qualifiers{
    \qualifier{skip}{do not update an existing field}
}
\description
    Does the same as the `set_struct_field' function except
    that the given field is created first if it does not
    exist.
\seealso{set_struct_field, struct_combine}
\done

\function{crossing_number_polygon}
\usage{cn=crossing_number_polygon(P,V)}
\synopsis{return the crossing number for a point P in a polygon}
\description
 Return the crossing number for a point P in a (potentially
 complex polygon defined by the vertex points V).
 The polygon has to be closed, i.e. V.x[n]==V.x[0] and
 V.y[n]==V.y[0] where n is the number of polygon points.

 The crossing number is the number of times that a ray starting 
 from point P crosses the polygon.  The point is outside of the
 polygon if the crossing number is even, and inside if the
 crossing number is odd (even-odd rule).

 The point P is a struct{x,y}, while the polygon V is 
 defined by its vertex points, which are stored in a
 struct{x[],y[]}
 where the arrays are the x- and y-coordinates.

 See the URL below for more explanations.

 Based on code by Dan Sunday,
 http://geomalgorithms.com/a03-_inclusion.html
 and patterned after Randolph Franklin (2000),
 http://www.ecse.rpi.edu/Homepages/wrf/research/geom/pnpoly.html

\seealso{winding_number_polygon,point_in_polygon,simplify_polygon}
\done

\function{cross_power_density}
\synopsis{Timing Tools: Cross Power Density (aka. Cross-Spectrum)}
\usage{(cpd, errcpd) = cross_power_density(rate1, rate2, numseg, dimseg)}
\description
    Calculates the CPD, averaged over all segments, as
    Conj(DFT(rate1))*DFT(rate2)
    using the dorDFT function. The return values are of Complex_Type.
    The error is the standard error on the mean of the averaged CPD.
\seealso{dorDFT}
\done

\function{cstat_goodness}
\synopsis{computes the theoretical Cash-statistics and its
uncertainty for the current data and model}
\usage{Struct_Type ctheo = cstat_goodness();}
\description
 This function computes the theoretical Cash statistics for the
 currently set data, using the expressions given by Kaastra
 (2017, A&A 605, A51). These values are returned in the tags
 cstat_theory and c_variance. The tag n_bins contains the number
 of bins entering these values, it should be the same as that
 obtained with eval_stat_counts().

 The values can be used to compute the goodness of the fit by
 comparing its best fit statistics (obtained with eval_stat_counts())
 with cstat_theory and the variance. The "f-sigma" probability for
 the best fit cash statistic is the range cstat_theory - f *c_variance
 to cstat_theory + f * c_variance.

\qualifiers{
\qualifier{quiet}{if set, the function will not complain if the fit
                    statistics is not set to Cash or if there are bins without counts.}
\qualifier{data}{if set, uses the measured counts in the computation
                   (the default is the expected counts in the bin)}
}
 
\seealso{kaastra_cstat_goodness,eval_stat_counts,set_fit_statistic}

\done

\function{cutoffpl2 (fit-function)}
\synopsis{describes a power law with exponential cutoff in wavelength space}
\description
    S(l) = \code{norm} * l^(-\code{index}) * exp(-l/\code{cutoff})\n
    The bin-integrated cutoffpl2 fit-function is computed
    by the incomplete Gamma-function from the GSL library.
    (The original cutoffpl didn't converge for low energies.)
\seealso{cutoffpl, gamma_inc}
\done

\function{cut_dataset_range}

\synopsis{Get get_data_counts-struct clipped to energy range}
\usage{Struct_Type = cut_dataset_range (hist_index , E_min , E_max);}
\description
       This function will return a structre just as get_data_counts
       but clipped to the energy range from E_min up to E_max (energies in keV).
\example
       isis>id = load_data("data.fits");
       isis>variable cut_spectrum = cut_dataset_range (id , 0.2 , 2.3);
\seealso{get_data_counts}
\done

\function{cyl2cart}
\synopsis{Convert cylindrical to Cartesian coordinates}
\usage{cyl2cart(Double_Types r[], phi[], z[], vr[], vphi[], vz[]);}
\description
    Convert cylindrical (r,phi,z,vr,vphi,vz) to Cartesian (x,y,z,vx,vy,vz)
    coordinates with phi rotating counter-clockwise from the positive x-axis.
\example
    (x,y,z,vx,vy,vz) = cyl2cart(1,PI/2,1,2,3,4);
    (x,y,z,vx,vy,vz) = cyl2cart([1,0],[PI/2,0],[1,0],[2,0],[3,0],[4,0]);
    (x,y,z,vx,vy,vz) = cyl2cart( cart2cyl(1,1,1,2,3,4) );
\seealso{cart2cyl}
\done

\function{data_list (isis_fancy_plots package)}
\synopsis{Turn an array into a list.}
\usage{data_list(Array);}
\description
\seealso{list_to_array}
\done

\function{data_map_function}
\synopsis{executes a function on specific datasets}
\usage{data_map_function(String_Type filter, Ref_Type function, arguments...; qualifiers);}
\description
    Loops over all defined datasets and matches each data
    information against the given filter. In the simplest
    way, this filter is a string respresenting the wanted
    instrument as specified in the field of 'get_data_info'.
    If the 'fi' qualifier is provided, the given filter is
    interpreted as if-statement, which has to be fullfiled.
    Thereby, the associated data information can be accessed
    via the 'info' variable. The given function is applied
    to each matching dataset in the form
      function(dataset, arguments...; qualifiers);
\example
    % set the energy ranges for all RXTE-PCA sepctra
    data_map_function("PCA", &xnotice_en, 3.5, 50);

    % apply grouping to all Suzaku-PIN spectra
    data_map_function(`is_substr(info.file, "hxd_pin")`,
      &group; min_sn = 40, fi);
\seealso{get_data_info, eval, array_map}
\done

\function{DateOfJD}
\synopsis{Calculates the date corresponding to a JD value}
\usage{Struct_Type date = DateOfJD(JD);}
\description
    The return value is a structure of the following form:\n
       \code{date = struct { year, month, day, hour, minute, second };}
    The algorithm used is that by Meeus, Astronomical Formulae for
    Calculators. This routine is array safe.
\qualifiers{
\qualifier{julianswitch}{For JD smaller than this date, return the date
        in the Julian calendar rather than in the Gregorian
        calendar. The default is JD2299161 (15 October 1582),
        which is appropriate for most of catholic Europe.
        Note, however, that there are countries that switched
        only much later (see https://en.wikipedia.org/wiki/Julian_calendar
        and https://de.wikipedia.org/wiki/Gregorianischer_Kalender).    
        For example, Russia only switched on 1 February 1918 (Gregorian).}
\qualifier{julian_calendar}{force returning the date in the Julian Calendar
        (ignore julianswitch)}
\qualifier{gregorian_calendar}{force returning the date in the Gregorian Calendar
        (ignore julianswitch)}
}
\seealso{JDofDate, MJDofDate}
\done

\function{DateOfMJD}
\synopsis{calculates the date from its MJD value}
\usage{Struct_Type date = dateOfMJD(MJD);}
\description
    The return value is a structure of the following form:\n
       \code{date = struct { year, month, day, hour, minute, second }
    and always in the Gregorian Calendar. See DateOfJD if you want more
    functionality. This function is array safe.}
\seealso{DateOfJD, MJDofDate}
\done

\function{dateOfMJD}
\synopsis{calculates the date from its MJD value}
\usage{Struct_Type date = dateOfMJD(MJD);}
\description
    The return value is a structure of the following form:\n
       \code{date = struct { year, month, day, hour, minute, second }
    and always in the Gregorian Calendar. See DateOfJD if you want more
    functionality. This function is array safe.}
\seealso{DateOfJD, MJDofDate}
\done

\function{dayOfWeek}
\synopsis{returns the day of the week of the given date}
\usage{Integer_Type dayOfWeek(Integer_Type year, month, day);}
\description
    This function returns the day of the week from 1 (Monday) to
    7 (Sunday) of the given date in the Gregorian (default) or the
    Julian calendar.

\qualifiers{
\qualifier{julian_calendar}{: the date is in the Julian calendar
                              (default: Gregorian calendar)}
}
\example
    dayOfWeek(1837, 9, 9);
\done

\function{daysPerMonth}
\synopsis{returns the number of days of a month}
\usage{Char_Type daysPerMonth(m[, y])}
\description
    \code{m} = 1, 2, ... 12 specifies the month Jan, Feb, ..., Dec.\n
    If \code{y} is not specified, a non-leapyear is assumed.
\seealso{leapyear}
\done

\function{dcf}
\synopsis{compute the Edelson & Krolik discrete correlation function and returns a structure with the correlation}
\usage{dcf(time_a, val_a, tima_be, val_b);}
\qualifiers{
\qualifier{err_a [=0]}{:  array containing the measurement errors of
             val_a. If given, this is taken into
             account by using eq. (3) of Edelson & Krolik.
             NOTE: There are lots of problems with interpreting the
             DCF computed this way instead of using Edelson & Krolik,
             eq. (2), and it is NOT recommended to give erra and errb
             (see, e.g., White & Peterson, 1994, PASP, 106, 879)}
\qualifier{err_b [=0]}{:	as err_a except for val_b}
\qualifier{minlag [=mininum time difference between time_a and time_b]}{:	minumum lag to consider}
\qualifier{maxlag [=maximum time difference between time_a and time_b]}{:	maximum lag to consider}
\qualifier{numf [=int(min([length(val_a) ,length(val_b) ])/10)]}{:	number of lag bins for which to compute the DCF}
\qualifier{minpt [=10]}{:	minimum number of data points for a DCF value to be}
}
\description
    This is the imported version of the IDL subroutine dcf.pro \n
    It will compute the Edelson & Krolik discrete correlation function. \n
\done

\function{define_atime}
\synopsis{defines a dataset of arrival times}
\usage{Integer_Type define_atime(Struct_Type atime[, Integer_Type divide]);}
\qualifiers{
    \qualifier{modnum}{a reference to a variable, where to store
             the pulse numbers found by the model}
    \qualifier{noff}{do not add 'arrtimes' to the actual fit-
             function correspondig to the dataset}
}
\description
    Takes the given arrival times structure to define
    an ISIS dataset. This structure may be created by
    'atime_det' or loaded by 'load_atime'. Accordingly
    all usual ISIS routines, which operate on datasets,
    e.g. fitting, can be used. The fit function
    'arrtimes' handle arrival times including orbital
    motion and pulse ephemeris. If not disabled by the
    'noff' qualifier the actual fit function is auto-
    matically extended by the resulting dataset using
    the 'arrtimes' model. It also takes the qualifier
    'modnum' into account to return the pulse numbers
    determined by the model. The reference to the
    pulse numbers is stored into the metadata of the
    dataset.
    If the second parameter 'divide' is given the
    dataset is defined several times accordingly to
    the given integer. The datasets are then noticed
    automatically such that the whole data is divided
    into the given number of parts of equal length.
    Each part may be then fitted individually.
    The returned integer corresponds to ISIS dataset
    index.
\seealso{arrtimes, atimes_det, load_atime, atime_metavalid, xnotice_atime}
\done

\function{define_counts_2d}
\synopsis{defines a pseudo-spectrum from two-dimensional data}
\usage{Integer_Type define_counts_2d(value[, err][, X, Y])}
\description
    \code{value} and \code{err} are 2d arrays of Double_Type.
    ISIS usually deals with 1d spectra. For fitting 2d data, \code{define_counts_2d}
    can be used to define a pseudo 1d spectrum by reshaping arrays,
    to which ISIS' internal fit routines can be applied.\n
    User defined fit-functions should not use the bin_lo, bin_hi arguments,
    but the actual data-grid which can be set / obtained with \code{set}/\code{get_2d_data_grid},
    if the user doesn't prefer to take care of the data grid on his own.
    If the optional 1d Double_Type arrays \code{X} and \code{Y} are specified,
    \code{set_2d_data_grid} is already called by \code{define_counts_2d}.
\seealso{gauss_2d}
\done

\function{define_xydata}
\synopsis{defines an xy-dataset to be modeled with xyfit_fun}
#c%{{{
   \usage{Integer_Type data_id = define_xydata ( x[], y[] [, yerr[]] ); }
\altusage{Integer_Type data_id = define_xydata ( x[], xerr[], y[], yerr[] ); }
\qualifiers{
\qualifier{N[=1000]}{: number of curve points, when xerr is considered}
\qualifier{x_mdl [default: array covering \code{x} with \code{N} steps]}{:
            (initial) x-values of the model, when \code{xerr} is considered}
\qualifier{y_mdl [default: array covering \code{y} with \code{N} steps]}{:
            (initial) y-values of the model, when \code{xerr} is considered}
}
\description
    This function creates a dummy spectral dataset for ISIS and stores
    the xy-dataset in its metadata, which are only considered when
    fitting with a dedicated xy-fit-function defined by \code{xyfit_fun}.
    Fitting via \code{fit_counts} will minimize the sum of squared residuals,
    see \code{xyfit_residuals}.
    
    If no \code{xerr} is specified, the xy-fit-function will only be evaluated
    at the x-values of the data. (yerr defaults to 1, if not specified.)
    
    If \code{xerr} is given as well, a curve is constructed in order to compute
    the residuals of the data points. The xy-fit-function may produce a
    (reasonably smooth!) graph (x, y(x)) or parameterized curve (x(t), y(t)).
    It operates on the model points and possibly (constant) parameters
    defined by \code{set_xy_qualifier}. Model points for an xy-dataset
    are initially defined by the \code{N} or \code{x_mdl} and \code{y_mdl} qualifiers.

    If xerr, yerr are lists with two array entries, than the first is
    interpreted as lower and second as upper uncertainty

\example
    N = 150; (x, y) = ellipse(8, 5, 0.2*PI, grand(N)*1.5);
    x += grand(N)+4; y += grand(N)+1;
    id = define_xydata (x, ones(N), y, ones(N));
    xyfit_fun("ellipse_xy");
    set_xyfit_qualifier(id; curve_parameter=[0:2*PI:#3000]);
    ()=fit_counts;
    set_par("ellipse_xy(1).pos_angle", 30); ()=fit_counts; % help with angles...
    plot_xyfit(id);
    % fit_interactive(&plot_xyfit);
\seealso{xyfit_fun, set_xyfit_qualifier, xyfit_residuals, plot_xyfit}
\done

\function{deg2dms}
\synopsis{Convert a floating point angle in degrees to degrees, minutes, seconds}
\usage{(d,m,s) = deg2dms(degree)}
\qualifiers{
\qualifier{hours}{If set, return the coordinate in hours, minutes, sec (RA)}
\qualifier{radian}{If set, the angle is in radians, not degrees}
}
\description
 This is a convenience routine to convert astronomical coordinates given
 as a floating point number into a number in degrees, minutes, seconds.

 For negative angles, the sign of the coordinate is given to the highest 
 non-zero integer.

 Use angle2string to generate strings from angles, and generate_iauname
 if you want to build strings to use in source names.

\seealso{dms2deg, hms2deg, angle2string, generate_iauname}
\done

\function{detconst}
\synopsis{fit-function providing detector calibration constants}
\usage{detconst(id)}
\description
    Simply returns a constant depending on the detector of
    the current dataset the model is evaluated on. This can
    be used to account for flux calibration differences
    between several detectors.

    To use this fit-function, the detectors has to be
    defined first using 'detconst_init'.
\seealso{detconst_init}
\done

\function{detconst_init}
\synopsis{initializes the "dectonst" fit-function}
\usage{detconst_init(String_Type[] detectors);
 or detconst_init(List_Type detectors);}
\description
    Before any detector calibration constants can be fitted
    by using the 'detconst' fit-function, this function has
    to be called. It initalizes and defines the fit-function
    based on the given detector names (and if statements).

    In general, the detector names are given as an array of
    strings. Each entry has to represent the 'instrument'
    field defined in the FITS-header of any used dataset.
    Internally, the 'detconst' matches this field against
    the given names and returns the corresponding parameter.

    If the instrument field is not enough to determine the
    parameter (e.g., if multiple instruments are build within
    the same detector), a list of strings can be provided
    instead of an array. If a list item itself is an array
    with exactly two strings, the first one is the detector
    name and the second specifies the if statement to be
    evaluated to determine the corresponding parameter.
    Within the if statement, information about the dataset
    can be accessed via the 'info' struct (see get_data_info).

    If the 'debug' qualifier is given, the S-Lang code of
    the fit-function is printed out as well.
\example
    % RXTE consisted of two instruments: PCA and HEXTE
    detconst_init(["PCA", "HEXTE"]);
    
    % SUZAKU consists of six instruments: XIS0-3, PIN,
    % and GSO. PIN and GSO are, however, within one detector
    % called HXD (as in the FITS-header). Thus, identify
    % these two instruments via the PHA-filename
    detconst_init({"XIS0", "XIS1", "XIS2", "XIS3",
      ["PIN", "is_substr(info.file, \\"hxd_pin\\")"],
      ["GSO", "is_substr(info.file, \\"hxd_gso\\")"]
    }; debug);
\seealso{detconst, get_data_info, add_slang_function}
\done

\function{diff}
\synopsis{returns the differences between adjacent elements in an array}
\usage{Double_Type diff(Double_Type[] array);}
\description
    a = sqr([1:4]); % [1,4,9,16]
    b = diff(a);    % [3,5,7]
\done

\function{difmap_restore}
\synopsis{DIFMAP is used to restore a radio image with a new beam}
\usage{String_Type outname = difmap_restore(String_Type \code{fitsfile}, Double_Type \code{smajor}, \code{sminor}, \code{pos_angle})}
\qualifiers{
\qualifier{chan}{[=i] choose channel (i,q,u)}
\qualifier{xsize}{[= key "NAXIS1"] number of RA pixels}
\qualifier{xsetp}{[= key "CDELT1"] step size of each RA pixel in mas}
\qualifier{ysize}{[= key "NAXIS2"] number of DEC pixels}
\qualifier{ysetp}{[= key "CDELT2"] step size of each DEC pixel in mas}
\qualifier{uvtaper}{[= " "] pply a uvtaper before restoring}
\qualifier{outname}{[= "xxxxMHz_restore.fits"] filename of the output, be default the
                                frequency is read from the input file}
\qualifier{overwrite}{overwrite file with outname if it already exists}
}
\description
    This function creates a radio image with a given beam using DIFMAP. The
    new beam has to be defined by its semimajor and semiminor axis and its
    position angle. The new images is saved in the current working directory.
    It is assumed that the UVF and MOD files have the same basename as the
    provided file name if the FITS image.
\seealso{make_spix}
\done

\function{diskline2}
\synopsis{describes a line emission from a relativistic accretion disk}
\description
    norm    = photons/cm**2/s in the spectrum\n
    LineE   = line energy\n
    Betor10 = power law dependence of emissivity.
              If this parameter is 10 or greater then the accretion
              disk emissivity law (1-sqrt(6/R))/R**3 is used.
              Otherwise the emissivity scales as R**par2.\n
    Rin     = inner radius (units of GM/c**2)\n
    Rout    = outer radius (units of GM/c**2)\n
    Incl    = inclination (degrees)\n
    (The original diskline model uses Rin(M) and Rout(M).)
\seealso{diskline / Fabian et al., MNRAS 238, 729.}
\done

\function{disk_map}
\synopsis{Plots the megamaser disk using the output of Mark
   Reid's Bayesian disk fitting routine.}
\usage{disk_map(infile);}
\qualifiers{
\qualifier{radius: }{size of the plotted disk radius, in mas, default: reference radius from file}
\qualifier{no_axes: }{flag to turn off the axes}
\qualifier{no_los: }{flag to turn off the line-of-sight bar}
\qualifier{no_disk: }{flag to turn off the wireframe disk model}
\qualifier{no_data: }{flag to turn off the data points}
\qualifier{no_grid: }{flag to turn off the grid}
\qualifier{no_color: }{flag to plot everything in greyscale}
\qualifier{scale: }{flag to scale the size of the data points by SNR}
\qualifier{reverse_x: }{flag to reverse the x-axis orientation; not used here}
\qualifier{side: }{see the maser disk along the line-of-sight in the x-y-plane}
\qualifier{above: }{see the maser disk from above in the x-z-plane}
\qualifier{projection: }{projects the maser spots in addition to the 3d view to the 2d planes}
\qualifier{scalen: }{half scale length of the axes; default 1.1*radius of the disk, axes are [-scalen,scalen]}
\qualifier{scalen_frac: }{distance from ticlabels to axes; default 1.05}
\qualifier{label_dist: }{distance from labels to axes; default 1.3*radius}
\qualifier{tics_length: }{length of the tics; default 0.2}
}
\description
    Plots the megamaser disk using the output of Mark
   Reid's Bayesian disk fitting routine. The input file for
   this script is simply a text file containing the printout
   of the Bayesian program. For instance, if the Bayesian
   executable had the file name "fit_disk_v20*", then the Unix command
        
   ./fit_disk_v20* > input.prt
           
   would generate the appropriate input file for this script.
   This script is matched to version 20 of Mark's program.
   The default view on the disk is at theta=120 and phi=-130,
   with the x-axis showing upwards.

   This function is rewritten from the IDL program disk_map.pro
   from Dom Pesce in the version from July, 22, 2014.
\example
   For a 2D plot from above:
   
   isis> bla=disk_map("/userdata/data/litzinger/Radio/NGC1194/accel/disk_fitting/fit_disk_v20/jan2615/outjan2715";above);
   isis> bla.render("disk_map_above.pdf");

   or for a 3D plot with projection:

   isis> bla=disk_map("/userdata/data/litzinger/Radio/NGC1194/accel/disk_fitting/fit_disk_v20/jan2615/outjan2715";projection);
   isis> bla.render("disk_map.pdf");

   Be careful!: If you plotted the 3d view and would like to plot
   in 2d again you have to restart isis, as it still would plot in
   3d view. To set the labels and tics correctly first plot the disk
   with the default values and then change the values.
   
\seealso{xfig_3d_orbit_on_cube}
\done

\function{distribution_triangle}
#c%{{{
\synopsis{Calculate 1D and 2D probability distributions}
\usage{Struct_Type dm = distribution_matrix(Array_Type/List_Type v);
\altusage{dm = distribution_matrix(Struct_Type v);}}
\qualifiers{
\qualifier{gridX}{histogram grid for dimension X where X starts counting at 0}
\qualifier{gridXmin}{[=min(vX)] bottom boundary for dimension X grid}
\qualifier{gridXmax}{[=max(vX)] top boundary for dimension X grid}
\qualifier{n}{number of grid points (only relevant for min/max)}
\qualifier{fields}{Use only struct fields given in this name array (only
  relevant if input is a struct)}
}
\description
    Calculates the probability distributions from a given high dimensional
    set of state vectors (think MCMC results). The resulting structure
    can be used with \code{xfig_plot_distribution_matrix} to plot the distributions
    in a projection like triangle matrix.

\seealso{xfig_plot_distribution_matrix}
\done

\function{dms2deg}
\synopsis{Convert angle in degree, minute, seconds to floating points degrees or radian}
\usage{degree = dms2deg(d,m,s);}
\qualifiers{
\qualifier{hours}{If set, the coordinate is given in hour, minutes, sec (RA)}
\qualifier{radian}{If set, return angle in radians, not degrees}
}
\description
This is a convenience routine to convert astronomical coordinates given
in degrees, minutes, and seconds into a floating point number.
The sign of the highest non-zero argument decides the sign of the
returned floating point number. In other words: The routine tries to 
be intelligent for negative declinations in the 0>=dec>-1 strip. 
For example, if the declination is -0d 14' 30.2", call this routine
as dms2deg(0,-14,30.2).

This routine is array safe (as long as d, m, s are arrays of equal length).
\example
 % Convert the coordinates 
 % alpha=19h 49m 35.49s and dec=-30d 12' 31.8"
 variable ra=hms2deg(19,49,35.49);
 variable dec=dms2deg(-30,12,31.8);

\seealso{hms2deg}
\done

\function{dof}

\synopsis{Number of degrees of freedom}
\usage{Double_Type = dof (hist_index);}
\description
       Use this function to retrieve number of
       degrees of freedom.
\example
       isis>xray = load_data("data.pha");
       isis>variable num = dof(1);

\seealso{num_bin}
\done

\function{dont_use_line}
\synopsis{sets the EW / amplitude of a line in the lines-model to zero}
\usage{dont_use_line([id,] line);}
\description
    If \code{id} is not specified, \code{id=1} is used.
    \code{line} is the name in the lines-model, appearing as parameters
    \code{line_lam}, \code{line_EW}, \code{line_FWHM} and \code{line_A}.
\seealso{lines}
\done

\function{dopplerorbit}
\synopsis{fit-function for the orbital Doppler shift factor}
\usage{fit_fun("dopplerorbit(1; qualifiers) * ...");}
\qualifiers{
    \qualifier{t90}{switch to the time of mean longitude 90 degrees (see below)}
    \qualifier{K}{switch to velocity semi amplitude (see below)}
    \qualifier{metacor}{string array of structure-field-paths inside the meta-
             data of the current dataset, which will be assumed to
             be time in MJD and corrected for binary motion (default:
             ["time"]). Set to NULL to disable any correction.}
}
\description
    Calculates the radial velocoity, v_rad, of a star in a binary and
    returns the corresponding Doppler factor, beta, given by
      beta = (1 + V_rad/Const_c)
    assuming that V_rad/Const_c << 1. The grid of the data (lo,hi)
    the model is fitted to needs has to be the time in MJD.

    The parameters of the fit-function are:
      asini - semi major axis (lt-s)
           or velocity semi amplitude (km/s)
      porb  - orbital period (days)
      tau   - time of periastron passage (MJD)
           or time of mean longitude 90 degrees (MJD)
      ecc   - eccentricity
      omega - longitude of periastron (degrees)
      v0    - systemic velocity (km/s)

    In case the 't90'-qualifier is set, the parameter 'tau' has the
    meaning of the time of mean longitude of 90 degrees. In case the
    'K'-qualifiers is set, the parameter 'asini' has the meaning of
    the velocity semi amplitude.

    The influence of the binary motion is removed from the input
    time-array 'lo' using 'BinaryCor'. The corrected times are saved
    as 'binarytime' into the fitfun_cache of the fit-function. This
    can be used by other fit-functions, like 'pulsartaylor'.

    The fit-function also corrects time arrays inside the metadata of
    the current dataset (Isis_Active_Dataset). The name of the fields
    are set by the 'metacor'-qualifier, which is an array of strings
    giving the "path" to the field. By default, the 'time'-field
    inside the metadata is corrected in order to simplify the usage
    of, e.g., 'pulsartorque'. The corrected times are save into the
    'metacor'-struct inside the cache using the same "path".
\seealso{radial_velocity_binary, BinaryCor, KeplerEquation}
\done

\function{Doppler_velocity}
\synopsis{calculates a Doppler velocity shift from a wavelength and the rest wavelength}
\usage{Double_Type v = Doppler_velocity(Double_Type lambda, Double_Type lambda0);}
\description
    \code{v = (lambda-lambda0)/lambda_0 * c;  %} speed of light \code{c = 299792} km/s
\seealso{get_line_velocity}
\done

\function{dorDFT}
\synopsis{Timing Tools: Discrete Fourier Transform}
\usage{dft = dorDFT(rate);}
\description
    Performs a Fast Fourier Transform on a given rate array with the
    same properties as IDL FFT:
    * Normalization = sqrt(length(rate))
    * renormalize rate around 0 by subtracting mean such that
      variability is calculated wrt. mean rate
    * return only meaningful bins for PSD calculations, i.e. first
      bin up to Nyquist frequency
\done

\function{douglas_peucker}
\synopsis{Simplify polygons using the Douglas Peucker Algorithm}
\usage{(xx,yy)=douglas_peucker(x,y,d2);}
\description
   This function implements an algorithm to simplify complex
   polygons (Douglas & Peucker, 1973, Cartographica 10(2), 112).
   A recursive version presented by Hershberger & Snoeyink (1992,
   Proc. 5th Intl. Symp. on Spatial Data Handling, 134-143) is
   used.
   For a polygon P defined by positions in given by the arrays (x,y),
   the algorithm returns the polygon P2 defined by coordinates (xx,yy) 
   with the property that the maximum distance between the lines segments
   of P and P2 is smaller than sqrt(d2). 
   Note 1:
   The algorithm only yields a good result if P is not self-intersecting.
   Note 2:
   As discussed by Hershberger & Snoeyink, the worst performance of
   the Douglas-Peucker-algorithm is O(N^2). Hershberger & Snoeyink (1998,
   Computational Geometry 11(3-4), 175-185) present a O(N log N) algorithm.
   Unfortunately for me (J. Wilms), this algorithm is too complex to be
   implementable on the ICE between Hamburg and Bamberg...
   Note 3:
   This function is called recursively and does not perform sanity checks on
   x,y,d2. If you do not know about the properties of your data, use
   simplify_polygon.
   
\seealso{simplify_polygon}
\done

\function{draw_plot_commands}
\synopsis{creates plot commands for drawing lines with the mouse}
\usage{draw_plot_commands();}
\qualifiers{
\qualifier{init}{[\code{=1}] initializes plot window at the beginning}
}
\done

\function{draw_progress_bar}
\synopsis{Draw a progress bar}
\usage{draw_progress_bar( position , maximum )}
\qualifiers{
\qualifier{tip}{String to draw tip of the arrow (default: ">")}
\qualifier{bar}{String to draw bar of the arrow (default: "=")}
\qualifier{front}{String to draw space in front of arrow (default: ".")}
\qualifier{append}{String to append to each drawn progress bar (default: "")}
\qualifier{columns}{Columns to use to write progress bar (default: Terminal width)}
\qualifier{fmt}{Format to use for writing the percentage info in front of the bar.
		   The printing function gets passed the percentage of the current
		   state to the function as the second argument after the format
		   (default: "%.1f%%", for example 01.3%)}
}
\description
	Draw a progress bar across the prompt.
	The width of the bar (arrow) is calculated from the fraction of
	position/maximum. In case position is greater than maximum nothing happens.
	Note that the function automatically determines the width of the current terminal.
	This procedure takes some tens of milliseconds. The function can be sped by
	setting the "columns" qualifier manually.
\done

\function{EasterSunday}
\synopsis{Calculate the JD or date of Easter Sunday for the Gregorian or Julian calendar}
\usage{jd=EasterSunday(year)}
\description
This function calculates the JD of Easter Sunday for the Gregorian or
Julian (=orthodox) calendar using the algorithms given by J. Meeus, 1998,
Astronomical Algorithms, 2nd ed, William-Bell.

The function either returns the JD (more precise: the JD at midnight
of Easter Sunday), or a struct with the same format as DateOfJD.

This function is array safe, i.e., an array of years can be given.

\qualifiers{
\qualifier{mjd}{Return dates in  MJD, not in JD.}
\qualifier{orthodox}{Return the JD of Easter Sunday using the rules
                  appropriate for Eastern Christianity such as the
                  Russian Orthodox church.}
\qualifier{get_date}{Return date as a struct (year, month, date, hour,
                  minute, second). If the orthodox keyword is given,
                  the date will be in the Julian calendar, otherwise
                  it will be in the Gregorian calendar.}
\qualifier{backward}{An undocumented function with the same name
                  as this one existed in the isisscripts until
                  17 January 2020. It returned the date of Easter Sunday
                  in the Gregorian calendar as the number of days
                  since -1 March (sic!) of the given year. This
                  qualifier switches the current function to this
                  backward compatible behavior.}
}
\seealso{DateOfJD}
\done

\function{EchoMap}
\synopsis{reprocesses a signal on a given surface}
\usage{Struct_Type EchoMap(
      Struct_Type surface, Struct_Type signal
      Vector_Type observer[, Double_Type length]
    );}
\qualifiers{
    \qualifier{c}{defines the speed of light (default: 1)}
    \qualifier{noFiniteC}{infinite speed of light}
    \qualifier{noDoppler}{disable Doppler Shift}
    \qualifier{powRedist}{reference to a function returning the re-
                distributed power of all surface elements
                (i.e., all parameters are arrays!) in
                direction to the observer. The parameters
                passed are
                  * the power to be redistributed
                  * the cosine of the angle between the
                    element and the observer
                  * the surface element as structure with
                    the corresponding fields as below
                  * wether the passed power is the surface's
                    intrinsic one (1) or incident power (2)
                  * a reference to a variable, which may be
                    set to 1 to trigger re-calculating the
                    powers (i.e. the given function is called
                    again for the two cases *after* it was
                    called for both cases in the first place;
                    might be useful if, e.g., the incident
                    power changes properties of the surface)
                This function is called twice: (1) for the
                intrinsic power of a surface element and
                (2) the reprocessed relative (!!) power. The
                last parameter specifies these two cases.
                If you need to pass additional qualifiers
                you may set the powRedistQual-qualifier
                to a structure holding the qualifiers
                (default: power*cosine_to_observer)}
    \qualifier{response}{response function of a surface element,
                see EchoMap_makelc for details}
    \qualifier{reproment}{instead of returning the echo signal the
                reproment structure is returned (see text)}
}
\description
    This function propagates the given 'signal' on an object
    defined by a 'surface'. There the signal is absorbed and
    re-emitted (reprocessed) by each surface element. The
    resulting response signal, as seen in direction to an
    'observer', is returned by calling EchoMap_makelc after
    all geometrical have been calculated (called reproment
    here). If the corresponding qualifier is set, the latter
    call is skipped and the reproment structure is returned
    instead. This structure has the following fields:
      visible_obs - boolean value if the surface is visible
      visible_sig   by the observer and the signal origin
      lt_surf - light travel times from the signal to the
      lt_obs    surface and from there to the observing plane
      doppler - Doppler factor in direction to the observer
      flux_int - intrinsic flux of the surface as seen by
                 the observer
      flux_rec - received signal flux of the surface
      flux_emi - emitted relative flux of the surface as seen
                 by the observer

    The following effects are taken into account during the
    calculation of the response signal:
    - light travel time between each surface element to the
      signal source and to the observing plane
    - Doppler shift by a moving surface
    - effective areas and projection effects
    - power redistribution function at the surface (qualifier
      'powRedist')
    - power response function of the surface (qualifier
      'response', see EchoMap_makelc for details)

    Restrictions:
    - sqrt(surface.A) << vector_norm(surface.r-signal.origin)
      (small surface elements relative to distance to source)
    - surface.v << c
    - vector_norm(observer) != 1
    - signal fields have to be sorted by time
                
    all qualifiers are passed to EchoMap_makelc
    
    The 'surface' structure contains the surface elements,
    defined by the fields
      Vector_Type[] r - position vector to each element
      Double_Type[] A - area of each element
      Vector_Type[] n - normal vector of each element
      Vector_Type[] v - velocity vector of each element
                        (optional)
      Double_Type[] L - intrinsic power of each element,
                        which will be added to the response
                        signal (optional, in energy/time)
    The 'signal' is treated as power (energy/time) as a
    function of time and defined by the fields
      Double_Type[] time  - lower time bin (in seconds)
      Double_Type[] power - power in each time bin
      Vector_Type origin  - position vector to the signal's
                            source (default: (0,0,0))
    Any length is considered to be in units of the speed of
    light (lt-s). If another unit is used, the internal speed
    of light has to be changed via the 'c' qualifier.
\seealso{EchoMap_makelc, EchoMap_binary, Vectory_Type}
\done

\function{EchoMap_binary}
\synopsis{reprocesses a signal on the companion of a binary}
\usage{Struct_Type EchoMap_binary(
      Struct_Type surface, Double_Type lum, Struct_Type signal,
      Struct_Type orb, Double_Type phiorb, Double_Type mq
      [, Double_Type length]
    );}
\qualifiers{
    \qualifier{remark: }{any qualifiers are passed to 'EchoMap'}
}
\description
    Uses the 'EchoMap' function to reprocess an input 'signal'
    on the 'surface' of the secondary star with total luminosity
    'lum'. The fields required for the 'surface' and 'signal'
    structure are described in the 'EchoMap' help. The
    'Roche_lobe_surface' function might be useful to calculate
    the deformed surface of the star within the Roche potential.
    Note, that the velocities of the surface elements are
    calculated automatically if no 'v' field is specified. If
    not specified, the luminosity field 'L' for each surface
    element is calculated as well. Thereby, it is assumed that
    each surface element has the same area luminosity, such
    that the total luminosity still is 'lum'.
    The orbit of the binary is described by the 'orb'ital
    structure an has to contain the fields as described in the
    'check_ephemeris' help. In addition, the inclination can
    be specified optionally via the 'i' field. To complete the
    description of the binary, the mass ratio
      mq = M_primary / M_companion
    has to be given at last.
    As default, the source of the signal is the position of
    the primary star. It can be changed by setting the 'origin'
    field of the signal structur to the position vector.
    Using the 'phiorb' parameter the orbital phase is defined,
    which corresponds to the viewing angle on the binary.

    As a conclusion and for further details the reference
    frame is shown in the following:
#v+
                                   position of M2 depending
      omega (right handed)         on orbital phase
      observer (i = 0)
        z                                 0.50
        |  y                              ---
        | /                              |   |
        |/                         0.75 |  x  | 0.25
    M1  o ---- x                         |- -|
       /                                   | observer
      o M2 (phiorb = 0)                  0.00
#v-
    - orbital plane = x-y-plane (fix)
    - inclination and orbital phase is realized by rotating
      the observer accordingly
    - distance M_1 to M_2 = 1 (fix)

    WARNING: at the moment circular orbits (ecc = 0) are
             are implemented only!
    NOTE:    for elliptical orbits, the shape of the stars
             depends on the orbital phase
\seealso{EchoMap, Roche_lobe_surface}
\done

\function{EchoMap_makelc}
\synopsis{takes reprocessing information to produce a response signal}
\usage{Struct_Type EchoMap_makelc(
      Struct_Type reproment, Struct_Type signal[, Double_Type length]
    );}
\qualifiers{
    \qualifier{noDoppler}{disable Doppler Shift}
    \qualifier{response}{reference to a function returning the
                relative emissivity of a surface element
                as response to an incoming power peak at
                t = 0. For details see the description
                (default: delta peak at t=0)}
    \qualifier{surfsigs}{reference to a variable, which will get the
                output signal for each element (2dim-array
                defined as [element,power]; use the time
                from the returned total signal)}
}
\description
    Takes the 'reproment' structure of a former run of
    EchoMap and returnes the output signal produced by
    reprocessing the input 'signal' on the surface used
    to caluclate the reproment (i.e., the geometrical
    effect of the reprocessing). By default, the returned
    power vs. time is the total response to the incomming
    signal. However, if the optional argument 'length' is
    provided, the input signal is periodically repeated
    until the output signal has at least the given length.
    Furthermore, the signal is repeated backwards in time
    as well such that no rising or declining parts of the
    echo is visible in the output.

    By default, it is assumed that each surface element
    mirrors the incoming power instantaneously. More complex
    behaviour can be implemented easily using the 'response'
    qualifier, which defines a function returning the
    relative 'power' emissivity over 'time' (in seconds) of
    a surface element. Furthermore, the response has to
    fullfil energy conservation such that the integrated
    power does not exceed 1, but may be less, i.e. power is
    reprocessed via non-radiative processes. The time
    resolution of the response has to be equal or better
    than the input signal!
\seealso{EchoMap}
\done

\function{ecliptical2equatorial}
\synopsis{convert ecliptical (lambda,beta) to equatorial coordinates}
\usage{(alpha,delta)=ecliptical2equatorial(lambda,beta;qualifiers)}
\altusage{Vector_Type eqp=equatorial2ecliptical(ecl;qualifiers)}
\qualifiers{
\qualifier{equinox}{equinox of the transformation. Float: JD, string: equinox
                  designation (e.g., "J2000.0" or "B1950.0";
                  default: J2000.0)}
\qualifier{deg}{interpret angular arguments in degrees (default is radians!)
                applies also to the return value.}
\qualifier{mjd}{interpret equinox as a MJD (default: JD)}
\qualifier{true}{perform calculation for apparent coordinates, i.e., include nutation terms}
}
\description
  The routine takes coordinates in the ecliptical (lambda, beta)
  system and converts them into equatorial coordinates (right ascension,
  declination), using the obliquity of the ecliptic for the equinox
  and vice versa.

  Alternatively, the routine also accepts a Vector_Type and then
  returns an Vector_Type vector in equatorial coordinates (or
  vice versa if the inverse qualifier is given).

  The default equinox is J2000.0.

  This routine just calls equatorial2ecliptical with all arguments and the
  inverse qualifier, but is a simpler interface.

\seealso{Vector_Type, equatorial2ecliptical, JDofEpoch,dms2deg,hms2deg}
\done

\function{edit_var}
\synopsis{allows to edit S-Lang variables in an editor}
\usage{edit_var(&x);
\altusage{Any_Type y = edit_var(Any_Type x);}
}
\qualifiers{
\qualifier{tmpfile}{temporary file [default: /tmp/edit_var_$UID_$PID]}
}
\description
    edit_var supports the following data types:
    - Undefined_Type (=Void_Type), Null_Type,
    - Integer_Type, Double_Type, Complex_Type
    - String_Type, BString_Type
    as well as
    - Array_Type
    - Assoc_Type
    - Struct_Type
    - List_Type
    in a recursive way. (Circular linked list
    are currently not supported.)

    S-Lang code defining the variable x is shown
    in the editor specified by the EDITOR environ-
    ment variable or jed, if EDITOR is undefined.
    edit_var uses jed's folding mode (one should
    run 'Buffers' => 'Folding' => 'Enable Folding')
    for nested data structures, which can hence be
    very easily investigated.

    The S-Lang code for x can be modified. After
    saving the temporary file and closing the editor,
    the file is evaluated, which should return an
    S-Lang object that is either stored in x
    (if passed by reference) or returned.
\example
    \code{i = edit_var(struct { uc='A', s=1H, us=1UH, l=1L, ul=1UL, ll=1LL, ull=1ULL });}
\done

\function{element2Z}
\synopsis{returns Z for an element symbol or name}
\usage{z=element2Z(String_Type name)}
\description
 This function returns the nuclear charge for the given element symbol
 symbol or element name.

 This function is array safe.
\seealso{element_name, element_symbol}
\done

\function{element_name}
\synopsis{returns the name of the element with proton number Z}
\usage{String_Type element_name(Integer_Type Z)}
\qualifiers{
\qualifier{lc}{return symbol or name in lower case (default: Capitalized)}
}
\description
 This function returns the name of the element with
 nuclear charge Z for all named elements.

 This function is array safe.
\seealso{element_symbol,element2Z}
\done

\function{element_symbol}
\synopsis{returns the symbol of the element with proton number Z}
\usage{String_Type element_symbol(Integer_Type Z)}
\qualifiers{
\qualifier{full}{return full element name rather than symbol}
\qualifier{lc}{return symbol or name in lower case (default: Capitalized)}
}
\description
 This function returns the symbol or the name of the element with
 nuclear charge Z for all named elements. 
 Z=0 returns 'Bare'.

 This function is array safe.
\seealso{element_name, element2Z}
\done

\function{ellipse}
\synopsis{calculates points on an ellipse centered at the origin}
\usage{(Double_Type X,Y) = ellipse(Double_Type smaj, smin, posang, phi)
}
\qualifiers{
\qualifier{x}{[\code{=1}] compression/streching factor along the x-axis}
\qualifier{y}{[\code{=1}] compression/streching factor along the y-axis}
}
\description
    This function calculates the coordinates \code{X}, \code{Y} of an ellipse with
    semimajor axis \code{smaj}, semiminor axis \code{smin}, and position angle \code{posang}
    (measured in rad with respect to the x-axis) which is parameterized with \code{phi}.
    The complete ellipse is covered when \code{phi} covers the values from 0 to 2*PI.
\examples
    variable phi=[0:2*PI:#200];
    plot ( ellipse(5,3,0.2*PI,phi) );
    oplot( ellipse(5,3,0.2*PI,phi ; x=0.5) );
\seealso{enclosing_ellipse}
\done

\function{emcee--driver}
\synopsis{Set emcee parallel computation method}
\usage{driver="method;options"}
\description
  The driver method can be set with the function string
    "method;parameter"

  Available methods:
  serial : The serial driver. No parallelization at all

  fork : The fork (& socket) parallel driver. Per default uses
          _num_cpus many tasks.
    ; tasks : [=_num_cpus] Number of total processes used

  mpi : The mpi parallel driver using as many nodes as registered
        in an mpi environment
\done

\function{emcee--file}
\synopsis{Set emcee file input and output methods}
\usage{input="method;options"
  \altusage{output="method;options"}}
\description
  The file input/output methods can be set with the function string
    "method;parameter"

  Available methods:
  fits : Fits file interface to write the chain as fits table extension
    ; filename  : [emcee-<date>.fits] The input/output file name.
    ; parameter : If given, on read we draw new starting positions from the
                  parameter settings stored in the file instead of reading
                  the last iterations.

  mike : Fits file interface (compatible to previous emcee routine)
    ; filename  : [emcee-<date>.fits] The input/output file name.
    ; parameter : if given, on read we draw new starting position from the
                  header
    ; cycle     : [=50] the number of steps to calculate before writing to
                  file

  par  : Parameter file interface to draw initial walkers from parameter
         files.
    ; filename : [emcee-<data>.fits] Multiple parameter files can be separated
                 by a semi colon with an optional additional multiplier
                 (separated by a colon). A string of the form
                 'file1.par:2;file2.par' means that 2/3 of all walkers are
                 drawn from file1.par and 1/3 is drawn from file2.par.
\done

\function{emcee--init}
\synopsis{Set emcee initialization function}
\usage{init="method;parameters";}
\description
  The initialization method can be set with the function string
    "method;parameter"
  Initialization methods that read from file use the defined input
  method (default: fits).

  Available methods:
  uniform : Draw initial walker positions from a uniform distribution
            within the parameter ranges.
    ; width : [=1.0] Sub range used for initialization (must be <= 1).
                     The initiale cube center coincides as much as
                     possible with the current parameter values.

  gauss   : Draw initial walker positions from a gaussian distribution
            within parameter ranges.
    ; sigma : [=0.1] Sigma of the gauss function in terms of the
                     parameter range. I.e, sigma=1 is the full parameter
                     range. Allowed range between 1e-3 and 2 (outside will
                     be cliped).

  file    : Load initial walkers from a valid chain file created by the
            emcee method. This is used with 'continue'. The parameter ranges
            can be different compared to a previous run. This is also true
            for the number of walkers. In order to truely continue a chain
            those have to be set equal to the previous run.

  chain   : Draw initial walkers from an approximated CDF of an existing
            chain file.
    ; steps : [=10] The number of steps to concider for constructing the CDF
                 (from the end of the chain)
    ; rng   : [=&rand_uniform] uniform random number generator

\done

\function{emcee--progress}
\synopsis{Set emcee progress report}
\usage{progress="method;options"}
\description
  To get a progress report for the running emcee
  algorithm use one of the available options.

  Available report methods:
  none   : Do not report
  report : Report the number of steps done every n steps
    ; n : [=50] Report for every n steps.
    ; overwrite : if given, overwrite last status (useful for
       interactive sessions).
    ; format : [="Status: %D/%T (%%P)"] The report format
       where %D is the current step, %T total steps and %P
       the percentage.
\done

\function{emcee--step}
\synopsis{Set emcee step algorithm}
\usage{step="method;options"}
\description
  The step algorithm can be set with the function string
    "method;parameter"

  Available algorithms:
  stretch : The stretch move as described in Goodman & Weare 2010
    ; scale : [=2] Scale for the range of possible moves
\done

\function{emcee_chain_hist_collect}
\synopsis{Collect chain histograms from multiple emcee files}
\usage{Struct_Type[] chist = emcee_chain_hist_collect( String_Type[] InFiles, Interger_Type[] PID )}
\altusage{Struct_Type[] chist = emcee_chain_hist_collect( String_Type[] InFiles )}
\qualifiers{
\qualifier{chatty}{If given, enable informative output.}
\qualifier{ncut}{[=0] #sim. steps to be cut from the start of the chains.}
\qualifier{autocut}{If given, 'ncut' is automatically set, s.t. the faulty sim. steps
                      containing ZERO entries are deleted. These entries corresponds
                      to the initialisation of the walkers.}
\qualifier{id[=0]}{Index of the InFiles to check consistency to.}
\qualifier{nbin[=100]}{Number of histogram bins.}
\qualifier{pmin}{Double_Type[length(PID)]: Containing parmater lower limits.}
\qualifier{pmax}{Double_Type[length(PID)]: Containing parmater upper limits.}
}
\description
   Generate parameter histograms by collecting chains from several FITS Infiles
   created with emcee. It is required that all InFiles are based on the same model
   and data. Only InFiles are having the same 'model' key and same 'free_par'
   indices are taken into account.

   IMPORTANT is that a common random number server was used for all InFiles to
   ensure there statistical independency! This is not checked automatically!

   Chains from InFiles (passed the check) for each parameter in 'pid' are collected,
   that is added to a common histogram with 'nbin' bins. With 'pmin' and 'pmax'
   the limits of the histograms can be set manually, where 'pmin'/'pmax' must
   be an array of the same length as 'pid'.

   'id' sets the index in InFiles, which is used to perform the consistency check.

   With 'ncut' the number of simulation iterations at the beginning of the
   the chains which should be cut away are set.
   'autocut' can be used to set 'ncut' automatically, s.t. all iteration
   steps related to the walker initialization are cut away.

\seealso{emcee}
\done

\function{emcee_hammer}
\synopsis{Explore parameter space with MCMC method}
\usage{emcee_hammer (Int_Type);}
#c%{{{
\qualifiers{
  \qualifier{Basic Qualifiers}{}
  \qualifier{walkers}{[=10]: Number of walkers per parameter}
  \qualifier{continue}{If given (and possible set to a file) continue chain from this file
                         (using init="file", file="fits" per default)}
  \qualifier{cont}{same as 'continue'}
  \qualifier{infile}{Set the input file name for reading and continuing}
  \qualifier{outfile}{Set the output file name}
  \qualifier{clobber}{Ovwerwrite output file if exists}
  \qualifier{Advanced Qualifiers}{}
  \qualifier{init}{[="uniform" or "file"] The walker initialization method}
  \qualifier{driver}{[="mpi"] The parallelization method}
  \qualifier{step}{[="stretch"] The walker step algorithm}
  \qualifier{input}{[="fits"] The file reading method}
  \qualifier{output}{[="fits"] The file writing method}
  \qualifier{progress}{="none"] Show progres}
  \qualifier{urand}{[=&rand_uniform] PRNG for uniform numbers (Double_Type[] = urand(Int_Type))}
  \qualifier{upick}{[=&rand_int] PRNG to chose complement walker (Int_Type[] = upick(Int_Type, Int_Type, Int_Type))}
}

\description
  The MCMC parameter space exploration algorithm as described by
  Foreman-Mackey et al. The function expects that data and a model is loaded.
  The only input parameter gives the number of iterations the algorithm
  performs. The resulting walker positions are written to a file which can
  be set with the "outfile" qualifier.

  The function allows to choose other algorithms for the step proposition,
  the read and write routines and how the walker ensamble is initialized.
  To get more information about the methods read 'help emcee_<method>'.

  Per default a new chain is started when the function is called. To continue
  a chain use the "continue" qualifier. If the intention is to continue a
  previous chain, make sure the paremter ranges are set to the same values as
  for the previous run. \code{emcee_hammer} always uses the current settings.

  To set a prior for parameters use the \code{set_fit_constraint} interface.
  This ensures that the stored fit values are according to the posterior
  and not just the likelihood.

\seealso{emcee--init, emcee--step, emcee--driver, emcee--input, emcee--file, emcee--progress}
\done

\function{emcee_merge}
\synopsis{Merge emcee fits files}
\usage{emcee_merge( String_Type[] InFiles, String_Type OutFile)}
\altusage{emcee_merge( String_Type[] InFiles )}
\qualifiers{
\qualifier{chatty}{If given, enable informative output.}
\qualifier{force}{If given, overwrite existing OutFile.}
\qualifier{adjuststeps}{If given, do not disregard InFiles with different #simsteps.
                        Instead cut all InFiles to min(#simsteps).}
\qualifier{ncut}{[=0] #sim. steps to be cut from the start of the chains.}
\qualifier{autocut}{If given, 'ncut' is automatically set, s.t. the faulty sim. steps
                      containing ZERO entries are deleted. These entries corresponds
                      to the initialisation of the walkers.}
\qualifier{id[=0]}{Index of the InFiles to check consistency to.}
}
\description
   Merge several FITS Infiles created with emcee into one OutFile. It is required that
   all InFiles are based on the same model and data. Only InFiles are merged that
   have the same 'model' key and same 'free_par' indices compared to the first file
   in InFiles.

   IMPORTANT is that a common random number server was used for all InFiles to
   ensure their statistical independency! This is not checked automatically!

   InFiles (passed the check) are merged in a sense that the different walkers
   from all InFiles are incorporated into a common chain, e.g., if InFiles are
   two files with 5 free Parameters, 11 walkers and 1000 nsteps, respectively,
   then the OutFile has 5 free Parameters, 22 walkers and 1000 nsteps.

   To be able to merge this way NSTEPS in all InFiles has to be the same. InFiles
   with different NSTEPS will be disregarded. With the 'adjuststeps' qualifier
   it is possible to cut all InFiles to the mininmal occuring NSTEPS. NOTE that
   this means to throw away simulation iterations in all InFiles.

   With 'ncut' the number of simulation iterations at the beginning of the
   the chains which should be cut away are set.

   An existing OutFile can be overwritten with the 'force' qualifier.

   If no OutFile is given, 'string_intersection' is used to determine the
   OutFile path/name!

\seealso{emcee}
\done

\function{empty_struct}
\synopsis{Returns a struct with 0 fields}
\usage{ Struct_Type = empty_struct();}
\description
  This function returns a struct with 0 fields,
  which is usable with other funtions, e.g.,
  struct_field_exists.
\done

\function{enclosing_ellipse}
\synopsis{calculates the smallest ellipse enclosing two ellipses centered at the origin}
\usage{(Double_Type smaj,smin,posang) = enclosing_ellipse(Double_Type smaj1, smin1, posang1, smaj2, smin2, posang2)
}
\description
    This function calculates the semimajor axis \code{smaj}, the semiminor axis \code{smin},
    and the position angle \code{posang} of the smallest ellipse enclosing
    two ellipses centered at the origin.
\examples

    smaj1 = 5.; smin1 = 2.; posang1 = 0.3;
    smaj2 = 4.; smin2 = 1.; posang2 = 0.7*PI;
    variable phi=[0:2*PI:#200];
    
    plot ( ellipse( enclosing_ellipse(smaj1, smin1, posang1, smaj2, smin2, posang2) ,phi) );
    oplot( ellipse(smaj1, smin1, posang1 ,phi) );
    oplot( ellipse(smaj2, smin2, posang2 ,phi) );
\seealso{ellipse}
\done

\function{energyflux}
\synopsis{evaluates the energy flux of the current fit model in a given energy range}
\usage{Double_Type energyflux(Double_Type Emin, Emax)}
\qualifiers{
\qualifier{factor}{[=\code{1.001}] step size of the logarithmic energy grid}
\qualifier{Emin}{minimum energy of (extended) grid}
\qualifier{Emax}{maximum energy of (extended) grid}
\qualifier{cgs}{return flux in erg/cm^2/s}
}
\description
    This function calculates the energy flux of the current best fit model
    in keV/cm^2/s by integrating over the model on a logarithmic energy grid. 
    The logarithmic step size of the model is given by the \code{factor}
    qualifier.
    
    In other words, energyflux returns\n
      \code{int_{Emin}^{Emax} E * S_E(E) dE}  (in keV/s/cm^2)
    where \code{S_E(E)} (in 1/s/cm^2/keV) is defined by \code{fit_fun}.
    
    If the fit-function is a convolution model, e.g., Compton reflection,
    it may be necessary to use an extended grid defined by the \code{Emin} 
    and \code{Emax} qualifiers.

    Note: While physicists generally use the term ``flux density'' for 
    the quantity returned by this function, with the exception of radio
    astronomy astronomers generally call the returned value the ``flux''.
    The change in function name is based on the experience that most
    users of isisscripts did not recognize that this function is what
    they required.

\seealso{eval_fun, eval_fun_keV, luminosity}
\done

\function{energyfluxdensity}
\synopsis{deprecated function}
\usage{do not use}
\description
    This function had the wrong name. Please use the energyflux function
    instead.
\seealso{energyflux}
\done

\function{enflux (fit-function)}
\synopsis{fits the photon flux in a given energy range}

\qualifiers{
    \qualifier{norm}{If given a reference to a variable, store the
                     normalization factor in there.}
}
\description
    This function can be used as a convolution model to determine
    the energy flux [keV/s/cm^2] of the model in the energy range
    given by \code{E_min} and \code{E_max}. Only bins within(!) this energy range
    are considered for the calculation of the flux. The energy
    flux is calculated by multiplying the photon flux in each bin
    by the mean energy of the bin. For that reason the model should
    be evaluated on a fine grid. It is strongly recommended to use
    a very fine user grid for this purpose.
    As this function fits the normalization of the total convolved
    model, the normalizations of its components are not defined
    absolutely, but only relativ to each others. For that reason it
    is meaningful to freeze the normalization of one component, at
    best the one of the continuum to avoid ambiguities during
    fitting.

    IMPORTANT:
    1) E_min and E_max have to be completely covered by the data grid
    2) the function requires a fine grid for evaluation
    3) absolute normalizations of functions convolved with enflux are useless
    
    If the 1) and 2) are not fulfilled by the data (RMF grid), a user
    grid has to be used (see example). In case 1) the command:
    \code{set_kernel (data_id, "std;eval=all");} is required in order to
    evaluate the model on bins outside of the range of data set \code{data_id}
    (see set_eval_grid_method).

\examples
    % data definition:
    \code{variable lo = _A([1:10]);}
    \code{variable hi = make_hi_grid(lo);}
    \code{variable my_data = define_counts(lo,hi,lo,sqrt(lo));}\n
    \code{variable my_emin = 2.5;}
    \code{variable my_emax = 6.5;}
    % defining a fine user grid
    \code{define fine_grid(id, s)}
    \code{\{}
    \code{   (s.bin_lo,s.bin_hi) = _A(log_grid(1,10,1000));}
    \code{   return s;}
    \code{\}}
    
    \code{set_eval_grid_method (USER_GRID, my_data, &fine_grid);}\n
    \code{fit_fun("enflux(1,powerlaw(1))");}

    \code{set_par("enflux(1).E_min",  my_emin,  1); % keV}
    \code{set_par("enflux(1).E_max",  my_emax,  1); % keV}
    \code{freeze("powerlaw(1).norm");}
    \code{()=fit_counts();}
    \code{list_par;}


    % It is also possible to determine only the flux of certain
    % model components, e.g., the unabsorbed flux:
    \code{fit_fun(phabs(1)*enflux(1, powerlaw(1)));}

\seealso{phflux, set_eval_grid_method}
\done

\function{enlarge_image}
\synopsis{enlarge an image to subpixel resolution by 2d interpolation}
\usage{Double_Type IMG = enlarge_image(img, Integer_Type n);
\altusage{Double_Type IMG = enlarge_image(img, Integer_Type nx, ny);}
}
\qualifiers{
\qualifier{interp}{reference to interpolation function (see below)}
\qualifier{y_first}{interpolate first in y-, then in x-direction}
}
\description
    The pixels of \code{img} are mapped to every \code{nx}-th pixel
    in x-direction and every \code{ny}-th pixel in y-direction
    (in the first usage, nx = ny = n) of \code{IMG}:
#v+
       IMG[ [::ny], [::nx] ] = img;
#v-
    As no extrapolation is performed at the boundary,
    width and height of \code{IMG} are smaller than \code{nx*w} and \code{ny*h},
    where \code{w} and \code{h} are width and height of \code{img}.

    Intermediate pixels are interpolated using the function
    specified by the \code{interp} qualifier, which defaults to
    \code{gsl->interpol_cspline} if the gsl module is available,
    and otherwise to ISIS' \code{interpol} function. In general,
    it can be a reference to any function of the form
#v+
       newy[] = interpol(newx[], oldx[], oldy[]);
#v-
    All qualifiers of \code{enlarge_image} are passed to \code{@interp}.

    The interpolation is first performed in x-direction
    and then in y-direction -- unless the \code{y_first} qualifier
    is specified. For linear and cubic spline interpolation,
    the final result is independent of the order.
\done

\function{epatplot}
\usage{epatplot(Struct_Type events);}
\done

\function{epferror}
\synopsis{Estimate epoch folding uncertainty with Monte-Carlo simulation
    approach.}
\usage{(Double_Type err) = epferror(Double_Type time, rate, period[, rate_error]);}
\qualifiers{
\qualifier{pstart}{start period for epoch folding period search.
      (default: 0.5*period)}
\qualifier{pstop}{stop period for epoch folding period search.
      (default: 1.5*period)}
\qualifier{ntrial}{number of MC iterations. (default: 20)}
\qualifier{pget}{function reference to determine the period from the
            chi^2 landscape. Arguments passed are
              Double_Type[] p, stat
            as returned by epfold. Has to return the period and -1
            if the period could not be determined. (default: find
            maximum, see below)}
\qualifier{pdist}{set to a variable reference in order to retrieve
             the simulated period distribution.}
\qualifier{chatty}{chattiness of this function: If >0 print some debug
              messages. If >1 plot the chi^2 landscape at each MC
              iteration. (default: 0)}
\qualifier{fchatty}{chattiness piped to epfold.}
}
\description
    Note: All qualifiers are passed to pfold (for pulse profile
    calculation) and epfold (for period search)! 
 
    This routine tries to estimate the uncertainty of a previously
    received period using the epoche folding approach (see epfold).
    It is adopted from the IDL script with the same name, but does
    not yet allow for GTI or Poisson statistics. It implements the
    following strategy:
    1.) calculate a mean profile with given period. 
    2.) compute the intensity for all times applying the
        period multiplied profile.
    3.) simulate an uncertainty for all times (assuming normal
        distribution with sigma = error, or, if not given
        sqrt(rate)).
    4.) perform epoch folding for that simulated lightcurve.
    5.) determine the maximum of the epoch folding chi^2 landscape
        and remember the corresponding period. Use the 'pget'
        qualifier in order to implement a user-defined approach!
    6.) go to step 2.) Ntrial times, which results in a distribution
        of determined periods.
    7.) compute the standard deviation of the period distribution
        obtained and return this as the uncertainty of the intial
        epoch folding
 
    NOTE: The processing may last a long time (be prepared to wait  
          hours to weeks)!

    NOTE: Is is _important_ to use the same qualifiers used for
          epfold previously for the actual data! That is epferror
          has to search for the period in the same way as for the
          data! This might further require to provide a function
          for the determination of the best period from an epoch
          folding result (see 'pget' qualifier).

    References:
       Davies, S.R., 1990, MNRAS 244, 93
       Larsson, S., 1996, A&AS 117, 197
       Leahy, D.A., Darbro, W., Elsner, R.F., et al., 1983,
          ApJ 266, 160-170
       Schwarzenberg-Czerny, A., 1989, MNRAS 241, 153

\done

\function{epfold}
\synopsis{peforms epoch folding on a lightcurve in a given period
interval}
\usage{(Struct_Type res) = epfold(Double_Type t, r, pstart, pstop);
\altusage{(Struct_Type res) = epfold(Double_Type t, pstart, pstop) ; (event data)}}

\qualifiers{
\qualifier{nbins}{number of bins for the pulse profile (default=20)}
\qualifier{exact}{calculate the pulse profile in a more exact way, see description of pfold (not recommended as it takes a very long time!).}
\qualifier{dt}{exposure of every lightcurve time bin, should be given to ensure correct results.}
\qualifier{sampling}{how many periods per peak to use (default=10)}
\qualifier{nsrch}{how many periods to search in a linear grid (default not set)}
\qualifier{dp}{delta period of linear period grid (default  not set)}
\qualifier{lstat}{use L-statistics instead of chi^2 statistics}
\qualifier{chatty}{set the verbosity, chatty-1 is piped to pfold (default=0)}
\qualifier{gti}{GTIs for event data, given as struct{start=Double_Type, stop=Double_Type}} 
}


\description
   Performs epoch folding on a given lightcurve between the periods
   pstart and pstop. The function was adopted from
   the IDL routine of the same name. GTI correction only implemented
   for event-data yet.

   By default, periods are sampled according to the triangular rule
   for estimating the period error, using "sampling" periods per peak.
   If a linear grid is to be used, either "dp" for a given distance
   between two consecutive periods or "nsrch" for a given number of
   periods can be given. These qualifiers are mutually exclusive. 

   The returned structure "res" contains four fields: "p" for the
   evaluated period and "stat" for the value of the statistic used.
   Additionally the field "nbins" contains the number of phase bins
   used and "badp" contains indices for res.p marking periods
   where the pulse profile showd empty phase bins. Values of
   res.stat[res.badp] should be taken with great care!
        
   Compared to the similar function sitar_epfold_rate, epfold.sl
   uses the chi^2 statistic as a default and is based on pfold.sl,
   which can take errors of the lightcurve into account.
   If the qualifier "lstat" is given, the statistic is switched to
   the l-stat statistic as in sitar_epfold_rate, but errors of the
   lightcurve are no longer taken into account. lstat is not
   available for event-data.

   If the "exact" qualifier is given, the function takes the exposure
   time of every time bin into account in the sense that, that a
   given time bin may overlap over two phase bins. The corresponding
   exposure time in every phase bin is reduced accordingly.

   NOTE: the "exact" qualifiers slows the script considerably down,
   depending on the length of the lightcurve and the number of bins
   for up to a factor of >100!
   
   If "exact" is not given, the script is ~10% slower than
   sitar_epfold_rate.
   
   The script is still in the develepment phase, please report any
   bugs or missing features to Felix
   (felix.fuerst@sternwarte.uni-erlangen.de).
   
   NOTE: Please read  
          Davies, S.R., 1990, MNRAS 244, 93 (L-statistics!)
          Larsson, S., 1996, A&AS 117, 197
          Leahy, D.A., Darbro, W., Elsner, R.F., et al.,1983, ApJ 266, 160-170
          Schwarzenberg-Czerny, A., 1989, MNRAS 241, 153
   
\done

\function{epfoldpdot}
\synopsis{peforms epoch folding on a lightcurve in a given period
interval}
\usage{(Struct_Type res) = epfoldpdot(Double_Type t, r, pstart, pstop);
or (Struct_Type res) = epfoldpdot(Double_Type t, pstart, pstop) ; (event data)}

\qualifiers{
\qualifier{nbins}{number of bins for the pulse profile}
\qualifier{exact}{calculate the pulse profile in a more exact way, see description of pfold (not recommed as it takes a very long time!).}
\qualifier{dt}{exposure of every lightcurve time bin, should be given to ensure correct results.}
\qualifier{sampling}{how many periods per peak to use (default=10)}
\qualifier{nsrch}{how many periods to search in a linear grid (default not set)}
\qualifier{dp}{delta period of linear period grid (default  not set)}
\qualifier{lstat}{use L-statistics instead of chi^2 statistics}
\qualifier{chatty}{set the verbosity, chatty-1 is piped to pfold (default=0)}
\qualifier{gti}{GTIs for event data, given as struct{start=Double_Type, stop=Double_Type}}
\qualifier{pdstart}{start p-dot for grid search (default=0)}
\qualifier{pdstop}{stop p-dot for grid search (default=0)}
\qualifier{pdnsrch}{search point for p-dot grid, (default = nsrch
(if defined, otherwise 10))}
}


\description
   Performs epoch folding on a given lightcurve between the periods
   pstart and pstop and period derivatives (p-dot) pdstart and pdstop.
   The function was adopted from
   the IDL routine of the same name. GTI correction only implemented
   for event-data yet.

   By default, periods are sampled according to the triangular rule
   for estimating the period error, using "sampling" periods per peak.
   If a linear grid is to be used, either "dp" for a given distance
   between two consecutive periods or "nsrch" for a given number of
   periods can be given. These qualifiers are mutually exclusive.

   P-dot is always sampled on a linear grid with  pdnsrch points.

   The routine uses the S-lang function 'parallel_map' to use
   Isis_Slaves.num_slaves cores on your machine for parallel
   computing. On a MacBook Pro with quadcorse i7 this gives a speed
   improvement of a factor ~>2 over single core calculations.

   The returned structure "res" contains five fields: "p" for the
   evaluated period, pd for the evaluated period derviates,
   and "stat" for the value of the statistic used. "stat" is a 2D
   array of dimnesion (np, npd).
   Additionally the field "nbins" contains the number of phase bins
   used and "badp" contains indices for res.p marking periods
   where the pulse profile showd empty phase bins. Values of
   res.stat[res.badp] should be taken with great care!

   Compared to the similar function sitar_epfold_rate, epfold.sl
   uses the chi^2 statistic as a default and is based on pfold.sl,
   which can take errors of the lightcurve into account.
   If the qualifier "lstat" is given, the statistic is switched to
   the l-stat statistic as in sitar_epfold_rate, but errors of the
   lightcurve are no longer taken into account. lstat is not
   available for event-data.

   If the "exact" qualifier is given, the function takes the exposure
   time of every time bin into account in the sense that, that a
   given time bin may overlap over two phase bins. The corresponding
   exposure time in every phase bin is reduced accordingly.

   NOTE: the "exact" qualifiers slows the script considerably down,
   depending on the length of the lightcurve and the number of bins
   for up to a factor of >100!

   NOTE: "exact" qualifier is currenlty untested and may lead to
   erronoeus results!

   If "exact" is not given, the script is ~10% slower than
   sitar_epfold_rate.

   The script is still in the develepment phase, please report any
   bugs or missing features to Felix
   (felix.fuerst@fau.de).

   NOTE: Please read
          Davies, S.R., 1990, MNRAS 244, 93 (L-statistics!)
          Larsson, S., 1996, A&AS 117, 197
          Leahy, D.A., Darbro, W., Elsner, R.F., et al.,1983, ApJ 266, 160-170
          Schwarzenberg-Czerny, A., 1989, MNRAS 241, 153

\done

\function{EpochofJD}
\synopsis{Returns the Julian or Besselian epoch of a (M)JD}
\usage{epoch=EpochofJD(jd;qualifiers)}
\qualifiers{
\qualifier{mjd}{argument is a modified julian date, not a JD}
\qualifier{besselian}{return the Besselian epoch}
\qualifier{julian}{return the Julian epoch (the default)}
}
\description
   This function converts a (modified) julian date into the corresponding
   Julian or Besselian epoch (e.g., J2000, B1950.0 etc.) using the formulae
   given by Lieske (1979, A&A 73, 282) for Besselian dates and the standard
   definition of the Julian Date. 

   If high precision is relevant, note that JD is assumed to be in TT,
   note that the epoch will slowly drift with respect to Gregorian year
   fractions such as those calculated by jd2year, but contrary to that
   function the Julian/Besselian epoch linearly maps to TT.

   This routine is array safe.

\seealso{JDofEpoch,jd2year}
\done

\function{equation_equinoxes}
\usage { egam = equation_equinoxes(JD;qualifiers)}
\qualifiers{
   \qualifier{mjd}{the time argument is in MJD, not in JD}
   \qualifier{deg}{return equation of equinoxes in degrees}
   \qualifier{arcsec}{return equation of equinoxes in arcseconds}
   \qualifier{mas}{return equation of equinoxes in milliarcseconds}
   \qualifier{seconds}{return equation of equinoxes in seconds}
}

\description
 This function calculates the equation of equinoxes, i.e., the 
 difference between the Greenwich Mean Siderial Time and the
 Greenwich Apparent Siderial Time caused by the motion of the
 equinox due to nutation. The implementation uses a truncated
 form of a longer series due to the IERS (2003), given by
 Eq. 2.14 of Kaplan (2009, USNO circular 181). The default
 returns the equation of equinoxes in radian (more commonly
 the values are tabulated in units of seconds).

 This function is array safe.

\seealso{gmst,nutation_angles}
\done

\function{equatorial2ecliptical}
\synopsis{convert equatorial (alpha,delta) to ecliptical (lambda,beta) coordinates}
\usage{(lambda,beta)=equatorial2ecliptical(alpha,delta;qualifiers)}
\altusage{Vector_Type ecl=equatorial2ecliptical(eqp;qualifiers)}
\qualifiers{
\qualifier{equinox}{equinox of the transformation. Float: JD, string: equinox
                  designation (e.g., "J2000.0" or "B1950.0";
                  default: J2000.0)}
\qualifier{deg}{interpret angular arguments in degrees (default is radians!)
                applies also to the return value.}
\qualifier{mjd}{interpret equinox as a MJD (default: JD)}
\qualifier{inverse}{perform the conversion from ecliptical to equatorial
                    coordinates (see ecliptical2equatorial() for an
                    equivalent interface)}
\qualifier{true}{perform calculation for apparent coordinates, i.e., include nutation terms}
}
\description
  The routine takes coordinates in the equatorial (right ascension, 
  declination) system and converts them into ecliptical coordinates 
  using the obliquity of the ecliptic for the equinox and vice versa..

  Alternatively, the routine also accepts an Vector_Type and then
  returns a vector in ecliptical coordinates (or vice versa if the inverse
  qualifier is given).

  The default equinox is J2000.0.

\seealso{Vector_Type, ecliptical2equatorial, JDofEpoch,dms2deg,hms2deg}
\done

\function{equatorial2galactic}
\synopsis{convert J2000.0 equatorial (alpha,delta) to galactic (l,b) coordinates}
\usage{(l,b)=equatorial2galactic(alpha,delta;qualifiers)}
\altusage{Vector_Type gal=equatorial2galactic(eqp;qualifiers)}
\qualifiers{
\qualifier{deg}{interpret angular arguments in degrees (default is radians!)
                applies also to the return value.}
\qualifier{inverse}{perform the conversion from galactic to equatorial
                    coordinates (see galactic2equatorial() for an
                    equivalent interface)}
\qualifier{blaauw}{use the original Blaauw definition of the IAU galactic
              coordinate system; see below for caveats}
}
\description
  The function takes coordinates in the equatorial (right ascension, 
  declination) system and converts them into galactic
  coordinates (or vice versa, if the inverse qualifier is given) for
  the IAU 1958 Galactic coordinate system (Blaauw et al., 1960,
  MNRAS 121, 123).

  Alternatively, the routine also accepts a Vector_Type and then
  returns a vector in galactic coordinates (or vice versa if the 
  inverse qualifier is given).

  The function is for J2000.0 coordinates only, use the
  precess function to convert to that equinox if needed.

  See Lane (1979, PASP 91, 405) for a discussion of the subtleties of
  this conversion and Johnson & Soderblom (1987, AJ 93, 864) for a
  non-standard derivation of the conversion matrix (the one used here
  is basically identical, but uses proper Euler angles). By default, 
  the conversion is done using the precessed pole and inclination for 
  the FK5 system (J2000.0) given by Liu et al (2011, A&A 526, A16), 
  which is a thorough derivation of values that are essentially 
  identical to those also given in the appendix of Reid & Brunthaler
  (2004, ApJ 616, 872).

  If the qualifier Blaauw is given, then the coordinates are first 
  precessed to B1950.0 and then transformed. Note that this approach
  is not good for the highest precision, since no conversion from,
  e.g., FK5 to FK4 coordinates is performed as this would result in a
  non-orthogonal coordinate system. Please use this qualifier only
  if you know what you are doing and read Liu et al. (2011) before
  using the qualifier.

\seealso{Vector_Type, galactic2equatorial,precess,dms2deg,hms2deg}
\done

\function{equatorial2horizon}
\synopsis{convert equatorial (alpha,delta) to horizon (elevation, azimuth) coordinates}
\usage{(azi,ele)=equatorial2horizon(alpha,delta;qualifiers)}
\altusage{Vector_Type hor=equatorial2horizon(eqp;qualifiers)}
\qualifiers{
 \qualifier{JD}{JD for which the calculation is to be performed (in UT1/UTC). Mandatory.}
 \qualifier{lon}{geographic longitude of the observer, positive towards the east. Mandatory.}
 \qualifier{lat}{geographic latitude of the observer, positive towards the north. Mandatory.}
 \qualifier{deg}{interpret all angular arguments in degrees (default is radians!)
                 applies also to the return value.}
 \qualifier{mjd}{interpret JD argument as a MJD (default: JD)}
 \qualifier{inverse}{perform the conversion from horizontal to equatorial
                     coordinates (see horizontal2equatorial() for an
                     equivalent interface)}
}
\description
  The routine takes coordinates in the equatorial (right ascension, 
  declination) system for the equinox and ecliptic of the date
  (i.e., "apparent coordinates"), and converts them into the elevation
  and azimuth for a given observer's position and time. Note that
  the JD, lon, and lat qualifiers are mandatory!

  The JD argument is to be measured in UT1, for most cases it is sufficient to
  approximate this as UT. Note that ephemerides work in TDB or TT, which has
  an offset of several tens of seconds. For most applications this should not
  matter, especially because of the uncertainty of refraction, but please take
  this into account if highest precision is needed.

  Formally the coordinates have to be topocentric coordinates in
  order to include parallax effects. For most applications this is not 
  important as long as a precision of a few arcsec or worse is needed.
  Eventually a routine applying all of these effects will be provided.

  Alternatively, the routine also accepts an Vector_Type and then
  returns a vector in horizon coordinates (or vice versa if the inverse
  qualifier is given).

\seealso{Vector_Type, horizontal2equatorial, JDofEpoch,dms2deg,hms2deg,tai2tt,utc2tai,tt2tdb}
\done

\function{eqwFit}
\synopsis{fit function for replacing a feature's norm by its equivalent width}
\usage{eqwFit(id, continuum, feature1, ..., featureN)}
\description
    The so-called equivalent width of a feature in a spectrum
    is a measurement for its flux F_{feature} compared to the
    underlying continuum F_{continuum}. It is defined as the
    width 'eqw' of a rectangle centered at the features
    maximum or minimum 'E_0' and with a height equal to
    the continuum at E_0:

      $\\int F_{feature}(E) dE = F_{continuum}(E_0) * eqw$

    As a result, the equivalent width stays constant if the
    flux of continuum and the feature are correlated.

    The equivalent width of a feature replaces its norm. Due
    to that, the norm has to be at a fixed value, e.g, 1.
    The fit function defined here then scales the feature
    to the equivalent width, given as a fit parameter.
    The function provies several fit parameters:
      E_min  - lower energy boundary for the feature's flux
      E_max  - higher energy boundary
      widthN - the equivalent width of the Nth feature
      multiN - if 1, the feature is considered to be
               multiplied with the continuum, otherwise
               its additive
    The returned value of the fit function is the continuum
    with all the given features applied!

    *IMPORTANT*
    The number of features the fit function can handle has
    to be set beforehand via 'eqwFit_init'. Once set, the
    number CAN NOT be changed afterwards. The number of
    given features has to be fulfilled EVERY TIME the fit
    function is used! If the function should be evaluated
    for less features, the remaining features have to be
    set to ZERO!

    *NOTE*
    Some multiplicative models do not provide a proper
    normalization, such that the norm N is defined like

      model = N * ...

    For example, although the gaussian absorption 'gabs'
    has a depth 'tau', due to its definition

      gabs = exp(-tau * exp(...))

    it CAN NOT be used with eqwFit properly! Instead,
    use additive models to mimic a multiplicative one:

      mygabs = 1 - egauss
\example
    % init the fit function to handle two features
    eqwFit_init(2);

    % equivalent width of a single gaussian,
    % the resulting model is equal to:
    % powerlaw(1) + egauss(1)
    fit_fun("eqwFit(1, powerlaw(1), egauss(1), 0)");%
    set_par("eqwFit(1).width1", 300); % eqw = 300eV
    set_par("eqwFit(1).multi1", 0); % additive
    set_par("egauss(1).area", 1, 1); % freeze area

    % equivalent width of an additive and
    % multiplicative gaussian,
    % the resulting model is equal to:
    % powerlaw(1)*(1 - egauss(2)) + egauss(1)
    fit_fun("eqwFit(1, powerlaw(1), egauss(1), 1 - egauss(2))");
    set_par("eqwFit(1).width2", 200); % eqw = 200eV
    set_par("eqwFit(1).multi2", 1); % multiplicative
    set_par("egauss(2).area", 1, 1); % freeze area
\seealso{eqwFit_init, eqw}
\done

\function{eqwFit_init}
\synopsis{initializes the fit function 'eqwFit'}
\usage{eqwFit_init(Integer_Type N);}
\description
    Defines and initializes the equivalent width fit
    function 'eqwFit' to handle 'N' number of features.
    Note, that the number has to be set before one can
    use the fit function and that it can only be set once!
\seealso{eqwFit, eqw}
\done

\function{erfinv}
\synopsis{Computes the inverse error function in abs(z)<1}
\usage{Double_Type erfinv(Double_Type z);}
\qualifiers{
   \qualifier{eps}{See description. Typically not needed.}
}    
\description
    This function computes the inverse error function, i.e.,
    erf(erfinv(x))=x. This can be used to calculate a confidence level by
    \code{sqrt(2)*erfinv(fraction)}

    By default the function uses two fast polynomial approximations
    by Mike Giles which have a relative accuracy of better than 1.2e-7
    over the whole interval between 0 and 1.

    If the eps qualifier is given, the method switches to a Newton-Raphson
    method that terminates once the relative error of the function is
    smaller than eps. This is significantly slower and only needed if
    an extremely high precision is needed (pretty much never). On Joern's
    laptop, this method needs on average 0.08ms per function evaluation
    compared to 0.03 mus for the polynomial approximation. 

    The function is array safe.

\seealso{erf [in gsl],cerf,cerfc}
\done

\function{erg2keV}
\synopsis{converts energy from erg to keV}
\usage{Double_Type new_value = erg2keV(Double_Type old_value)}
\qualifiers{
\qualifier{y_fac}{: divide the value by 10^{y_fac} }
}
\seealso{plot_unfold}
\done

\function{err_map_gaussian}
\synopsis{plots data points with their errorbars}
\usage{err_map_gaussian(x, [xErr,] y, yErr);
\altusage{err_map_gaussian(Struct_Type s);}
}
\qualifiers{
\qualifier{xerr}{               change 3-argument-syntax to \code{err_map_gaussian(x, xErr, y);}}
\qualifier{xminmax}{            changes the meaning of \code{xErr} -- and \code{x}, if \code{xErr} is not a list}
\qualifier{yminmax}{            changes the meaning of \code{yErr} -- and \code{y}, if \code{yErr} is not a list}
\qualifier{minmax}{             equivalent to both \code{x}- and \code{yminmax}}
\qualifier{x_pixel [=400]}{     number of x-axis bins of the image}
\qualifier{y_pixel [=400]}{     number of y-axis bins of the image}
\qualifier{i}{                  index-array of subset of data points to be plotted}
\qualifier{xmin [=min(x data)]}{minimal value of x-axis}
\qualifier{xmax [=max(x data)]}{maximal value of x-axis}
\qualifier{ymin [=min(y data)]}{minimal value of y-axis}
\qualifier{ymax [=max(y data)]}{maximal value of y-axis}
\qualifier{min_xerr [=1e-10]}{  minimal error for x-values}
\qualifier{min_yerr [=1e-10]}{  minimal error for y-values}
\qualifier{xlog}{               switch to logarithmic x-axis}
\qualifier{ylog}{               switch to logarithmic y-axis}
}
\description
    Plots a 2D gaussian for each data point. The given errors are used as the
    width(1 sigma) of the Gaussian profiles. If asymmetric errors are used, the
    profile is the combination of two Gaussians. The volume of each profile is
    normalized.
    
    In order to use asymetric errors for x and/or y,
    the correspondig \code{Err} argument has to be a list \code{{ Err1, Err2 }}.
    If one of the \code{minmax} qualifiers is used,
    the corresponding \code{Err} list contains directly minimum and maximum values.

    If one of the \code{minmax} qualifiers is used, but \code{Err} is not a list,
    the value and \code{Err} arguments actually mean minimum and maximum values.
    The actual value is infered to be the mean of minimum and maximum.
\examples
    % examples with symmetrical errorbars:\n
     plot_image( err_map_gaussian([1,2], [0.1,0.3], [1,1], [0.5,0.3];xmin=0,xmax=3,ymin=0,ymax=2) );\n
    \n
    % examples with asymmetrical errorbars:
     plot_image( err_map_gaussian([1], {[0.1],[0.3]}, [1], {[0.1],[0.1]};xmin=0,xmax=2,ymin=0,ymax=2) );\n
    \n
\seealso{plot_with_err}
\done

\function{eval_array}
\synopsis{evaluate an expression for an array of values}
\usage{Array_Type eval_array(Type_Type type, String_Type expression, Array_Type values)
\altusage{eval_array(String_Type expression, Array_Type values);}
}
\qualifiers{
\qualifier{marker}{[\code{="*"}] place holder in the \code{expression} string,
                     where the \code{values} are to be inserted}
\qualifier{n_markers}{[=all] number of markers in \code{expression} to be replaced;
                        if positive, the first \code{n_markers} are replaced,
                        if negative, the last \code{|n_markers|} are replaced.}
}
\description
    The \code{marker} string in \code{expression} is sequentially replaced with
    each value of the array \code{values} (note that the values are converted
    through the \code{string} function), and the resulting string is evaluated.
    Unless \code{type==Void_Type}, each evaluation is expected to result in one value
    of this \code{type}, and \code{eval_array} returns an array of all these values.
    If \code{type==Void_Type} (which can be omitted), no return value is expected.
\examples
    \code{%} set fields in an array of structures\n
    \code{public variable s = Struct_Type[8];  _for $1 (0, 7, 1)  s[$1] = struct { a, b };}\n
    \code{eval_array("s[*].a = 1", [0:3]);  % =>  s[[0:3]].a = 1;}\n
    \code{eval_array("s[*].a = 2", [4:7]);  % =>  s[[4:7]].a = 2;}\n

    \code{%} using the array several times\n
    \code{eval_array("s[*].b = *",     [0:3]);               % =>  s[[0:3]].b =  [0:3];}\n
    \code{eval_array("s[*].b = * * 2", [4:7]; n_markers=2);  % =>  s[[4:7]].b =  [4:7] * 2;}\n

    \code{%} using a different marker to clarify the last example\n
    \code{eval_array("s[#].b = 2 * #", [4:7]; marker="#");   % =>  s[[4:7]].b =  2 * [4:7];}\n
\seealso{array_map, array_struct_field, eval}
\done

\function{eval_fun2_keV}
\synopsis{evaluate a fit-function on a user-defined energy-grid}
\usage{flux = eval_fun2_keV(handle, E_bin_lo, E_bin_hi[, params[, args]]);}
\description
    The fit-function given by \code{handle} (\code{S_E(E)}) is evaluated
    on an arbitrary grid  defined by \code{E_bin_lo} and \code{E_bin_hi} (in keV):\n
       \code{flux = int_{E_bin_lo}^{E_bin_hi} S_E(E) dE}\n
    The unit of \code{flux} is ph/s/cm^2/bin.
\seealso{eval_fun2, eval_fun_keV}
\done

\function{eval_fun_keV}
\synopsis{evaluate the fit-function on a user-defined energy-grid}
\usage{Double_Type flux[] = eval_fun_keV(Double_Type E_bin_lo[], E_bin_hi[]);}
\description
    The currently defined fit-function \code{S_E(E)} is evaluated
    on an arbitrary grid  defined by \code{E_bin_lo} and \code{E_bin_hi} (in keV):\n
       \code{flux = int_{E_bin_lo}^{E_bin_hi} S_E(E) dE}\n
   Note that flux is bin integrated, i.e., the unit of \code{flux} is
   ph/s/cm^2. To plot the flux density (ph/s/cm^2/keV), you need to
   divide this quantity by the bin  width de=E_bin_hi - E_bin_lo. 
\seealso{eval_fun}
\done

\function{eval_simputfile}
\synopsis{evaluates the SIMPUT structure}
\usage{Integer_Type sucess = eval_simputfile(Struct_Type str);}
\description
    This function evalutes the SIMPUT structure, created for example
    with "get_simputfile_struct". Generally, all fields of the
    structure == NULL are skipped.
\qualifiers{
\qualifier{quiet}{don't show any output}
}
\seealso{create_basic_simputfile,get_simputfile_struct,set_simputfile_model_grid,set_simputfile_flux}
\done

\function{eval_xyfun}
\synopsis{evaluate current xy-function for points x [y]}
#c%{{{
\usage{(Double_Type[] x, Double_Type[] y) = eval_xyfun(Double_Type[] x [, Double_Type[] y]);}
\qualifiers{
\qualifier{id[=Isis_Active_Dataset]:}{ uses parameter and model for dataset #id}
}
\description
Evaluate the current xy-model on the points (x,y)
\seealso{define_xydata, xyfit_fun}
\done

\function{eval_xyfun2}
\synopsis{evaluate valid xyfit_fun string with parameters}
#c%{{{
\usage{(Double_Type[] x, Double_Type[] y) = eval_xyfun2 (handle, x[ , y, par])}
\qualifiers{
\qualifier{qualifier:}{ pass qualifier structure to function}
}
\description
Evaluates function \code{handle} (either the name of a xy-function or reference)
on the points x (and y) with parameters \code{par}.
\seealso{eval_xyfun, define_xydata, xyfit_fun}
\done

\function{exponential_xyfit}
\synopsis{linear xy fit function to be used with xyfit_fun}
\usage{xyfit_fun ("exponential");}
\description
    This function is not meant to be called directly!
    
    Calling \code{xyfit_fun ("exponential");} sets up a powerlaw fit
    function for xy-data. It has the form \code{y = norm*x^{-index}}
\seealso{xyfit_fun, define_xydata, plot_xyfit, linear_regression}
\done

\function{ext_info_string}
\synopsis{converts the ext_line_info into a string}
\usage{String_Type ext_info_string(Struct_Type info[, Integer_Type format])}
\description
    format=0 => string [default] \n
    format=1 => PGPLOT \n
    format=2 => TeX
\seealso{ext_line_info, ext_info_string}
\done

\function{ext_info_string_PGPLOT}
\synopsis{converts the ext_line_info into a PGPLOT string}
\usage{String_Type ext_info_string_PGPLOT(Struct_Type info)}
\seealso{ext_line_info, ext_info_string}
\done

\function{ext_line_info}
\usage{Struct_Type info = ext_line_info(Integer_Type id);
\altusage{Struct_Type info = ext_line_info(Integer_Type Z, Integer_Type ion, Integer_Type nr);}
}
\seealso{line_info}
\done

\function{e_bv}
\synopsis{Calculates the E(B_V) color excess after Predehl & Schmitt (1995)}
\usage{Double_Type = e_bv(Double_Type N_H);}
\qualifiers{
    \qualifier{R_V}{Scalar specifying the ratio of total to selective extinction
               R(V) = A(V) / E(B - V). If not specified, then R = 3.1
               Extreme values of R(V) range from 2.3 to 5.3}
}
\description
     From a given hydrogen absorption column density N_H in the diredtory
     of the object, the color excess E(B-V) is calculated after
     Predehl & Schmitt (1995): E(B-V) = N_H/(1.79e21*R_V).

 EXAMPLE
     Calculate the color excess for Cen A (RA 13h25m27.6s DEC -43d01m09s) for
     N_H = 8.09e20 and an "average" reddening of for the diffuse interstellar
     medium (R(V) = 3.1).

     isis> N_H = 8.09e20;
     isis> ebv = e_bv(N_H);
     isis> print(ebv);

\seealso{fm_unred;}         
\done

\function{factorial}
\synopsis{calculates n!}
\usage{Double_Type factorial(Integer_Type n);}
\description
    \code{n}!  \code{=  n * (n-1) * ... * 2 * 1}

    Note that \code{n} is always converted to an integer (without rounding);
    for fractional \code{n} use, e.g., the GSL module's Gamma function.
\seealso{gsl->gamma}
\done

\function{factorized_arf_rmf}
\synopsis{changes an ARF/RMF pair into factorized ARF/normalized RMF}
\usage{(newARF, newRMF) = factorized_arf_rmf(ARF, RMF);}
\seealso{factor_rsp}
\done

\function{Faddeeva}
\synopsis{Compute w(z) = exp((-iz)^2 erfc(-iz) for complex z}
\usage{Complex_Type[] Faddeeva(Complex_Type[]);}
#c%{{{%
\description
    This function uses continued fraction expansion and the algorithms described by
    Humlicek () and Hui () to compute an approximation to the Faddeeva function

    The algorithm used only allows to compute w(z) for Im(z)>=0, but using the relation
    w(-z) = 2*exp(-z^2)-w(-z) gives the remaining half.

    The derivative is given via dw/dz = 2i/sqrt(pi) - 2*z*w(z)
    \seealso{Faddeeva_dz}

    It is claimed that the algorithm has an accuracy of <4e-4.
\done

\function{Faddeeva_dz}
\synopsis{Compute derivative of Faddeeva function}
\usage{Complex_Type[] Faddeeva_dz (Complex_Type[]);}
\description
    Computes complex derivative of Faddeeva function
    \seealso{Faddeeva}
\done

\function{fake_pulsar_lightcurve}
\synopsis{creates a synthetic lightcurve of a pulsar}
\usage{Struct_Type synthetic_pulsar_lightcurve(
      Struct_Type lightcurve or Double_Type[] time,
      Double_Type or Struct_Type period[, Struct_Type orbit
      [, Struct_Type profile[, Struct_Type or Double_type fluxevolution
      [, Ref_Type noise_fun]]]]
    );}
\qualifiers{
    \qualifier{lcdt}{time resolution of the input lightcurve in days
           (default: difference of first two time bins)}
    \qualifier{interpol}{function reference used to to time-grid interpolations
               (default: &interpol_points)}
    \qualifier{pfold}{structure of qualifiers to be passed to 'pfold'
            (default: struct { nbins = 32, dt = ..., t0 = ..., pdot = ... })}
    \qualifier{fluxlc}{structure of qualifiers to be passed to 'pulse2pulse_flux_lc'
             (default: NULL)}
    \qualifier{tophase}{structure of qualifiers to be passed to 'pulseperiod2phase'
              (default: struct { t0 = ... })}
    \qualifier{chatty}{show or hide output messages (default: 1)}
}
\description
    This function fakes a pulsar's lightcurve including the following aspects:
    - longterm flux evolution (on timescales larger than the pulse period)
    - lightcurve modulation by pulse profile
    - pulse period change including orbital motion
    - gaussian or user-defined observation noise

    Two modi are possible:
    a) providing an observed initial lightcurve from which all needed aspects
       are derived. A pulse period or its evolution is mandatory. Certain
       aspects can be overwritten by user input. The resulting faked and
       the input lightcurve have the same time-grid.
    b) providing the output time-grid. The to be included aspects have to be
       given explicitely.
\seealso{check_pulseperiod_orbit_struct,pulse2pulse_flux_lc,pulseperiod2phase,pfold}
\done

\function{fancy_plot_unit}
\synopsis{Change the x-axis, and possibly the y-axis units, in the isis_fancy_plots package.}
\usage{fancy_plot_unit( String_Type [, String_Type]);}
\description

 fancy_plot_unit(xunit [,yunit]);

 Change the X-axis plot units to "xunit" (as for the ISIS command
 plot_unit; and change the Y-axis unit to "yunit" (default yunit=
 "photons").  These will be used for the functions: plot_counts,
 plot_data, plot_unfold, plot_fit_model. Unit names are case insensitive.

 Available X-units:

      eV, keV, MeV, GeV, TeV,
      Angstrom, A, nm, um, mm, cm, m,
      Hz, kHz, MHz, GHz,
      psd   (used for plotting power spectra from SITAR)

 Available Y-units:

      photons (default), mJy, ergs, watts, psd_leahy, psd_rms

 Units added via add_plot_unit are also supported.

 **NOTE**: Y-units photons/mJy/ergs/watts affect only plot_unfold, while
 psd_* are for plot_counts, but will also affect plot_data/plot_unfold.

 Fundamentally, power=1 is proportional to photons/cm^2/s/xunit
 (plot_unfold) or Counts/bin (plot_counts), with higher (lower) powers
 multiplying (dividing) by xunit. plot_data is always Counts/sec/xunit
 ("power" has no effect).

 mJy: y-unit for plot_unfold/power=2 is mJy.

 ergs: y-unit for plot_unfold/power=3 (xunit=keV, etc.) or power=1
       (xunit=A, etc.) is ergs/cm^2/sec.

 watts: Similar behavior to the ergs unit, but yielding Watts/cm^2.

 psd_leahy/psd_rms are for use with SITAR timing routines, and plot
 Power Spectra in Leahy or (RMS/Hz)^2 units vs. Hz, using plot_counts.
\seealso{plot_unit, add_plot_unit, set_plot_labels, new_plot_labels}
\done

\function{fermi2MJD}
\synopsis{calculate MJD from Fermi seconds}
\usage{Double_Type = fermi2MJD (Double_Type);}
\description
	Calculates MJD(UTC) for a given Fermi Mission
	Elapsed Time (MET) in seconds.
\example
	variable m = 239557420.0;
	variable fermi_s = fermi2MJD(m);
\seealso{MJD2fermi, MJDref_satellite}
\done

\function{filter_gti}
\usage{Integer_Type ind[] = filter_gti(Double_Type time[], Struct_Type gti);
 or                      filter_gti(Double_Type time_lo[], time_hi[], Struct_Type gti);}
\qualifiers{
    \qualifier{minfracexp}{minimum fractional exposure a time bin has to have,
                otherwise it is considered bad (lightcurves only, default: 1e-4)}
    \qualifier{fracexp}{if set to a reference, returns the fractional exposure
                of each time bin (lightcurves only)}
    \qualifier{exposure}{if set to a reference, returns the livetime
                in each time bin (lightcurves only)}
    \qualifier{indarray}{return an array of index arrays instead (in case
                of events only)}
}
\description
    For a lightcurve given by \code{time_lo} and \code{time_hi}, this function
    returns all indices \code{ind} to the time bins, for which the fractional
    exposure time as defined by the good time intervals defined by \code{gti}
    is at least \code{fracexp}.

    For a list of events measured at times \code{time}, return a list of indices
    to all events that were measured during the given set of good time intervals.
    The good time intervals are defined by a \code{struct {start,stop}} where
    \code{start} and \code{stop} define the start and stop times.

    The code assumes that
    - time and time_hi are in ascending order
    - the gti does not contain any overlapping intervals
    - gti, time, and time_hi have the same unit

    Warning: the function applies a time sorting to the gti if necessary. This
    	will MODIFY THE INPUT since structures are passed as references in S-Lang!
\done

\function{find_correlations}
\synopsis{calculates all correlations between columns of a table}
\usage{Struct_Type info = find_correlatins(Struct_Type s);}
\description
    \code{info.corr[i] = correlation_coefficient(s.<info.x[i]>, s.<info.y[i]>);}
\seealso{correlation_coefficient}
\done

\function{find_function_maximum}
\synopsis{looks for the position of a function's maximum value}
\usage{Double_Type x0 = find_function_maximum(Ref_Type &f, Double_Type x1, Double_Type x2);
\altusage{Double_Type x0 = find_function_maximum(Ref_Type &f, Double_Type x1, Double_Type x2, &f_x0);}
}
\qualifiers{
\qualifier{qualifiers}{structure of qualifiers to be passed to f}
\qualifier{eps}{[=1e-12]}
}
\description
    \code{f} has to be a real function with one argument.
    A binary search is performed to find \code{x0} such that\n
       \code{f(x0)  =  max( f([x1:x2]) )}.\n
    If \code{f} is not convex in \code{[x1:x2]}, the algorithm does not need
    to succeed. Otherwise, the accuracy of \code{x0} is \code{(x2-x1)*eps}.
\seealso{find_function_value}
\done

\function{find_function_value}
\synopsis{computes an inverse function}
\usage{Double_Type x0 = find_function_value(Ref_Type &f, Double_Type val, x1, x2);}
\qualifiers{
\qualifier{qualifiers}{structure of qualifiers to be passed to f}
\qualifier{eps}{[=1e-12]}
\qualifier{quiet}{do not show error message}
}
\description
    \code{f} has to be a real function with one argument.
    A binary search is performed to find \code{x0} such that\n
       \code{f(x0)  =  val} .\n
    If \code{f} is not strictly monotonic in \code{[x1:x2]}, the algorithm does not
    need to succeed. Otherwise, the accuracy of \code{x0} is \code{(x2-x1)*eps}.
\seealso{find_multiargumentfunction_value, find_function_maximum}
\done

\function{find_multiargumentfunction_value}
\synopsis{computes an inverse function}
\usage{Double_Type xi0 = find_multiargumentfunction_value(Ref_Type &f, Double_Type val, x1, x2, ..., xn);}
\qualifiers{
\qualifier{qualifiers}{structure of qualifiers to be passed to f}
\qualifier{eps}{[=1e-12]}
}
\description
    \code{f} has to be a real function with n arguments.
    While \code{xi = [xi_min, xi_max]} is an array, \code{xj} (for \code{j!=i}) are constants.
    A binary search is performed to find \code{xi0} such that
    \code{f(x1, x2, ...,  xn)  =  val} for \code{xi = xi0}.\n
    If \code{f} is not strictly monotonic for \code{xi_min < xi < xi_max}, the algorithm does not
    need to succeed. Otherwise, the accuracy of \code{xi0} is \code{(xi_max-xi_min)/1e12}.
\seealso{find_function_value}
\done

\function{find_peak}
\synopsis{Written for epoch folding purposes, it finds peaks of certain characteristics}
\usage{Struct_Type find_peak(Struct_Type input)}
\description

 The input should be a Struct_Type of the form:
     { p, stat, [ lc ], [ nbins_epfold ] , [ expectation ] , [ sigma ] }
 The individual fields are:
     * p, stat: arrays of doubles, the peak is looked for in stat
     * lc: struct { time, rate }, the original lightcurve
     * nbins_epfold: necessary in order to obtain an error estimate
     * expectation: approximate location of the peak in p
     * sigma: approximate error the expectation value has
 lc and nbins_epfold reflect the original application of this function,
 but it can be used without these arguments to find peaks in other
 curves.
 The output is a Struct_Type of the form:
     { period, error, [ profile ], err_area, err_theory, width_lo,
     width_hi, statvsp, flags, [ bayes ], [ fit ], badness }
 flags is a structure of indicators regarding the quality of the
 output, badness is a coarse estimate how well the process worked.
 A value of 0 is desirable, 1-2 could be acceptable, 3-4 is usually
 an indication of bad input data and for larger values something has
 seriously gone wrong. Fields marked [] will not be output if the
 qualifiers chosen require it.
 In the default configuration, this function works the following way:

     (1) A bayesian block analysis finds possible maxima
         * if the block analysis fails, a smoothing algorithm is used
           to remove noise in the curve, since the block analysis is
           quite susceptible to noise
     (2) The maxima are ranked considering:
         * the width (broad is an advantage, but very broad maxima in
           comparison to dp = p^2 / T resp sigma are discarded)
         * the distance to the expected value, where anything within
           sigma is ranked approximately equal
     (3) Maxima with a quality exceeding the tolerance level are
         taken and the accurate peak is found:
         * a first estimate is a weighted mean of points within the
           maximum block
         * the accurate value is determined by fitting a given
           xy-function to the peak. The proportion of dp used for
           choosing the range for the fit is controlled by the
           qualifier fit_part. If you do not expect considerable
           secondary maxima, this variable can be set as big as 1.
           However, if secondary peaks are expected to occur, a value
           this large can make the fit useless.
     (4) For each possible peak the lightcurve is folded and the
         resulting pulse profile is examined. The peak corresponding to
         the best overall result is taken.

 If the fit_fct qualifier is used, the function has to have the following
 parameters (in this specific order):
     (1) peak
     (2) measure for the height
     (3) measure for the broadness of the peak
     (4) absolute offset
     (5) measure for the asymmetry of the peak
 All qualifiers can also be passed using a structure named
 'find_peak_qualifiers'. If this qualifier is given, its content will
 overwrite all other qualifiers. If using this structure, all
 qualifiers are expected to have values (i.e. only_max = 1). This is
 helpful if you want to make things easier to read in scripts.

\qualifiers{
\qualifier{only_max}{neither block analysis nor fit are performed}
\qualifier{blocks_but_only_max}{block analysis is performed but only the highest bin in the best block taken}
\qualifier{weighting}{weights for properties of a peak [area, sharpness, distance to expectation, profile quality] (default: [1,1,1,1])}
\qualifier{no_fit}{nlock analysis is performed but only the first estimate taken}
\qualifier{fit_fct}{an xyfit function for the fit, passed as a string (default: "sqrsinc")}
\qualifier{fit_part}{measure for the part of the peak used for the fit (default: .3)}
\qualifier{tolerance}{determines how many peaks are passed to step (4), in [0,1) (default: .99)}
\qualifier{smoothing}{how strong the smoothing algorithm works (default: 10.)}
\qualifier{ncp_prior}{argument for block analysis. If a string is given, the default block analysis routine will be used (default: 100)}
\qualifier{fp_rate}{argument for block analysis, see its help}
\qualifier{no_profile}{No pfold will be executed. Corresponds to tolerance -> 1 and less computation}
\qualifier{nbins_pfold}{argument for pfold, see its help}
\qualifier{pfold_not_exact}{changes how pfold is executed. Recommended as long as the exact-bug is not fixed}
\qualifier{exact}{passes the information that the statistic was calculated using the exact qualifier}
\qualifier{flag_info}{detailed information regarding the content of the flags structure is given, NOTHING else is done}
\qualifier{error_info}{detailed information about the calculation of errors is given, NOTHING else is done}
\qualifier{chatty}{boolean value (default: 1)};
}
\seealso{pulseperiod_search, epfold, pulseperiod_epfold, split_and_epfold_lc, bayesian_blocks, pfold}
\done

\function{first_valid_digit}
\synopsis{gives the position of the first valid digit}
\usage{Integer_Type fvd = first_valid_digit( Integer/Double_Type x )};
\done

\function{fitfun_cache}
\usage{see below}
\description
    The so-called caching extension is part of the ISISscripts and not
    an ISIS internal feature.
    
    The caching extension provides an easy way to manage additional data for
    the internal usage of user-defined fit-functions. The cache consists of
    an slang structure for each defined dataset and fit-function using this
    extension. This concept is similar to the metadata associated to each
    dataset (see, e.g., 'get_dataset_metadata').

    Inside of a user-defined fit-function, e.g., 'myfun' the cache for this
    function and the current dataset (Isis_Active_Dataset) can be retrieved
    by 'fitfun_get_cache':
#v+
    define myfun_fit(lo, hi, pars) {
      ...
      variable cache = fitfun_get_cache();
      ...
    }
#v-
    Here, the variable 'cache' would hold the corresponding slang structure.
    Since slang structures are internally passed and returned via references,
    any changes of the fields of 'cache' are permanent.

    In order to define and/or initialize the fields of the structure, one
    has to call 'fitfun_init_cache' and providing the slang structure.
    For the example above this could look like
#v+
    fitfun_init_cache("myfun", &myfun_fit, struct { tempresult, lastpars });
#v-
    The caching extension can be used to, e.g., save temporary results from
    calculations, which can then be retrieved once the model is evaluated
    again. For this purpose it might be useful to also save the parameters
    in a field like 'lastpars' into the cache, which can then be checked
    against changes:
#v+
    define myfun_fit(lo, hi, pars) {
      ...
      variable cache = fitfun_get_cache();
      if (any(cache.lastpars != pars)) {
        ...
      }  
      cache.lastpars = pars;
      ...
    }
#v-
    Please read the documentation of 'fitfun_get_cache' and
    'fitfun_init_cache' fot details and further examples.
\seealso{fitfun_get_cache, fitfun_init_cache, fitfun_cache_enabled}

\done

\function{fitfun_cache_enabled}
\synopsis{returns whether the ISISscripts caching extension
    for fit-functions is enabled (1) or disabled (0).}
\usage{Integer_Type fitfun_cache_enabled();}
\seealso{fitfun_cache, fitfun_enable_cache, fitfun_disable_cache}
\done

\function{fitfun_disable_cache}
\synopsis{disables the ISISscripts caching extension}
\usage{fitfun_disable_cache();}
\seealso{fitfun_cache, fitfun_enable_cache, fitfun_cache_enabled}
\done

\function{fitfun_enable_cache}
\synopsis{enables the ISISscripts caching extension}
\usage{fitfun_enable_cache();}
\seealso{fitfun_cache, fitfun_disable_cache, fitfun_cache_enabled}
\done

\function{fitfun_get_cache}
\synopsis{return the cache for the current dataset and the given fit-function}
\usage{Struct_Type fitfun_get_cache([String_Type fit-function_name]);}
\description
    The cached slang structure for the currently evaluated
    fit-function and the current dataset (Isis_Active_Dataset) is
    returned. In case caching is disabled (by 'fitfun_disable_cache')
    a new structure as defined using 'fitfun_init_cache' is returned.

    The optional and only parameter specifies the name of a
    fit-function, which stack is to be returned. This allows to share
    and access the cache between different fit-functions. If this
    particular fit-function has not been evaluated yet (or is not
    used in the current model) NULL is returned.

    Note: if the number of bins of the data grid of the current
          dataset changes, the cache is initialized again with the
          structure defined by 'fitfun_init_cache'. This might be
          necessary as soon as, e.g., the fit-function is
          'eval_fun'ed on a user-grid.
\example
    % use the cache inside a fit-function in order to save a temporary
    % result, which depends on a subset of the input parameters only
    define myfun_fit(lo, hi, pars) {
      ...
      variable cache = fitfun_get_cache();
      if (any(cache.lastpars[[:1]] != pars[[:1]])) {
        % assuming that 'expensive_calculation' depends on the first
        two fit-parameters  
        cache.tempresult = expensive_calculation(pars[0], pars[1]);
      }  
      cache.lastpars = pars;
      ...
      return cache.tempresult ...;
    }

    % retrieve the cache from another fit-function inside a fit-function
    define mybetter_fit(lo, hi, pars) {
      ...
      variable ocache = fitfun_get_cache("myfun");
      if (ocache != NULL) {
        ocache.tempresult ...
        ...
      }
      ...  
    }

    % investigate the temporary results after a fit from the command line
    isis> Isis_Active_Dataset = 2; % get the cache for this dataset
    isis> cache = fitfun_get_cache("myfun");
    isis> print(cache.tempresult);
\seealso{fitfun_cache, fitfun_init_cache}
\done

\function{fitfun_init_cache}
\synopsis{}
\usage{fitfun_init_cache(String_Type fit-function_name, Ref_Type fit-function_handle, Struct_Type init_struct);}
\description
    Defines the structure, which is used to initialize the cache of the
    the given fit-function. This should be called after a user-defined
    fit-function, which uses caching, has been added to the list of
    available models.

    The first parameter 'fit-function_name' specifies the name of the
    fit-function, 'fit-function_handle' is the reference to the actual
    function calculating the model, and 'init_struct' is the structure
    the cache will be initialized with for each dataset.
\example
    % Define a new fit-function, add it to the list of available
    % models, and initialize its cache with a structure with empty
    % fields. Inside the fit-function 'myfun' the value of lastpars
    % can be checked on NULL or changed parameters in order to
    % trigger an expensive calculation.

    define myfun_fit(lo, hi, par) {
      ...
    }
  
    add_slang_function("myfun", ["par1", "par2", ...]);

    fitfun_init_cache(
      "myfun", &myfun_fit, struct { tempresult, lastpars = NULL }
    );
\seealso{fitfun_cache, fitfun_get_cache}
\done

\function{fits_add_fit}
\synopsis{adds different saved models and observation info togethter in one FITS table}
\usage{Struct_Type str = fits_add_fit(String_Type filename, String_Type save1
                         [,String_Type save2] [, ...);
 or Struct_Type str = fits_add_fit(String_Type filename, Struct_Type save1
                      [, Struct_Type save2] [, ...);}
 or Struct_Type str = fits_add_fit(String_Type filename, Array_Type);
\description

   This function is based on fits_save_fit and fits_load_fit_struct.
   If called with String_Type filenames, these files are loaded with
   fits_load_fit_struct, merged, and saved to the (new) file
   "filename".
   If called with Struct_Type, these structure should have been
   created with fits_save_fit_struct, which will then be merged and
   saved to "filename".
   If called with Array_Type, the entries of the array should either
   be strings loadable with fits_load_fit_struct or structures
   created with fits_save_fit_struct.
   It returns the merged structure.
                                                                                                         
\seealso{fits_load_fit_struct,fits_save_fit_struct,fits_write_fits_struct,fits_save_fit,merge_struct_arrays}
\done

\function{fits_append_binary_table}
\synopsis{appends a binary table to the end of a FITS file}
\usage{fits_append_binary_table(filename, [extname], data[, keys[, hist]]);}
\description
    This function employs functions from ISIS' cfitsio module
    in order to open/create a FITS file,
#c    find the number of HDUs,
#c    move to the end of the file,
    and append a binary table extension.
#c\seealso{fits_open_file, _fits_get_num_hdus, _fits_movabs_hdu, fits_write_binary_table}
\seealso{fits_open_file, fits_write_binary_table}
\done

\function{fits_append_extension}
\synopsis{appends a FITS extension to another FITS file}
\usage{fits_append_extension(String_Type infiles[], String_Type outfile);}
\description
    As the external FTOOL fappend is used for this task,
    \code{infiles} may contain extension numbers according to the FTOOLS conventions.
    The extensions are appended at the end of \code{outfile}.

    This function should usually not be used!
    ISIS' cfitsio module allows to write several extensions
    into a file after opening it with fits_open_file.
\seealso{fappend [FTOOLS], fits_append_tmp_extension}
\done

\function{fits_append_tmp_extension}
\synopsis{appends a temporary FITS extension to another FITS file before deleting it}
\usage{fits_append_tmp_extension(String_Type infiles[], String_Type outfile);}
\description
    As the external FTOOL fappend is used for this task,
    \code{infiles} may contain extension numbers according to the FTOOLS conventions.
    The extensions are appended at the end of \code{outfile}.
    Afterwards, all \code{infiles} are deleted.

    This function should usually not be used!
    ISIS' cfitsio module allows to write several extensions
    into a file after opening it with fits_open_file.
\seealso{fappend [FTOOLS], fits_append_extension}
\done

\function{fits_column_unit_struct}
\synopsis{creates a structure of FITS header keywords containing the units of columns}
\usage{Struct_Type fits_column_unit_struct(Struct_Type data; field1=unit1, field2=unit2)}
\example
    \code{variable data = struct { time, rate };}\n
    \code{fits_write_binary_table("lc.fits", "lightcurve", data, fits_column_unit_struct(data; time="MJD", rate="counts/s/PCU") );}
\seealso{fits_write_binary_table}
\done

\function{fits_conv_to_legal_char}
\synopsis{converts a string into legal characters to be used as a FITS column name}
\usage{String_Type legal_str = fits_conv_to_legal_char(String_Type str);}

\seealso{fits_save_fit}
\done

\function{fits_get_hdu_names}
\synopsis{returns the names of all extensions within a FITS-file}
\usage{String_Type[] fits_get_hdu_names(Fits_File_Type fp);}
\description
  Moves to the first extension of a FITS-file using
  `_fits_movabs_hdu' and then iterates over all extensions
  using `_fits_movrel_hdu' to read the 'EXTNAME' keyword.
  The string array of all extension names is returned.
  
  Note that the file-pointer is located at the last
  extension in the end. Furthermore the first extension
  has the index 1 (at least for `_fits_movabs_hdu'), which
  has to be taken into account if the indices of the name-
  array are used to find specific extensions.
\seealso{fits_open_file, fits_read_key}
\done

\function{fits_lc_exposure}
\synopsis{returns the exposure time of a lightcurve,
    given by a FITS-file, in seconds}
\usage{Double_Type fits_lc_exposure(String_Type file)}
\done

\function{fits_list_fit_pars}
\synopsis{Lists parameter values and confidence intervals for a file saved with fits_save_fit.}
\usage{fits_list_fit_pars(String_Type fit.fits)}
\description
    When loading a previously saved fit with fits_load_fit, \code{list_par}
    does not print the borders of the parameter confidence intervals saved
    in appropriate fields of fit.fits.
    Instead the parameter limits are printed.
    As a work-around use this function to get a similar list as for
    \code{list_par} including the confidence intervals if already determined.
\example
    isis> fits_list_fit_pars("fit.fits");
\seealso{fits_save_fit,fits_load_fit}
\done

\function{fits_load_fit}
\synopsis{defines the data and model of a FITS file written by 'fits_save_fit'}
\usage{Integer_Type = fits_load_fit(String_Type filename[, Integer_Type index = 0]);}
\qualifiers{
  \qualifier{loadfun}{data load function (defualt = &load_data)
            The following format is neccessary: 
            Argument: String_Type Filename
            Return:   Dataset ID}
  \qualifier{nodata}{do not load the data}
  \qualifier{norebin}{do not notice and rebin the data}
  \qualifier{nomodel}{do not define the model}
  \qualifier{noff}{do not set the fit function. Instead
            only values of existing parameters
            of the current model are loaded and
            set. Maybe more useful in combination
            with 'nodata' to just restore actual
            fit parameters without overwritting
            any additional (not saved) components}
  \qualifier{noeval}{do not evaluate the model}
  \qualifier{noerr}{do not load the systematic error fraction}
  \qualifier{toeval}{structure of qualifiers
            passed to 'eval_counts'}
  \qualifier{strct}{reference to a variable to return the
            structure loaded by fits_load_fit_struct}
  \qualifier{ROC}{array of values to set
            Rmf_OGIP_Compliance for each
            spectrum before loading
            (default values = 2)}
}
\description
  This function restores the fit saved previously
  by 'fits_save_fit'. That includes the loaded
  data and the used model, which is evaluated at
  the end.
\seealso{fits_save_fit, fits_load_fit_struct, fits_list_fit_pars}
\done

\function{fits_load_fit_struct}
\synopsis{loads a FITS file written by 'fits_save_fit'}
\usage{Struct_Type str = fits_load_fit_struct(String_Type filename);}
\seealso{fits_save_fit}
\done

\function{fits_modify_header}
\synopsis{modifies the header of a FITS file}
\usage{fits_modify_header(String_Type filename, keyword, value[, comment]);}
\description
    fits_modify_header uses the external FTOOL \code{fmodhead} and is therefore deprecated.
    Use \code{fits_update_key} from ISIS' cfitsio module instead.
\seealso{fits_update_key}
\done

\function{fits_nr_extensions}
\synopsis{counts the extensions of a FITS file}
\usage{Integer_Type fits_nr_extensions(String_Type filename)}
\description
    This function counts the number of extensions
    in addition to the primary extension, i.e.,
    returns \code{fits_num_hdus(filename)-1}.
\seealso{_fits_get_num_hdus, fits_num_hdus}
\done

\function{fits_num_hdus}
\usage{Integer_Type fits_num_hdus(String_Type filename)}
\description
    This function is just a wrapper around the
    \code{_fits_get_num_hdus} function from ISIS' cfitsio module.
\seealso{_fits_get_num_hdus}
\done

\function{fits_plot_rmf}
\synopsis{plots a redistribution matrix from a compressed RMF file}
\usage{fits_plot_rmf(String_Type RMFfile);
\altusage{(interpol_matrix_density, Ebounds, energ) = fits_plot_rmf(RMFfile; getvalues)}
}
\qualifiers{
\qualifier{nx}{number of pixels in x-direction [default=400]}
\qualifier{ny}{number of pixels in x-direction [default=300]}
\qualifier{getvalues}{retrieves the interpolated matrix density}
\qualifier{noplot}{skip plotting, but retrieves the interpolated matrix density}
}
\seealso{fits_read_rmf}
\done

\function{fits_read_key_int_frac}
\synopsis{reads a keyword from a FITS file, which may be split in integer and fractional part}
\usage{fits_read_key_int_frac(String_Type filename, key);}
\seealso{fits_read_key}
\done

\function{fits_read_lc}
\synopsis{reads a light curve file in FITS format}
\usage{Struct_Type lc = fits_read_lc(String_Type filename[]);}
\description
     Reads light curves from \code{filename}, which can be
     a globbing expression, an array of filenames or both.
     If there are several light curves, they will be merged
     into one data structure with ascending times.

     The function assures that the returned structure contains
     the fields \code{time}, \code{rate} and \code{error}.
     The time field is always converted into Modified Julian Date
     according to the MJDREF[{I,F}], TIMEUNIT and TIMEZERO keywords.
\qualifiers{
\qualifier{verbose}{show the filename of the light curves when reading more than one}
\qualifier{cut}{cut all fields but time, rate, error}
\qualifier{rate_per_PCU}{divide count rate for RXTE-PCA light curves
                     by number of PCUs determined from the given filterfile
                     (*.xfl, see 'RXTE_nr_PCUs_from_filterfile').}
\qualifier{time}{[=\code{"time"}]: name of the time field, e.g., \code{"barytime"};
                      if \code{!= "time"}, this field is renamed \code{"time"},
                      overwriting any previously existing \code{time} field}
\qualifier{time_in_s}{add structure fields for time in sec, MJDref
                     and T0 to the output structure}
\qualifier{filename}{add filename structure field}
\qualifier{extension}{add number of extension to be read}
}
\seealso{fits_read_table, fits_read_key, fits_read_key_int_frac, RXTE_nr_PCUs_from_filename}
\done

\function{fits_read_rmf}
\synopsis{retrieves a matrix from a compressed RMF file}
\usage{Struct_Type rmf = fits_read_rmf(String_Type RMFfile);}
\description
     \code{rmf.matrix[j,i]} describes \code{rmf.ebounds.}*\code{[i]} and \code{rmf.energy.}*\code{[j]}.
\qualifiers{
\qualifier{check}{checks rmf-normalization: \code{rmf.matrixsum_ebounds} is the sum over all ebounds, which should be 1.}
\qualifier{spec}{\code{rmf.whitespectrum} is the sum over all ebounds, which should be 1.}
\qualifier{float}{use for larger RMFs to be able to be loaded in isis.}
}
\seealso{fits_plot_rmf}
\done

\function{fits_read_unsigned_img}
\synopsis{reads an image of unsigned integers}
\usage{Integer_Type img[] = fits_read_unsigned_img(String_Type filename);}
\done

\function{fits_save_data_model_flux}
\synopsis{saves ISIS spectral data into a FITS file}
\usage{fits_save_data_model_flux([filename[, ids]]);}
\done

\function{fits_save_fit}
\synopsis{saves the model and info of the observation to a FITS table}
\usage{fits_save_fit(String_Type filename[, Struct/String_Type conf]);}
\description

   This function saves information about the model and the observation
   in a FITS table. This routine combines the calls of
   \code{fits_save_fit_struct} and \code{fits_save_fit_write}.

   Additionally, confidence intervals can be given in form of a
   structure, containig the fields
   \code{name}:      name of the parameter like given in fit_fun(...)
   \code{value}:     best fit value of the parameter (might have changed
                     during error calculation)
   \code{conf_min}:  lower confidence limit
   \code{conf_max}:  upper confidence limit

   Therefore, the conf-Structure would, e.g., look like

   variable conf = struct {
                 name = ["powerlaw(1).index","powerlaw(1).norm"],
                 value = [2,1e-4],
                 conf_min = [1.8,1e-5],
                 conf_max = [2.2,2e-4]
                 };

   Alternatively, the filename of a FITS table created by
   pvm_fit_pars or the structure returned by pvm_fit_pars can be given.


   The values of the model are overwritten, as the error calculation
   should only yield values equal or better than the original ones.

\qualifiers{
\qualifier{info=Struct_Type}{appends the given structure to the table}
\qualifier{hard_limits}{also saves all hard limits of the parameters}
\qualifier{silent}{no warnings are printed to STDOUT}
}
\seealso{fits_load_fit_struct,fits_write_TeX_table,save_par,pvm_fit_pars,fits_save_fit_struct,fits_write_fits_struct,fits_list_fit_pars}
\done

\function{fits_save_fit_struct}
\synopsis{saves the model and info of the observation to a structure}
\usage{Struct_Type str = fits_save_fit_struct([, Struct/String_Type conf]);}
\description

   This function saves information about the model and the observation
   in a structure table. Using fits_save_fit_write it can be written to a
   fits file. See "help fits_save_fit" for more information.

\seealso{fits_save_fit,fits_load_fit_write}
\done

\function{fits_save_fit_write}
\synopsis{writes a FITS table with info on model and observation}
\usage{fits_save_fit_write([, Struct/String_Type conf]);}
\description

   This function saves information about the model and the observation
   in a FITS table. It uses the structure created by
   fits_save_fit_struct. Using self-created structures might lead to
   strange results. See "help fits_savs_fit" for more information.

\seealso{fits_save_fit,fits_load_fit_struct}
\done

\function{fits_wcs_struct}
\synopsis{creates a structure with a WCS that can be written to FITS file}
\usage{Struct_Type fits_wcs_struct(String_Type filename)
\altusage{Struct_Type fits_wcs_struct(Double_Type X[], Y[] [, String_Type xtype, ytype[, xunit, yunit]])}
}
\qualifiers{
\qualifier{arrays}{return a struct { ctype=[ctype1, ctype2], ... } with arrays
              instead of struct { ctype1=ctype1, ctype2=ctype2, ... }.
              This form is, e.g., required by ds9_put_wcs_struct.}
}
\description
    The (linear) World Coordinate System (WCS) can be read from a FITS file,
    or can be defined from an array of \code{X} and \code{Y} values.
\done

\function{fits_write_arf}
\synopsis{Write a FITS ancilliary response matrix file}
\usage{fits_write_arf(arfname,arf;qualifiers);}
 \qualifiers{
 \qualifier{telescope}{telescope for the ARF}
 \qualifier{instrument}{instrument for the ARF}
 \qualifier{filter}{filter of the instrument}
 \qualifier{detnam}{name of the detector}
 \qualifier{exposure}{exposure time associated with the ARF}
 \qualifier{chantype}{see FITS ARF specification (default: PI)}
 \qualifier{origin}{who write this file (default: ECAP)}
}
\description
 Write a FITS compliant ancilliary response matrix
    arfname: name of the file to be written
    arf: structure containing the following fields:
       bin_lo, bin_hi: energy bounds (keV)
       area: array of length(ebounds.bin_lo) containing the effective
              area in cm^2
    note: the energies given MUST be the same as the input energies
          of the corresponding response matrix!
\seealso{fits_write_rmf}
\done

\function{fits_write_arf_diff}
\synopsis{writes the difference of two ARFs in a corresponding FITS file}
\usage{fits_write_arf_diff(String_Type arffile0, arffile1, arffile2);}
\description
    arf0  =  arf1 - arf2\n
    The important header keywords are copied from arf1,
    assuming that they are the are the same in arf2.
\done

\function{fits_write_binary_table_extensions}
\synopsis{writes a binary FITS table with several extensions}
\usage{fits_write_binary_table_extensions(filename, data1, data2, ...);}
\description
    \code{data1}, \code{data2}, ... are either just the data structures
    or a list of the arguments 2, 3[, 4[, 5]] of \code{fits_write_binary_table}:
    \code{{ extname, data[, keys[, hist]] }}

    This function should usually not be used!
    ISIS' cfitsio module allows to write several extensions
    into a file after opening it with fits_open_file.
\examples
    \code{fits_write_binary_table_extensions("data.fits", struct { a1, b1 }, struct { a2, b2 });}\n
    \code{fits_write_binary_table_extensions("data.fits",}
    \code{                                   { "first", struct { a1, b1 } },}
    \code{                                   { "second", struct { a2, b2 } });}\n
    \code{fits_write_binary_table_extensions("data.fits",}
    \code{                                   { "first", struct { a1, b1 }, struct { key11="value11"; key12="value12" } });}\n
\done

\function{fits_write_gti}
\synopsis{creates a FITS file with good time intervals}
\usage{fits_write_gti(String_Type filename, Struct_Type gti, Double_Type MJDref);
\altusage{fits_write_gti(String_Type filename, Double_Type start[], stop[], MJDref);}
}
\qualifiers{
\qualifier{creator}{[\code{="isisscripts:fits_write_gti"}] (XMM SAS needs an arbitrary value)}
\qualifier{date}{[\code{="1998-JAN-01"}] (XMM SAS needs an arbitrary value)}
\qualifier{combineGTIs}{[\code{=1}]: combine intervals that adjoin each other}
\qualifier{verbose}{[\code{=1}]: tell when intervals are combined}
}
\description
     Good time intervals are organized as \code{gti} structures
     containing arrays \code{START} and \code{STOP} of the corresponding times,
     usually measured in s since the reference date \code{MJDref}.
     (If \code{MJDref} is a string, the date is read from this file.)
     Intervals that adjoin each other are combined.

     The header keywords \code{creator} and \code{date} (according to the qualifiers)
     are written to the primary header of the created FITS file.
\seealso{fits_write_binary_table}
\done

\function{fits_write_image}
\synopsis{writes an image to a FITS file}
\usage{fits_write_image(FITSfile[, extname], image);
\altusage{fits_write_image(FITSfile, extname, image, [xvalues, yvalues[, xlabel, ylabel]][, comments]);}
}
\qualifiers{
\qualifier{WCS}{[=\code{""}]: world coordinate system to use, e.g. \code{"P"}}
}
\description
    It is assumed that \code{x}/\code{yvalues} (if provided) are linear arrays,
    such that \code{CRVAL = values[0]} and \code{CDELT = (values[-1]-values[0])/(length(values)-1)}.
    \code{x}/\code{ylabel} can be a "label [unit]" string.
\seealso{fits_write_image_hdu}
\done

\function{fits_write_pha_file}
\synopsis{writes a spectrum to an OGIP PHA file inserting the required header keywords}
\usage{fits_write_pha_file(String_Type filename, Integer_Type data)}
\altusage{fits_write_pha_file(String_Type filename, Array_Type data[, Array_Type stat_err])
}

\description
    The 'filename' argument is the name of the output FITS file.

    The 'data' argument can be either the index of a data set, in 
    which case the isis data set is written to a file, 
    an array containing the spectrum. 
      If the array is of IntegerType, the spectrum contains total counts,
      If the array is of FloatType/DoubleType, the spectrum is given in counts/s.
    (DoubleType arrays will be type-casted to FloatType.) 

    If 'data' is an array, then the statistical uncertainty is assumed
    to be Poisson, unless the errors are given in the 3rd (optional) 
    argument of the function, 'stat_err'. Note that this argument is
    mandatory for the case of count rate spectra.
    If 'data' is an index to a data set, then the errors are taken 
    directly from the data set.

\qualifiers{
\qualifier{TELESCOP}{the "telescope" (mission/satellite name) ["unknown"]}
\qualifier{INSTRUME}{the instrument/detector ["unknown"]}
\qualifier{FILTER}{the instrument filter in use (if any) ["none"]}
\qualifier{EXPOSURE}{the integration time (in seconds) for the PHA data
                 (assumed to be corrected for deadtime, data drop-outs etc. ) [1.0]}
\qualifier{AREASCAL}{nominal effective area [1.0]}
\qualifier{BACKFILE}{the name of the corresponding background file (if any) ["none"]}
\qualifier{BACKSCAL}{background scale factor [1.0]}
\qualifier{CORRFILE}{the name of the corresponding correction file (if any) ["none"]}
\qualifier{CORRSCAL}{the correction scaling factor [1.0]}
\qualifier{RESPFILE}{the name of the corresponding (default) redistribution matrix file ["none"]}
\qualifier{ANCRFILE}{the name of the corresponding (default) ancillary response file ["none"]}
\qualifier{HDUCLASS}{should contain the string "OGIP" to indicate that this is an OGIP style file ["OGIP"]}
\qualifier{HDUCLAS1}{should contain the string "SPECTRUM" to indicate this is a spectrum ["SPECTRUM"]}
\qualifier{HDUCLAS2}{indicating the type of data stored: "TOTAL", "NET", "BKG" ["TOTAL"]}
\qualifier{HDUVERS1}{the version number of the format ["1.2.1"]}
\qualifier{CHANTYPE}{whether the channels used in the file have been corrected in anyway,
                 values: "PHA" or "PI" (see also CAL/GEN/92-002, George et al. 1992, Section 7)
                 ["PHA"]}
\qualifier{start_channel}{start value of 'channel' column [1]}
}

   \seealso{Definition of PHA FITS format: OGIP/92-007 and OGIP/92-007a}
\done

\function{fits_write_rmf}
\synopsis{Write a FITS response matrix file}
\usage{fits_write_rmf(rmfname,rmf;qualifiers);}
 \qualifiers{
 \qualifier{telescope}{telescope for the response}
 \qualifier{instrument}{instrument for the response}
 \qualifier{filter}{filter of the instrument}
 \qualifier{detnam}{name of the detector}
 \qualifier{effarea}{effective area of the detector (default: 1cm**2)}
 \qualifier{lo_thresh}{lower threshold of the matrix, i.e., values below
       this number have been set to zero. Default: 0.}
 \qualifier{chantype}{see FITS RMF specification (default: PI)}
 \qualifier{channel}{array of channel numbers for the ebounds extension 
     (default: channels are assumed to start at 1)}
 \qualifier{origin}{who write this file (default: ECAP)}
 \qualifier{constantwidth}{write data with width constantwidth around 
       maximum of the matrix.}
}
\description
 Write a FITS compliant response matrix
    rmfname: name of the file to be written
    rmf: structure containing the following fields:
       ebounds (bin_lo, bin_hi): channel energies of the response matrix
       energy (bin_lo, bin_hi): input energies of the response matrix
 The response matrix can be given in two ways. For most decent resolution
 instruments, define the matrix as
       matrix[energy,channel]: with dimensions matrix[length(energy),length(ebounds)]
 If this does not work (this is the case once the total array size exceeds
 the limits imposed by s-lang), then define matrix as an Array of Arrays:
       matrix=Array_Type[length(energy)];
 and assign an Array_Type[length(ebounds)] to each element of matrix
 
 Note: High resolution matrices can be VERY large. This code does not yet 
 write variable length arrays due to time reasons. As a work around it
 is possible just to write information around the diagonal using the
 constantwidth qualifier.
\seealso{fits_write_arf,fits_read_rmf}
\done

\function{fits_write_sixte_vignetting}
\synopsis{write a vignetting file, which can be used in a Sixte instrument configuration XML file.}
\usage{fits_write_sixte_vignetting((string) filename, energies, offaxangles [, phi = 0]);}
\description
  energies given in keV, offaxis angles given in arcmin
\done

\function{fits_write_tex_table}
\synopsis{creates a TeX table ready for input in your document.tex
          file.}

\usage{fits_write_tex_table(String_Type inputFile);}

\qualifiers{
\qualifier{pars}{model names given as an array}
\qualifier{exclude}{parameters to exclude. One can give whole
                  parameter name as is written in the window output, or just a main
                  part of it (see examples below).}
\qualifier{extraInfo}{extra information such as target name,
                  exposure, etc., the input name is given in
                  the fits header. This qualifier can lead to
                  nasty look of your table. }
\qualifier{texMulti}{tries to fix long extraInfo output in your table.}                  
\qualifier{everyPar}{parameter name as given can be used as a
                  qualifier itself for changing:
                  name, digits, factor and sciMode (see
                  TeX_value_pm_error on how to use them).
                  Example:
                  powerlaw_1_PhoIndex_value={"name","$\\Gamma$","digits",3}
                  }
\qualifier{sci}{change scientific output in one step, for
                  an easier look of the value and its errors in the table.
                  sci=0   will give you maximally correct output.
                  sci=1   will also give you nice output, but may not
                  care about last significant digit.}
\qualifier{colNames}{give new names for your output columns. Applicable
                  only when all names change.}  
\qualifier{fullStat}{prints also chi^2_red values.}
\qualifier{silent}{No output generated.}
\qualifier{flip}{flips the table.}
\qualifier{pdf}{produces pdfout.pdf file for a quick look at
                  your table.}
\qualifier{output}{TeX output name, default is "default.tex"}
}                         
\description
    - \code{inFile}   input fits file produced by fits_save_fit.
                  In the case of several files(in other words
                  fits for the same model) produced by fits_save_fit,
                  first add the files with fits_add_fit and use it in 
                  fits_write_tex_table.

                  Parameter names, digits, factor output have
                  all default values, but that can be changed
                  with the last mentioned qualifer in the list
                  above. 

      NOTE: the function is still under development. Hence, it may
      not be applicable to all available Xspec or local models.
      If issues emerge, contact refiz.duro@sternwarte.uni-erlangen.de.   

                           
\example

   fits_write_tex_table("input.fits";
                        pars=["cutoffpl","reflionx","diskbb","constant"]
                       ,exclude=["diskbb_1_tin_value","norm"]
                       ,constant_1_factor_value=["name","$c_\\mathrm{PCA}$"]
                       ,output="my_out.tex"
                       .extraInfo=["target","instrument"]
                       ,target={"name","Source"}
                       ,flip
                       ,pdf);

      will:
         1.  use input fits_save_fit file "input.fits"
         2.  look for components "cutoffpl" ,"reflionx", "diskbb" and
             "constant" in your main model
         3.  exclude parameter "diskbb_1_tin_value" and all parameters
             with "norm" in the parameter name
         4.  change name of now qualifier (actually a parameter)
             "constant_1_factor_value" to $c_\\mathrm{PCA}$
         5.  write TeX output to my_out.tex
         6.  include target and instrument information
         7.  change name of "target" to "Source"
         8.  flip your table
         9.  produce a pdfout.pdf

\seealso{fits_save_fit, fits_add_fit,  TeX_value_pm_error}%
\done

\function{fit_brute_force}
\synopsis{Performs a fit by stepping the given parameters and does a
    usual fit for the remaining ones}

\usage{Struct_Type fit_brute_force(Integer_Type[] par [, Double_Type[] stepsize])
 or Struct_Type fit_brute_force(String_Type[] par [, Double_Type[] stepsize])}
\qualifiers{
\qualifier{nofit}{instead of fitting the remaining parameters the
             model is just evaluated}
\qualifier{nomap}{the chi-square map will not be created}
\qualifier{chatty}{boolean value to show the remaining time
              (default=1)}
}
\description
    For the given parameters the best fit of the actual model
    is found by going through the complete parameter range.
    The parameters can be passed as an array containing their
    indices or names. The stepsize for each parameter can
    either be given by the second `stepsize' parameter or the
    intrinsic stepsizes are used, which can be set using a
    qualifier of the `set_par' function. The latter can also
    be used to determine the minimum and maximum value of each
    parameter. While the given parameters are stepped the other
    parameters of the model are fitted by the actual fitting
    method, that means the function `fit_counts' is called.
    If the qualifier `nofit' is given `eval_counts' is used
    instead, evaluating the model without performing a chi-
    square minimization.
    This function is very similar to the `get_confmap' function.
    But this function steps n given parameters in a very small
    grid. In contrast it is possible to perform a fit for all
    parameters of the model. However BE CAREFUL as the runtime
    rises exponentially with the number of parameters!! For
    this reason the function is chatty by default to show the
    estimated remaining time during the fit.
    The returned structure contains an array `bestpar' of the
    best values found for each given parameter, the corres-
    ponding reduced chi-square value `bestchi' and a structure
    `chimap', which holds the chi-square values for each tested
    parameter combination up to two given parameters. Because
    this map may be very big its creation can be suppressed by
    the `nomap' qualifier. In the structure the one or two
    dimensional array (depending on the number of parameters)
    `chisqr' contains the reduced chi-square values. In case of
    two dimensions the first one specifies the second parameter
    to directly pass it to image function like `plot_image'. The
    corresponding parameter values are stored in the x-field for
    the first parameters and y-field for the second one.
\seealso{get_confmap, fit_counts, eval_counts, set_par, plot_image}
\done

\function{fit_gauss_to_img_noise}
\synopsis{fits a gaussian profile to the distribution of pixel values}
\usage{mu,sigma = fit_gauss_to_img_noise(\code{img})}
\qualifiers{
\qualifier{grid_scale [="log"]}{change between fitting on "lin" or "log" grid}
\qualifier{cut_nsig [=NULL]}{set to \code{N} in order to ignore values above
                           \code{N} sigma after the first iteration}
\qualifier{keep_data}{do not delete loaded data after fitting}
}
\description
    This function fits a Gaussian profile to the distribution of
    pixel values in an image (arrays of different dimensions can be
    given) and returns the center \code{mu} and the width \code{sigma} of the
    best fit profile.
    If the majority of pixels in the image include only noise values,
    the obtained \code{mu} and \code{sigma} values characterize the mean value
    (base level) of the noise and its amplitude.

    NOTE: If the data is kept and further fitting is performed, the following
    relation has to be considered \code{mu=get_par("gauss(1).center")+min(img)-1e-5}
    (necessary to provide a proper grid for fitting).
\example
    img = grand(500*500);    % image with random numbers around 0 with sigma=1
    reshape(img,[500,500]);
    (mu,sigma) = fit_gauss_to_img_noise (img);
\seealso{plot_vlbi_map}
\done

\function{fit_interactive}
\synopsis{Interactivly change parameters and evaluate the model}

\usage{fit_interactive(Ref_Type plotfunction[, Any_Type arg0, arg1, ...]}
\description
    The Ref_Type 'plotfunction' is a reference to
    the plot function, which has to be used (see
    example below). Any additional arguments are
    passed to this function as well as given
    qualifiers.
    The user may changes the parameters interactivly
    by the following keys:
      LEFT/RIGHT: increase/decrease value of active
                  parameter by one stepsize
      UP/DOWN   : change active parameter
      PGUP/PGDN : increase/decrease stepsize by one
                  order
      t         : freeze/thaw active parameter
      r         : change range of active parameter
      v         : set value of active parameter
      z         : set the internal step size used by
                  fit_counts
      m         : modify fit function
      f         : perform a fit by running fit_counts
      s         : show/hide frozen parameter
      p/P       : load/save parameters in 'parfile'
      q         : quit interactive mode
    Every time the value of a parameter is changed,
    the model is evaluated automatically and the
    given plot function is called.
    Actual parameter values and the reduced chi-
    square are printed out to the plot window.
    The active parameter is shown in red, thawed
    ones in black and frozen ones in gray. If the
    parameter turns blue, one of the borders of the
    range is reached.
\qualifiers{
    \qualifier{parfile}{filename, which is used to save or load
              the fit parameters using 'save_par' or
              'load_par', respectively. The user will
              be asked for if 'parfile' is not set.}
    \qualifier{plotscript}{filename, used to load in a user-defined 
                 plotting script. If not set, function
                 reverts to Ref_Type 'plotfunction'.}
}
\example
    variable id = load_data("example.pha");
    fit_fun("cutoffpl");
    fit_interactive(&plot_data, id; res=1);

    The appearing plot window then is internaly
    called by
    plot_data(id; res=1);
\seealso{eval_counts, open_plot, keyinput}
\done

\function{fit_line}
\synopsis{activates a line in the lines model and fits its parameters}
\usage{fit_line([id,] line);}
\seealso{lines}
\done

\function{fit_pars}
\synopsis{computes single-parameter confidence limits for several parameters}
\usage{Struct_Type results = fit_pars([Integer_Type pars[]]);}
\qualifiers{
\qualifier{strict}{[=1]: restarts the calculation if a new best fit was found}
\qualifier{saveoutput}{[=1]}
\qualifier{basefilename}{[=<date_time>]}
\qualifier{level}{[=1]: specifies the confidence level. Values of 0, 1, or 2
                 indicate 68%, 90%, or 99% confidence levels respectively.
                 By default, 90% confidence limits are computed.}
\qualifier{chi2diff}{if given, fit_pars invokes fconf instead of
                    conf with the value given to caluculate confidence in chi2
                    range. Defaults to 1.0.}
\qualifier{tolerance}{convergence criterion for the calculation of the confidence
      limits (see help for the conf command). Default: 1e-3}
}
\qualifier{quiet}{If set, don't print any information on stdout}
\description
    The return value \code{results = struct { index, name, value, min, max, conf_min, conf_max, buf_below, buf_above, tex }}
    is a table with the following information for each parameter:\n
    \code{min} and \code{max} are the minimum/maximum values allowed.
    \code{conf_min} and \code{conf_max} are the confidence limits.
    \code{buf_below} (\code{buf_above}) is the fraction of the allowed range \code{[min:max]}
    which separates the lower (upper) confidence limit from \code{min} (\code{max}).
    If one of these buffers is 0, your confidence interval has bounced.
\seealso{pvm_fit_pars, conf, fconf}
\done

\function{fit_pars_read_stdoutlogfile}
\synopsis{reads chi2-improvements from a stdout-logfile created by (pvm_)fit_pars}
\usage{(t, chi2) = fit_pars_read_stdoutlogfile(String_Type filename);}
\description
    \code{t} is the number of seconds since the start of (\code{pvm_})\code{fit_pars}.
\seealso{fit_pars, pvm_fit_pars}
\done

\function{fit_steppar}
\synopsis{tries to obtain better fits for a stepped parameter}
\usage{Struct_Type result = fit_steppar(String_Type parname, parfiles[]);}
\qualifiers{
\qualifier{method}{"optimize", "interpol_guess", or "eval_all"}
\qualifier{extrapol}{allows for extrapolation as well}
\qualifier{fit [=1]}{}
\qualifier{min_improvement}{}
\qualifier{plot}{}
\qualifier{verbose [=1]}{}
}
\description
\done

\function{flux2lum}

\synopsis{Calculates the source luminosity}
\usage{Double_Type lum = flux2lum (Double_Type flux, Double_Type z);}
\qualifiers{
\qualifier{silent}{         If set, the program will not display adopted
                       cosmological parameters at the terminal.}
\qualifier{h0}{[=70] Hubble parameter in km/s/Mpc}
\qualifier{omega_m}{[=0.3] Matter density, normalized to the closure density,
                       default is 0.3. Must be non-negative.}
\qualifier{omega_lambda}{[=0.7] Cosmological constant, normalized to the
                       critical density.}
\qualifier{omega_k}{[=0] curvature constant, normalized to the critical density.
                       Default is 0, indicating a flat universe.}
\qualifier{q0}{[=-0.55] Deceleration parameter, numeric scalar = -R*(R'')/(R')^2}
}
\description
    This function calculates the luminosity of a source
    given a \code{flux} in erg/s/cm^2 as well as a redshift \code{z} using
    the cosmological parameters specified by the qualifiers
    (which are passed to the function \code{cosmo_param}). The
    distance is calculated with the function \code{lumdist}.
    \code{Flux} and \code{z} can be scalars or vectors. Use
    energyflux to calculate the flux.
\example
    variable flux=3e-13;
    variable z=0.0705;
    variable l = flux2lum(flux,z);
\seealso{energyflux, cosmo_param, lumdist}
\done

\function{fm_unred}
\synopsis{Deredden a flux vector using the Fitzpatrick (1999) parameterization}
\usage{Double_Type = fm_unred(Double_Type[] wave, Double_Type[] flux, Double_Type ebv);}
\qualifiers{
    \qualifier{N_H}{Hydrogen absorption column}
    \qualifier{R_V}{Scalar specifying the ratio of total to selective extinction
               R(V) = A(V) / E(B - V). If not specified, then R = 3.1
               Extreme values of R(V) range from 2.3 to 5.3}
    \qualifier{LMC2}{If set, then the fit parameters are set to the values determined
               for the LMC2 field (including 30 Dor) by Misselt et al.
               Note that neither 'AVGLMC' or 'LMC2' will alter the default
               value of 'R_V' which is poorly known for the LMC.}
    \qualifier{AVGLMC}{If set, then the default fit parameters c1,c2,c3,c4,gamma,x0
               are set to the average values determined for reddening in the
               general Large Magellanic Cloud (LMC) field by Misselt et al.
               (1999, ApJ, 515, 128)}
    \qualifier{extcurve}{If set to a variable as Ref_Type, the E(wave-V)/E(B-V)
               extinction curve is returned, interpolated onto the input
               wavelength vector}
    \qualifier{gamma}{Width of 2200 A bump in microns (default = 0.99)}
    \qualifier{x0}{Centroid of 2200 A bump in microns (default = 4.596)}
    \qualifier{c1}{Intercept of the linear UV extinction component
               (default = 2.030 - 3.007 * c2)}
    \qualifier{c2}{Slope of the linear UV extinction component
               (default = -0.824 + 4.717 / R_V)}
    \qualifier{c3}{Strength of the 2200 A bump (default = 3.23)}
    \qualifier{c4}{FUV curvature (default = 0.41)}
    \qualifier{wilm}{If N_H value has been determined using wilm abundances and Verner 
               cross-sections use this updated correlation (Nowak et al., 2012)}
    \qualifier{ngc3227}{Use the reddening curve for NGC3227 after Crenshaw, D.~M.,
               Kraemer, S.~B., Bruhweiler, F.~C., & Ruiz, J.~R. 2001, ApJ, 555, 633
               provided locally in /home/beuchert/work/ngc3227/scripts/reddening/}

}
\description
     The unreddened flux vector of the input 'flux' is calculated on the
     wavelength vector 'wave' using the Fitzpatrick (1999) parameterization.
     The scalar 'ebv' is the color excess E(B-V). If a negative EBV is supplied,
     then fluxes will be reddened rather than dereddened. If the scalar is not
     known the hydrogen absorption column in the directory of the object must be
     given as a qualifier and E(B-V) is calculated by
       E(B-V) = N_H/(1.79e21*R_V).
     
     The R-dependent Galactic extinction curve is that of Fitzpatrick & Massa 
     (Fitzpatrick, 1999, PASP, 111, 63; astro-ph/9809387 ).    
     Parameterization is valid from the IR to the far-UV (3.5 microns to 0.1 
     microns). UV extinction curve is extrapolated down to 912 Angstroms.
     This Function is adopted from the IDL-Function fm_unred.
         
     The five input qualifiers 'gamma', 'x0', 'c1', 'c2', 'c3' and 'c4' allow the
     user to customize the adopted extinction curve. For example, see Clayton et al.
     (2003, ApJ, 588, 871) for examples of these parameters in different interstellar
     environments.           

 EXAMPLE
     Determine how a flat spectrum (in wavelength) between 1200 A and 6500 A
     is altered by a reddening of E(B-V) = 0.5. Assume an "average"
     reddening for the diffuse interstellar medium (R(V) = 3.1)

     isis> wave = [1200:6500:#500];                               %Create a wavelength vector from 1200 to 6500 Angstrom with 500 steps between
     isis> flux = 1.*ones(500);                                   %Create a "flat" flux vector
     isis> ebv = 0.5;                                             %Value for E(B-V)
     isis> variable var;                                          %Referenz for extcurve 
     isis> funred = fm_unred(wave, flux, ebv; extcurve=&var);     %Redden flux vector
     isis> plot(wave,var);                                        %Plots the extinctioncurve versus the wavelength

\seealso{e_bv;}
\done

\function{foucalc}
\synopsis{calculates power-, cross power-, coherence- and timelag-spectra for timing analysis}
\usage{Struct_Type foucalc(Struct_Type lc, Integer_Type dimseg)}
\qualifiers{
\qualifier{verbose}{}
\qualifier{normtype}{normalization type of PSD data, can be \code{"Miyamoto"} [default], \code{"Leahy"}, or \code{"Schlittgen"}}
\qualifier{normindiv}{}
\qualifier{avgbkg}{array of average background rates for each energy band}
\qualifier{numinst}{[\code{=1}] number of activated PCUs on XTE, required for noise correction}
\qualifier{deadtime}{[\code{=1e-5}] detector deadtime in seconds}
\qualifier{nonparalyzable}{Set this qualifier if the deadtime is non-paralyzable}
\qualifier{fmin}{minimum frequency used for RMS calculation}
\qualifier{fmax}{maximum frequency used for RMS calculation}
\qualifier{RMS}{DEPRECATED, use rms qualifier instead.}
\qualifier{rms}{reference to a variable to store the signal, noise RMS and error
           of each light curve in the [fmin, fmax] band. Can only be used with
           normtype="Miyamoto". The error is calculated using Vaughan
           et al., MNRAS 345, 1271, 2003 Eq. 11.}
\qualifier{avgrate}{reference to a variable to store the average rate of each light curve}
\qualifier{compact}{compact the output structure, only keep the most important quantities}
\qualifier{noCPD}{do not calculate cross power densities and derived quantities}
}
\description
    \code{lc} contains (properly segmented) light curves in several energy bands.
    The best performance is achieved with a common structure of arrays,
    \code{lc = struct { time=[t_1, t_2, ...], rate1=[r1_1, r1_2, ...], rate2=... };}.
    [However, one can also use an array of structures (with an enormous overhead),
    \code{lc = [ struct { time=t_1, rate1=r1_1, rate2=...}, struct { time=t_2, rate1=r1_2, rate2=... }, ... ];}.]
    Also specifying "rate" instead of "rate1" is possible if no CPD
    should be computed.

    \code{dimseg} is the segment size used for the FFTs, which should therefore be a power of 2.

    The returned structure contains the following fields:\n
    - \code{freq}:
        the frequency grid\n
    - \code{numavgall}:
        The bin \code{i} in all power spectra has been averaged over \code{numavgall[i]} original bins.
        Here, \code{numavgall[i] = numseg}, as no frequency rebinning has been performed.\n
    - Power spectra for each energy band:\n
      * \code{rawpsd}, \code{errpsd = rawpsd/sqrt(numseg)}, \code{noipsd}:
          the raw power spectrum (from \code{makepsd}),
          its error from the average over segments,
          and the noise level (from \code{psdcorr_zhang})\n
      * \code{sigpsd = rawpsd - noipsd}:
          the signal power spectrum\n
      * \code{rawnormpsd}, \code{errnormpsd}, \code{noinormpsd}, \code{effnoinormpsd = noinormpsd / sqrt(numseg)}:
          the normalized power spectrum,
          its error, the noise level,
          and the effective noise level in the normalized power spectrum\n
      * \code{signormpsd = rawnormpsd - noinormpsd}:
          the signal in the normalized power spectrum\n
    - Cross power spectra, coherence and time lag functions for each pair of energy bands:\n
      * \code{realcpd}, \code{imagcpd}:
          real and imaginary part of the cross power density\n
      * \code{errrealcpd}, \code{errimagcpd}:
          standard error on the mean of the averaged cross power density\n
      * \code{noicpd = ( sigpsd_lo * noipsd_hi + sigpsd_hi * noipsd_lo + noipsd_lo * noipsd_hi ) / numseg}\n
      * \code{rawcof}, \code{cof}, \code{errcof}:
          non-noise-corrected (raw) and noise-corrected coherence function and its one-sigma uncertainty\n
      * \code{lag}, \code{errlag}:
          time lag and its one-sigma uncertainty\n
\seealso{makepsd, cross_power_density, colacal, psdcorr_zhang}
\done

\function{foufreq}
\synopsis{Timing Tools: Calculation of the Fourier Frequency Array}
\usage{Double_Type freq[] = foufreq(Double_Type time[]);}
\description
     Calculates the Fourier frequency array corresponding
     to a given equally-binned time array.
\done

\function{fplot}
\synopsis{plots data and residuals from the fits-files created with 'save_plot'}
\usage{fplot(filename, [color, [dataset]]);}
\qualifiers{
\qualifier{flux}{plot flux [photons/s/cm^2/keV] and not [counts/bin]}
\qualifier{overplot}{Overplot previous plot}
\qualifier{type}{defines what should be plotted. It can have the values 'model', 'data', 'res' (residuals),
      'ratio', 'diff' (difference between model an data. By default model and data is plotted.)}
\qualifier{nolabel}{The routine uses the labels given in 'plot_options' and does not create its own. }
\qualifier{space}{If at the qualifer 'type' data, model or diff is chosen, one can specifiy if it is
 plotted in counts/bin (by default), counts/keV/s ('density') or photons/keV/s/cm^2 ('flux').} 
\qualifier{auto}{Set the ranges automatically.}
\qualifier{xauto}{Set the x-range automatically.}
\qualifier{yauto}{Set the y-range automatically.}
\qualifier{unit}{Specifies the unit of the x-axis. By default the unit from the fitstable is taken. 
       Possible values are 'keV' or Angstrom 'A'.}
}
\description
   This function is designed to plot the data which was saved with
   'save_plot' in a fits-file. Using the qualifiers you can choose
   how the data should be displayed. By default the model and the
   data is plotted in counts/bin.
   
   The data, model and diff qualifier can be combined with the
   qualifier 'flux' in order to plot in photons/s/cm^2/keV instead of
   counts/bin.
   
   To define the colors of the plot, you have to provide an array of
   colors. The length of the array is equal to the number of loaded
   spectra, as each color is associated with the spectra in the same order
   given in the fits-file.
   
   If you provide the array 'dataset', only these spectra are
   plotted. The numbers are assinged according to the order of the
   single spectra in the extension of the fits-file. Using for
   example 'fv <filename>', you can easily look up these numbers, as the
   instrument which created the desired spectrum is also named in the
   header of each extension.
\examples
   % save one dataset in a fits-file
   save_plot("my_data",1);
   % plot data and model in flux and the residuals in an additional panel
   % -the data should be in red, color(2) and the data in black, color(1)
   % -the ranges are set automatically
   multiplot([3,1]);
   fplot("my_data",2;type="data",space="flux",auto);
   ofplot("my_data",1;type="model",space="flux",auto);
   ofplot("my_data",2;type="res",auto);
\seealso{save_plot}
\done

\function{fread_struct}
\synopsis{Read binary data from a file into a pre-defined structure}
\usage{fread_struct(Struct_Type s, File_Type fp);}
\qualifiers{
    \qualifier{char_to_string}{convert fields of an array of chars (Char_Type[])
                     with length greater one into strings}
    \qualifier{chatty}{be verbose}
}
\description
    This function uses `fread' to reads binary data from a file into
    the fields of a structure. All fields have to have a defined
    data type! That is at least each field value has to be set to a
    certain `DataType_Type'. In case field values are arrays, the
    corresponding amount of objects of the array's data type are read
    from the file. The fields have to be defined in the same order as
    their corresponding objects should be read from the file. Note
    that the function does not return anything, but the fields of the
    given structure are updated instead! Finally, make sure to use
    the correct number of bits to be read for each field. In doubt
    always use, e.g., Int32_Type instead of Integer_Type as the
    latter might depend on how S-lang was compiled.
\example
    % define the structure to be read
    variable s = struct {
      count = Int16_Type, % first, read one 16-bit integer
      list = Float32_Type[10], % secondly, read 10x 32-bit floats
      msg = Char_Type[64] % finally, read 64x characters
    };
    % read the file
    variable fp = fopen("mybinaryformat.file", "r");  
    fread_struct(s, fp; char_to_string);
    ()=fclose(fp);
    % print the message, which will be a string due to the
    % char_to_string-qualifier
    message(s.msg);  
\seealso{fread}
\done

\function{freeParameters}
\synopsis{find all free parameters of the current fit-function}
\usage{Integer_Type[] freeParameters()}
\description
    Free parameters are not frozen, tied to another one,
    or derived as functions of other parameters.
\seealso{thawedParameters}
\done

\function{frequency2velocity}
\synopsis{Calculate the velocity to a given frequency.}
\usage{frequency2velocity(freq,restfreq);}
\qualifiers{
    \qualifier{veldef}{The velocity definition which must be one of
                   OPTICAL, RADIO, or TRUE.  Defaults to RADIO.}
}
\description
    Convert frequency to velocity (m/s) using the given rest
    frequency and velocity definition. The units (Hz, MHz, GHz, etc)
    of the frequencies to convert must match that of the rest frequency argument.
    
    Adopted from the gbtidl script freqtovelo.pro
\done

\function{FriendCastor1982_dMdot_dOmega_CygX1}
\synopsis{calculates the mass loss rate per solid angle for Cyg X-1}
\usage{Double_Type FriendCastor1982_dMdot_dOmega_CygX1(Double_Type theta)}
\description
    \code{theta} is the angle from the binary axis in degrees.
    The return value is in units of 1e-6 solar masses / year / sr.
    (The total mass loss rate \code{2 pi int dMdot/dOmega(theta) sin(theta) dtheta}
     is 2e-6 solar masses / year.)
\seealso{Friend & Castor (1982), Fig. 4}
\done

\function{ftest_xspec}
\synopsis{calculates the F-test probability as in xspec}
\usage{Double_Type ftest_xspec(chisq2, dof2, chisq1, dof1)}
\description
    The new chi-square and DOF, chisq2 and dof2,
    should come from adding an extra model component
    to (or thawing a frozen parameter of)
    the model which gave chisq1 and dof1.
    If the F-test probability is low then it is
    reasonable to add the extra model component.

    WARNING: It is not correct to use the F-test statistic
    to test for the presence of a line
    (see Protassov et al astro-ph/0201457).
\done

\function{gal2cel}
\synopsis{Transform Cartesian Galactic to celestial coordinates}
\usage{gal2cel(Double_Types x[], y[], z[], vx[], vy[], vz[]; qualifiers)}
\description
    Transform right-handed, Cartesian Galactic coordinates (x [kpc], y [kpc], z [kpc], vx [km/s],
    vy [km/s], vz [km/s]) with the Galactic center at the origin, the Sun on the negative x-axis
    and the z-axis pointing to the north Galactic pole implying clockwise Galactic rotation when
    seen from the half space with positive z to celestial coordinates (right ascension [h, m, s],
    declination [deg, arcmin, arcsec], distance [kpc], radial velocity [km/s], proper motion in
    right ascension times cosine of declination [mas/yr], proper motion in declination [mas/yr]).
\qualifiers{
\qualifier{SunGCDist}{Sun-Galactic center distance [kpc];
      default: 8.4 (see Model I in Irrgang et al., 2013, A&A, 549, A137)}
\qualifier{vxs}{Sun's x-velocity component [km/s] relative to the local standard of rest;
      default: 11.1 (Schoenrich, Binney & Dehnen, 2010: MNRAS 403, 1829)}
\qualifier{vys}{Sun's y-velocity component [km/s] relative to the local standard of rest;
      default: 12.24 (Schoenrich, Binney & Dehnen, 2010: MNRAS 403, 1829)}
\qualifier{vzs}{Sun's z-velocity component [km/s] relative to the local standard of rest;
      default: 7.25 (Schoenrich, Binney & Dehnen, 2010: MNRAS 403, 1829)}
\qualifier{vlsr}{Local standard of rest velocity [km/s];
      default: 242 (see Model I in Irrgang et al., 2013, A&A, 549, A137)}
\qualifier{GC_NGP}{Celestial coordinates of the Galactic center and the north Galactic pole in the
      format [ah_GC, am_GC, as_GC, dd_GC, dm_GC, ds_GC, ah_NGP, am_NGP, as_NGP, dd_NGP, dm_NGP, ds_NGP];
      default: [17, 45, 37.224, -28, 56, 10.23, 12, 51, 26.282, 27, 07, 42.01] (Reid & Brunthaler, 2004, ApJ, 616, 872)}
}
\example
	(ah, am, as, dd, dm, ds, dis, vrad, pma, pmd) = gal2cel(-8.4, 0, 0, 11.1, 254.24, 7.25);
	% -> Sun's position and velocity
	(ah, am, as, dd, dm, ds, dis, vrad, pma, pmd) = gal2cel(0, 0, 0, 0, 0, 0);
	% -> Galactic center's position and velocity
	(ah, am, as, dd, dm, ds, dis, vrad, pma, pmd) = gal2cel([-8.4,0], [0,0], [0,0], [11.1,0], [254.24,0], [7.25,0]);
	% -> position and velocity of Sun and Galactic center stored in the same array
	(ah, am, as, dd, dm, ds, dis, vrad, pma, pmd) = gal2cel(-8.4, 0, 0, 11.1, 254.24, 7.25;
	GC_NGP=[17, 45, 37.1991, -28, 56, 10.221, 12, 51, 26.2755, 27, 7, 41.704]);
	% -> use non-standard coordinates for Galactic center and north Galactic pole
	(ah, am, as, dd, dm, ds, dis, vrad, pma, pmd) = gal2cel( cel2gal(17, 45, 37.224, -28, 56, 10.23, 8.33, -11.1, -2.91, -5.12) );
\seealso{cel2gal, rad2RD}
\done

\function{galactic2equatorial}
\synopsis{convert galactic (l,b) coordinates to J2000.0 equatorial (alpha,delta)}
\usage{(alpha,delta)=galactic2equatorial(l,b;qualifiers)}
\altusage{Vector_Type eqp=equatorial2galactic(gal;qualifiers)}
\qualifiers{
\qualifier{deg}{interpret angular arguments in degrees (default is radians!)
                applies also to the return value.}
}

\description
 The function  takes coordinates in the galactic (l,b) system
 and converts them into the equatorial J2000.0 system. Alternatively
 the routine also accepts a Vector_Type and then returns a
 vector in equatorial coordinates for J2000.0.

 The function is for J2000.0 coordinates only, use the
 precess function to convert the result to another equinox
 if needed.

 See the description of the function equatorial2galactic for
 references and further information. This routine is just a
 wrapper around that function, with the qualifier "inverse" set.

\seealso{Vector_Type, equatorial2galactic,precess,dms2deg,hms2deg}
\done

\function{galactic_rotation_velocity}
\usage{Double_Type galactic_rotation_velocity(Double_Type r; model=m)}
\qualifiers{
\qualifier{model}{\code{m=1} selects Model I (B&T, Fig. 2.20), \code{m=2} selects Model II (B&T, Fig. 2.22)
             If neither \code{m=1} nor \code{m=2} is specified, the constant 220 is returned.}
}
\description
    \code{r} is the distance from the Galactic Center in kpc.
    (\code{0.18 <= r <= 11.9} is covered by a lookup table.)
    The return value is the Galactic rotation velocity in km/s.
\seealso{J. Binney & S. Tremaine: Galactic Dynamics (2nd ed.), Sect. 2.7 (pp. 110ff)}
\done

\function{galaxy_elliptical (fit-function)}
\synopsis{fits a host galaxy in the optical/UV}
\description
	This function fits an elliptical host galaxy to the data, taking into
       account the redshift z.
  

\examples
    % data definition:
    load_data("optical.pha");
    fit_fun("galaxy_elliptical(1)+powerlaw(1)");
    

\done

\function{galLB_from_RAdec}
\synopsis{convert equatorial (RA, dec) coordinates to galactic (l, b) coordinates}
\usage{(l, b) = galLB_from_RAdec(RA, dec);}
\qualifiers{
\qualifier{ra_unit }{ [\code{="deg"}]: set the unit of the right ascension \code{RA}
                         \code{l_unit="rad"} \code{=>}  \code{RA} in rad
                         \code{l_unit="hms"} \code{=>}  \code{RA} in hours as a scalar
                         or an array of the form \code{[H, M]} or \code{[H, M, S]} }
\qualifier{dec_unit}{ [\code{="deg"}]: set the unit of the declination \code{dec}}
}
\description
    This function is deprecated. Please use equatorial2galactic instead.
\seealso{equatorial2galactic,galactic2equatorial}
\done

\function{galridge (fit-function)}
\synopsis{model for Galactic Ridge Emission}
\description
    Model for Galactic Ridge emission based on measurements
    and analysis by:
    Ebisawa, K., et al., 2008, PASJ, 60, S223-S230
    Obst, M., 2011, Bsc-Thesis, Remeis observatory

    The model consists of two bremsstrahlung continua and
    three gaussian emission lines, which originate from
    neutral, hydrogen- and helium-like Fe K_alpha lines
    at 6.40, 6.67 and 7.00 keV:
      2*bremss + 3*egauss
    The equivlant widths (areas) of those lines are set to
    free by default, but scaled initially by
      85:458:129
    as found by Ebisawa. To fix those ratios or different
    ones, one might use 'set_par_fun', e.g.
    
    set_par_fun("galridge(1).Fe67","galridge(1).Fe64/85*458");
    set_par_fun("galridge(1).Fe70","galridge(1).Fe64/85*129");

    The line widths are all freezed to 0.05 keV.
    The two continua are each described by a normalization
    factor (norm1/norm2) and a plasma temperature (kT1/kT2).
    By default, the second bremsstrahlung continuum is
    disabled by setting its norm to zero.
\seealso{bremss,egauss,set_par_fun}
\done

\function{gauss_2d_integrated}
\synopsis{fit a two dimensional Gaussian profile to a bin integrated image}
\description
    For fitting 2d data, define_counts_2d can be used to define
    a pseudo 1d spectrum by reshaping arrays.
    Contrary to \code{gauss_2d} this function allows to fit bin
    integrated values. Before using the fit function \code{gauss_2d_integrated}
    the "counts" have to be defined with \code{define_counts_2d} and the
    data grid has to be defined with \code{set_2d_data_grid}.
    To fit integrated values (X_lo,X_hi, Y_lo, Y_hi) have to be specified
    in \code{set_2d_data_grid}.
\examples
    variable delt = 0.1;
    variable X = [-2:2:delt];
    variable Y = @X;%
    variable XX, YY;  (XX, YY) = get_grid(X, Y);
    variable img = cos(XX)^2*cos(YY)^2*100;

    variable id = define_counts_2d (img);
    set_2d_data_grid (X-0.5*delt, X+0.5*delt,Y-0.5*delt, Y+0.5*delt); % valid value in pixel center
    fit_fun("gauss_2d_integrated(1)");
    set_fit_statistic ("chisqr;sigma=gehrels");
    set_par("gauss_2d_integrated(1).x0",0);
    set_par("gauss_2d_integrated(1).A", sum(img));
    ()=fit_counts;
    plot_image(get_2d_model(id));
\seealso{gauss_2d, define_counts_2d, set_2d_data_grid}
\done

\function{Gauss_complex}
\synopsis{Compute complex Gauss profile}
\usage{Complex_Type = Gauss_complex(z, z0, sigma);}
\done

\function{gcrs2j2000_matrix}
\synopsis{return the frame bias matrix to convert coordinates from the GCRS to J2000.0}
\usage{Matrix33_Type mat=gcrs2j2000_matrix();}
\description
  This function returns the frame bias matrix needed to convert
  a vector in the Geocentric Celestial Reference System (GCRS)
  into the J2000.0 dynamical reference system (see
  astronomical almanac, section B)

\seealso{precess,precession_matrix}
\done

\function{gehrels_confidence}
\synopsis{Calculate lower and upper confidence bounds according to Gehrels, 1986}
\usage{Double_Type[2] conf = gehrels_confidence(Integer_Type n, Double_Type CL);}
\description
    Estimate the lower and upper bounds of the confidence interval given by
    the level CL and the number of counts n using
    \code{gehrels_lower_confidence} and \code{gehrles_upper_confidence}.
    Please also read the documentation of these two functions.
    The first entry in the retured array is the lower bound, the second one
    the upper bound.
\seealso{gehrels_lower_confidence, gehrles_upper_confidence}
\done

\function{gehrels_error}
\synopsis{Calculate lower and upper error according to Gehrels, 1986}
\usage{Double_Type[2] err = gehrels_error(Integer_Type n, Double_Type CL);}
\description
    Same as \code{gehrels_confidence} but returns the errors instead.
\seealso{gehrels_lower_confidence, gehrles_upper_confidence}
\done

\function{gehrels_lower_confidence}
\synopsis{Calculate lower confidence bounds according to Gehrels, 1986}
\usage{Double_Type gehrels_lower_confidence(Double_Type n, Double_Type CL);}
\description
    Estimate the lower bound of the confidence interval given by the level
    CL and the number of counts n.
    It uses equation 14 in Gehrels, 1986.
    Please note that the confidence level must be in [0.8413,0.9995].
    The values for beta and gamma necessary for the calculation are
    interpolated using a single parametric powerlaw for beta and a double
    parametric powerlaw for gamma which was fitted to the data given in
    table 3 of Gehrels, 1986.
    Reading the source code is encounraged.
\seealso{gehrels_lower_confidence, gehrels_confidence}
\done

\function{gehrels_upper_confidence}
\synopsis{Calculate upper confidence bounds according to Gehrels, 1986}
\usage{Double_Type gehrels_upper_confidence(Double_Type n, Double_Type CL);}
\description
    Estimate the upper bound of the confidence interval given by the level
    CL and the number of counts n.
    It uses equation 9 in Gehrels, 1986.
    Please note that the confidence level must be in [0.8413,0.9995].
\seealso{gehrels_lower_confidence, gehrels_confidence}
\done

\function{generate_iauname}
\synopsis{for a given position, generate a coordinate string obeying the IAU convention}
\usage{string=generate_iauname(ra,dec)}
\qualifiers{
 \qualifier{prefix}{prefix for the string (e.g., "XMMU_")}
 \qualifier{radian}{if set, the angles are in radians, not degrees}
}
\description
 This is a convenience routine to produce strings of the type
 XMMU Jhhmmss.s+ddmmss
 from J2000.0 positions. The routine obeys the IAU convention of
 truncating (not rounding!) the coordinate to the digits shown.

 The default-string is "Jhhmmss.s+ddmmss", use the prefix qualifier
 to prepend the mission name.

 Use angle2string to format coordinates with appropriate rounding.

\seealso{dms2deg, hms2deg, angle2string, deg2dms}
\done

\function{geographic2vector}
\synopsis{Calculate a geocentric vector from geographic or geodetic coordinates.}
\usage{Vector_Type=geographic2vector(lambda,phi,hei;qualifiers)}
\qualifiers{
   \qualifier{deg}{if set, then lambda and phi are measured in degrees,
                     (default: radian).}
   \qualifier{datum}{a string defining the geodetic datum. The default
                     is WGS 84, which corresponds to GPS coordinates. Other
                     geodetic data are IERS 2010, GRS 80, IAU 1976, GRS 67,
                     IAU 1964, Hayford (1924), Clarke (1866), and Airy (1830),
                     per the parameters given in the Astronomical Almanac.}
   \qualifier{geocentric}{The coordinates are not geodetic, but geocentric. 
                     In this case hei is interpreted as the distance from the
                     geocenter (otherwise it is the height with respect to
                     the reference ellipsoid).
                     Note: it is very rare for coordinates to be geocentric!
                     GPS is geodetic.}
}
\description

 For a given set of geodetic coordinates, i.e., geographic longitude (positive
 towards the east!), latitude (positive on the Northern hemisphere), and a
 height  above the reference spheroid (close to height above mean sea level),
 calculate the x,y,z-position in a geocentric coordinate system. Here, the
 xy-plane is the Earth's equator, the xz-plane the ITRS meridian (close to
 Greenwich), and the z-axis points towards the Earth's north pole. The xyz-
 values are in meters.

 Note that the longitude and latitude are assumed to be in rad, unless the
 deg qualifier is given. The height is in meters.

 The routine returns a Vector_Type. If called with arrays, then an array
 of Vector_Type is returned.

\seealso{dms2deg}
\done

\function{get1match}
\synopsis{returns one (the first) matching substring of a regular expression matching}
\usage{String_Type match_str = get1match(String_Type str, String_Type regexp);}
\description
   get1match is deprecated. Use S-Lang's string_matches instead.
\seealso{string_matches}
\done

\function{getCommonGTIs}
\synopsis{returns a new set of GTIs which are both contained in gti1 and gti2}
\usage{Struct_Type gti = getCommonGTIs(Struct_Type gti1, Struct_Type gti2);}
\description
    Good time intervals (GTIs) are stored in \code{{ start, stop }}-structs.
\done

\function{getVarPeriod}
\synopsis{Determines the period of a variation in a lightcurve}
\usage{Double_Type getVarPeriod(Struct_Type lc, Double_Type minperiod, Double_Type maxperiod)}
\qualifiers{
    \qualifier{plot}{plot chi-square distribution and modelled
              gaussians. Pause the assigned number in
              seconds between each step (default 0.3)}
    \qualifier{nogauss}{skip the gaussian fit}

 \qualifier{PIPED TO EPFOLD}{:}
    \qualifier{dt}{exposure of every lightcurve time bin, should
               be given to ensure correct results.}
    \qualifier{sampling}{how many periods per peak to use (default=10)}
    \qualifier{nsrch}{how many periods to search in a linear grid (default not set)}
    \qualifier{dp}{delta period of linear period grid (default not set)}
    \qualifier{lstat}{use L-statistics instead of chi^2 statistics}
    \qualifier{nbins}{number of bins for the pulse profile}
    \qualifier{exact}{calculate the pulse profile in a more exact
               way, see description of pfold (not recommed
               as it takes a very long time!).}}
\description
    ****
    This function is deprecated!! It will be removed in
    2017 Mai. If you heavily depend on this function,
    please write an email to
    matthias.kuehnel@sternwarte.uni-erlangen.de
    ****
    Performs an Epoch Folding of the lightcurve in the
    given period range (same time unit as used in the
    lightcurve) and returns the period corresponding
    to the maximum.
    If not skipped by the 'nogauss' qualifier, a gaussian
    it fitted to the maximum to get its position more
    accurate. The gaussian parameters and used data-
    points are applied automatically and improved step
    by step. The steps can be monitored using the 'plot'
    qualifier.
\seealso{epfold}
\done

\function{get_all_data}

\synopsis{Get a list of all data-set indices}
\usage{List_Type = get_all_data;}
\description

       This function returns a list of data indices, which have been obtained via all_data. 
       The information is saved in a List, not an Array, which allows plotting of all datasets. 

\example
	isis>variable d = get_all_data;
	isis>plot_data(d);

\seealso{all_data}
\done

\function{get_arg_struct}
\synopsis{obtains the arguments/options the current isis instances was called with.}
\usage{Struct_Type = get_arg_struct( );}
\altusage{Struct_Type = get_arg_struct( String_Type[] name, type);}
\qualifiers{
\qualifier{delim}{[="="] Delimeter between argument name and value (e.g., also, "=.,").}
\qualifier{prefix}{[="--"] Argument prefix. Arguments missing this prefix are ignored.}
}
\description
   This function returns a Struct_Type with fields corresponding to
   the names of those arguments/options the current isis instance
   was called with, which have the given 'prefix'. Thereby the 'name'
   is defined as the string enclosed by the 'prefix' and the 'delimeter'.

   Expected SYNTAX: <prefix>"argname"<delimeter>"value"

   It is possible to give several 'delimeter' as a string-chain (e.g.,
   "=,.-"). If an argument/option contains several delimeters, only the
   string between the 1st and 2nd delimeter is taken as value for the
   corresponding name.

   This function can be given an string array 'name' and 'type', assigning
   a Data_Type to the argument 'name'. Thereby 'type' can be:

         "d" : Integer_Type
         "f" : Double_Type

   Otherwise, if used without arguments ('name' & 'type'), the
   values are returned as String_Type.

   If an argument has no value given, i.e. does not contain a delimeter
   the corresponding field of the returned structure is equal NULL.
\example
   /> isis -g --arg --arg1=1e-3 --arg2=1.01 --arg3=abcd --arg4=1=bla

   isis> print( get_arg_struct );
   {arg=NULL,
    arg1="1e-3",
    arg2="1.01",
    arg3="abcd",
    arg4="1"}

   isis> print( get_arg_struct("arg1","f") );
   {arg=NULL,
    arg1=0.001,
    arg2="1.01",
    arg3="abcd",
    arg4="1"}

   isis> print( get_arg_struct(["arg1","arg2","arg4"],["f","f","d"]) );
   {arg=NULL,
    arg1=0.001,
    arg2=1.01,
    arg3="abcd",
    arg4=1}

\seealso{atof, atoi, strreplace, strtok, set_struct_field, array_map, is_substr}
\done

\function{get_bin_corr_factor}
\synopsis{reads the bin correction factor set by set_bin_corr_factor}
\usage{Double_Type[nbins] corr_factor = set_bin_corr_factor(Integer_Type data_id)}
\seealso{load_fermi,set_bin_corr_factor}
\done

\function{get_blocks_data}
\synopsis{returnes the data of a Bayesian block representation}
\usage{Struct_Type blockdata = get_blocks_data(Struct_Type blocks[, Integer_Type dataindex]);}
\altusage{blockdata= get_blocks_data(Struct_Type[] blocks[, ...]);}
\qualifiers{
    \qualifier{datafun}{reference to a function to be called to
              derive the block data (default: see text)}
    \qualifier{errfun}{same as 'datafun', but for the block
              uncertainties}
    \qualifier{bydt}{divides the output data and uncertainties
              by the length of the corresponding block,
              can be assigned to a number used to divide
              the data to for further normalization}
}
\description
    Returnes the data of all block over time (e.g., a
    lightcurve) with variable binsize from a previous
    run of 'bayesian_blocks'. The default output
    structure has the fields:
      time  - lower time bin of a block
      dt    - length of each block
      data  - the data in the blocks and
      error - and the corresponding uncertainties
    The data and their uncertainties are calculated
    from the original input data by functions (if
    multiple dataset are present use the optional
    'dataindex' parameter), which will be called for
    each block and can be specified via qualifiers.
    Their calling sequence is
      Double_Type function(time, dt, data[, error])
    where error is given in case of data mode 3
    (point measurements) only. The default functions
    depend on the data mode used previously:
    1: Event data
      datafun = sum(events)
      errfun  = sqrt(sum(events))
    2: Binned data
      datafun = sum(nn_vec)
      errfun  = sqrt(sum(nn_vec))
    3: Point measurements
      datafun = weighted_mean(cell_data[0],
                              cell_data[1]; err)
      errfun  = sqrt(sumsq(datafun - cell_data[0])
                     / (length(cell_data[0]) - 1)
                     / length(cell_data[0]))
\example
    % let 'evts' be a list of time-tagged events,
    % find bayesian blocks
    blocks = bayesian_blocks(struct { tt = evts });

    % create a lightcurve of the block representation,
    % 'bydt' will convert counts to rate
    lc = get_blocks_data(blocks; bydt);

    % plot the blocks
    hplot(lc.time, lc.time+lc.dt, lc.data);
\seealso{bayesian_blocks}
\done

\function{get_color_map}
\synopsis{Get an intrinsic color map or construct one}
\usage{UInt_Type[256] map = get_color_map(name);}
\qualifiers{
  \qualifier{reverse}{Reverse returned map}
}
\description
  Retreive a colormap, either from the internal list, or
  constructed from a color palette.

  Qualifiers are passed to the constructor.

\seealso{get_color_palette, png_get_colormap_names}
\done

\function{get_color_palette}
\synopsis{Get a number of colors from a color palette}
\usage{UInt_Type[] = palette get_color_palette(String_Type name, UInt_Type n);}
\qualifiers{
  \qualifier{repeat}{If given, repeat colors to match \code{n}}
  \qualifier{cmap}{If given, force colormap search}
  \qualifier{string}{Return colormap as HEX string instead of value}
}
\description
  Get \code{n} colors (as hex encoded values) from a palette. Known
  palette names can be obtained by \code{get_color_palette_names}.

  This function forwards all qualifiers to the generator functions (if any).
  If no palette matching \code{name} is found, a colormap is searched.
  To generate a palette from a colormap only, use the \code{cmap}
  qualifier. For information how palettes are generated from colormaps
  see \code{palette--map}, else see \code{palette--name}, where \code{name}
  is the first argument.
  
  If it is not a colormap nor a generator function, a static color map is
  used. They are usually named by like \code{family:palette}.
  For most of them, information can be obtained via \code{palette--family}.

  Eventually the used map only has a finite number of colors. If \code{n}
  exceeds this number, only the maximum number of colors are returned. To
  repeat colors up to \code{n} times, use the \code{repeat} qualifier.  

\seealso{palette--hsl,palette--map,palette--sron}
\done

\function{get_color_palette_names}
\synopsis{Get list of available color palette names}
\usage{String_Type[] names = get_color_palette_names();}
\seealso{get_color_palette}
\done

\function{get_column_density_from_line}
\synopsis{}
\usage{Double_Type N = get_column_density_from_line([Integer_Type i,] String_Type line);}
\description
     N  =  mc^2/[pi e^2] * W_lambda / [f_lu * lambda^2];\n
        =  1.13e17/cm^2 * (W_lambda/mA) / [f_lu * (lambda/A)^2]\n
                                        %  f_lu = mc / [8 pi^2 e^2] * lambda^2 * g_u / g_l * A_ul\n
                                        %       = 1.4992e-16 * (lambda/A)^2 * g_u / g_l * (A_ul/s^{-1})\n
\done

\function{get_combined_data_model_residuals}
\synopsis{returns these 3 structures for a combination of data sets}
\usage{(d, m, r) = get_combined_data_model_residuals(Integer_Type id[]);}
\description
   All \code{d}, \code{m} and \code{r} are \code{bin_lo, bin_hi, value, err} structures.
   \code{d} and \code{m} are the sum of data and model counts of all data sets,
   rebinned to the grid of the first data set id[0].
   \code{d.err} is calculated from quadratic error propagation.
   \code{r.value = (d.value-m.value)/d.err} contains the residuals
   unless the \code{ratio} qualifier is set (see below).
\qualifiers{
\qualifier{ratio}{use ratio-residuals \code{r.value = d.value/m.value}}
}
\seealso{get_data_counts, get_model_counts, rebin}
\done

\function{get_component}
\synopsis{to extract the component from the epoch files}
\usage{get_component([array of position number in the epoch file], [array of epoch fits files]);}
\qualifiers{
\qualifier{quadrant}{[=1] Quadrant which is defined as a positive distance ( RA, DEC > 0 == 1 || RA < 0, DEC > 0 == 2 ||
  RA < 0, DEC < 0 == 3 || RA > 0, DEC < 0 == 4)}
\qualifier{zero}{shift all distances, so that the last component is at 0,0 ("core" component) }
}
\description
   This function returns a structure of a component.
   The required input is the position number starting by 1 in each epoch, and a corresponding list of epoch fits files.
   If a component is not existent in an epoch, the position number of the component must be 0.
\done

\function{get_confmap}
\synopsis{computes a 2d confidence map and possibly stores the fit-parameters in a file}
\usage{conf_map = get_confmap(par1, min1, max1[, n1], par2, min2, max2[, n2]);}
\qualifiers{
\qualifier{save}{[=\code{"confmap"}]: save all parameters to \code{save}+".fits"}
\qualifier{fail}{[=\code{NULL}]: failure recovery hook, see \code{conf_map_counts}}
\qualifier{mask}{[=\code{NULL}]: region mask out hook, see \code{conf_map_counts}}
}
\description
    (All qualifiers are also passed to the \code{conf_map_counts} function.)

    \code{par1} is stepped from \code{min1} to \code{max1} in \code{n1} (default=8) steps;
    \code{par2} is stepped from \code{min2} to \code{max2} in \code{n2} (default=8) steps.
    A save hook is used to write each step's parameter values and chi^2 to files
    named \code{sav+"*.dat}, which are finally collected by \code{get_confmap_collect_results}
    and converted to a table.
    Parameters from this table can be set with \code{set_par_from_confmap_table}.
\seealso{conf_grid, conf_map_counts, load_conf, plot_conf, get_confmap_collect_results, set_par_from_confmap_table}
\done

\function{get_confmap_collect_results}
\synopsis{collect results produced by get_confmap}
\usage{get_confmap_collect_results(String_Type save_basefilename);}
\description
    \code{get_confmap_collect_results} is used internally by \code{get_confmap},
    but it can also collect the results of an unfinsihed calculation.
\qualifiers{
\qualifier{remove_files}{: if set, \code{save_basefilename+"_*.dat"} files will be deleted when read}
\qualifier{use_file_from_save_conf}{: if set, results are appended to \code{save_basefilename+".fits"}}
}
\seealso{get_confmap}
\done

\function{get_contour_lines}
\synopsis{finds a set of contour lines for a 2d-array of values}
\usage{Struct_Type l[] = get_contour_lines(Double_Type f[], Double_Type f0);}
\qualifiers{
\qualifier{save}{filename of a FITS file to save contours}
}
\description
    \code{f} has to be a two-dimensional array (an image).
    The return value is an array of \code{struct { x, y }},
    whose fields \code{x} and \code{y} contain the indices
    for the contour line \code{f[y,x] = f0}.
\done

\function{get_coordinatearrays_of_image}
\synopsis{returns two 2d arrays of image coordinates}
\usage{(XX, YY) = get_coordinatearrays_of_image(Any_Type img[,]);}
\description
    \code{XX[y, x] = x}  and  \code{YY[y, x] = y} for every coordinate (\code{x,y}) of \code{img}.
\done

\function{get_count_rate}

\synopsis{Get different variants of countrates and counts}
\usage{Double_Type CR = get_count_rate (hist_index);
or
Double_Type (CR , CR_err) = get_count_rate (hist_index ; err);}
\description

       Use this function to retrive background subtracted
       countrates (default), countrates, background rates, or counts
       and respective uncertainties for spectra that have been loaded.
       Errors are estimated using the methode described by Gehrels 1986,
       see \code{gehrels_error} for details. If the err_asym qualifier is not
       specified the average of both errors is returned. If it is, an array
       is returned which contains the lower error in the first entry and the
       upper error as the second entry. The default confidence level for the
       error is 0.9, but can be changed using the err_conf qualifier within
       the possibilities offered by \code{gehrels_error}.

\qualifiers{
    \qualifier{tex}{additional TeXoutput of CRs, choose this if you need
         values and uncertainties in LaTeX-format (only printed in terminal)}
    \qualifier{bkg}{0 (default: background subtracted CR), 1 (count rates),
         2 (background count rate)}
    \qualifier{fake}{Set this qualifier if you have a faked spectrum and a
          faked background (which has not been deleted!);
          Assumption: hist_index(bkg) = hist_index(dset) +1 !!}
    \qualifier{fake_expo}{set exposure time of fake spectrum, in case
               set_data_exposure was not used when faking}
    \qualifier{bkg_id}{set hist_index(bkg) if background is loaded or faked, if set to 0, no background
            is subtracted. Will overrule the id-setting made by fake-qualifier}
    \qualifier{E_range}{Set this qualifier if you want the countrate within
             a certain energy range. Use E_range = [E_min , E_max].
             If not given full energy range is used. (Energy in keV!)}
    \qualifier{counts}{if present function will only return counts instead
             of countrates}
    \qualifier{err}{if present function will return CR and
             respective uncertainty}
    \qualifier{err_conf}{Confidence limit for the error estimation}
    \qualifier{err_asym}{Return asymmetric errorbars as an array}
}

\example
       isis> xray = load_data("data.pha");
       isis> variable countrate = get_count_rate (1 ;tex, bkg=1 , E_range = [0.2 , 2.3]);
       isis> variable counts , counts_err;
       isis> (counts , counts_err) = get_count_rate (1 ;tex, bkg=1 , E_range = [0.2 , 2.3] , counts , err);

\seealso{get_data_counts; TeX_value_pm_error; cut_dataset_range;}
\done

\function{get_dataset_response}
\usage{Response = get_dataset_response(id, [channel]);}
\synopsis{Compute response of dataset associated to 'id' to delta peak}
\qualifiers{
\qualifier{energy}{If given, return response on energy grid (default: wavelength)}
}
\description
    This function evaluates a delta peak function set at each nominal
    grid value folded through the response of dataset \code{id}. If a
    channel is given, only the response for the corresponding channel
    is calculated.

    The returned value is either an array of arrays for each channel in the
    detector, or the array of the response in channel \code{channel}.
    The delta function is placed at the nominal energy of the channel. If
    \code{channel} is outside of the detector channels, a zero array is
    returned.

    Per default the response is returned as 'seen' by ISIS, that is, on
    a wavelength grid. Use the 'energy' qualifier to revers the
    response.

\seealso{get_rmf_data_grid, assign_rmf, assign_rsp}
\done

\function{get_data_counts_with_tot_err}
\synopsis{returns spectral data, taking systematic errors into account}
\usage{Struct_Type data = get_data_counts_with_tot_err(Integer_Type id);}
\description
    \code{data.err = sqrt( stat_err^2 + [sys_err_frac * data.value]^2 );}
\seealso{get_data_counts, get_sys_err_frac}
\done

\function{get_ephemeris}
\synopsis{returns the epoch and period of an ephemeris}
\usage{Struct_Type get_ephemeris(String_Type ephemeris)}
\description
    The list of known ephemerides is stored internally
    and is shown if an unknown ephemeris is requested.
    Ephemerides are returned as a structure, containing
    at least the internal \code{name} of the ephemeris (which
    usually contains the source name and a reference),
    the epoch \code{T0}, and the period \code{P}.
    More information may be stored in further reasonably named
    fields, e.g., \code{Pdot} for the change of the period.
\seealso{orbitalphase}
\done

\function{get_flux_corrected_convolved_model_flux}
\synopsis{computes the model flux which can be compared with a flux-corrected spectrum}
\usage{Struct_Type get_flux_corrected_convolved_model_flux(Integer_Type hist_index)}
\description
    The flux-corrected convolved model flux is computed as\n
      \code{            int K(R(h,E), A(E), S(E)) dE  }\n
      \code{ F(h)  =  --------------------------------} ,\n
      \code{            int K(R(h,E), A(E),  1  ) dE  }\n
    where \code{S(E)} is the flux model, \code{A(E)} is the effective area (the ARF),
    \code{R(h,E)} is the redistribution function (the RMF),
    and the kernel \code{K} defaults to \code{K(R,A,S) = R*A*S}.

    The flux-corrected convolved model fluxes is defined in such a way
    that a folded model is "unfolded" in the same way as the data are
    by virtue of \code{flux_corr}. It can thus be compared with flux-corrected data.
\seealso{flux_corr, get_model_counts, get_model_flux, get_convolved_model_flux}
\done

\function{get_grid}
\usage{(XX, YY) = get_grid(Double_Type X[], Double_Type Y[]);}
\description
    \code{XX} and \code{YY} are two-dimensional arrays
    which span the grid defined by the arrays \code{X} and \code{Y}.
\example
    variable XX, YY;  (XX, YY) = get_grid([-2:2:0.01], [-2:2:0.01]);
    plot_image( atan2(YY, XX) );
\done

\function{get_instrument_resolution_from_data}
\usage{R = get_instrument_resolution_from_data(id)}
\synopsis{Compute energy resolution from data set 'id'}
\qualifiers{
\qualifier{energy}{If given will return the grid as energy grid
               instead of wavelength}
}
\description
   This function can be used to estimate the instrument resolution
   from the data set specified with id. The return value is a struct
   { value, bin_lo, bin_hi } where bin_lo and bin_hi is the data
   grid (usually in Angstrom) and value is the resolution in keV.

   Note that this function removes any binning from the data set.

\seealso{hist_fwhm_index}
\done

\function{get_intersection}
\synopsis{finds elements occuring in two arrays}
\usage{(Integer_Type i1[], i2[]) = get_intersection(array1[], array2[]);}
\description
     \code{array1[ i1 ]  ==  array2[ i2 ]}
\qualifiers{
\qualifier{ordered}{assumes that \code{array1} and \code{array2} are ordered increasingly}
}
\done

\function{get_lambda_parameters_of_all_lines}
\done

\function{get_lambda_parameters_of_lines}
\done

\function{get_lambda_parameters_of_lines_from_one_ion}
\done

\function{get_line_labels}
\qualifiers{
\qualifier{label_Ly_He}{ [=1]}
\qualifier{print_list}{}
}
\seealso{lines}
\done

\function{get_line_velocity}
\synopsis{calculates the velocity shift in a line of the lines-model}
\usage{Double_Type v = get_line_velocity([Integer_Type i,] String_Type line);}
\description
    \code{lambda = get_par("lines(i).line_lam");}\n
    \code{lambda0 = mean( get_line_lambdas(line) );}\n
    \code{v = (lambda-lambda0)/lambda_0 * c;  %} speed of light \code{c = 299792} km/s
\seealso{Doppler_velocity}
\done

\function{get_map_value_at_position}
\synopsis{Extract map value from ra/dec position}
\usage{Double_Type value = get_map_value_at_position(map,ra,dec);}
\qualifiers{
\qualifier{verbose}{Output detector coordinates on display. Default: 0}
}
\description
   This function returns the value of a map at a certain ra/dec position
   using the wcs of that map. Ra/Dec coordinates must be given as
   decimal numbers.
\seealso{read_difmap_fits, fitswcs_get_img_wcs, wcsfuns_project}
\done

\function{get_modelcomponents}
\synopsis{load binned model component values}
\usage{Struct_Type[] mc = get_modelcomponent( Integer_Type hist_index);}
\altusage{Struct_Type[] mc = get_modelcomponent( );}
\qualifiers{
\qualifier{fct [=NULL]:}{ NULL:    Using 'get_model' (default)
                  "counts": Using 'get_model_counts'
                  "flux":   Using 'get_model_flux'
 }
}
\description
     For each model component (see 'get_fun_components') within the given
     dataset 'hist_index' this funtion returns a stucture with three array
     fields, bin_lo [Angstrom], bin_hi [Angstrom] and value, containging
     only the contribution of this component to the overall model.
     If 'hist_index' isn ot given all included datasets are used!

     The model contribution of each component is obtained by setting all
     the norm parameters (get_par_info: is_a_norm=1) of all other components
     to zero and evaluating the model. NOTE THAT for each component an
     'eval_counts' is called!

     The 'fct' qualifier determines which function should be used to
     obtain the model components.
     
                                              
\seealso{get_fun_components, get_model, get_model_counts, get_model_flux}
\done

\function{get_params_from_file}
\synopsis{retrieves fit-parameter information from a file}
\usage{Struct_Type params[] = get_params_from_file(String_Type params);}
\seealso{load_par, get_params}
\done

\function{get_params_table_from_files}
\synopsis{retrieves fit-parameter information from files}
\usage{Struct_Type get_params_table_from_files(String_Type filenames[]);}
\qualifiers{
\qualifier{free}{only include free parameters}
\qualifier{par}{array of parameters to be included}
\qualifier{filename}{include filenames into the structure}
\qualifier{nominmax}{do not include min max values}
\qualifier{eval_counts}{evaluates the model and returns the statistic as well.
                   It is assumed that the appropriate data is already loaded.}
\qualifier{verbose}{show name of parameter files while being processed}
}
\description
    \code{filename} may be a globbing expression.
    It is assumed that all parameter files rely on the same model
    and that this model can be loaded with the current data set.
\done

\function{get_param_from_filename}
\synopsis{returns a double parameter value from a string / filename}
\usage{Double_Type val = get_param_from_filename(filename,key,regexp);}
\description
    For a given filename = data/f4l_flux0001000muCrabpattern.fits, the
    value can be extracted by specifying the key="flux"; and a regexp
    of regexp="flux\\([0-9.]+\\)muCrab"R;
    Note that there is a strict naming convention:
    1) only one "." to mark the file type
    2) parameters are separated by "_"
    3) the "key" parameter always has to be part of the parameter string
       (i.e. fluxXXXXXXXmuCrab or fluxX.XXXXXXcgs without an underscore)
    
    If onle one argument is given, the above example for the flux is
    automatically used as key and regexp.
\seealso{simput_athenacrab}
\done

\function{get_par_combinations}
\synopsis{calculates all possible parameter combinations}
\usage{Struct_Type[] parcomb = get_par_combinations( Struct_Type par )}
\description
   All possible parameter combinations are calculated for the
   given parameter structure. That means if there are multiple
   parameters, each with there own grid all possible combinations
   are returned.

   'par' must be a Struct_Type, but there are no restriction
   otherwise. Each field is representing a parameter and its
   value its grid, which can be any Data_Type (e.g., Double_Type
   or String_Type)!

   NOTE that the combination is done, s.t. the last parameter
   in the struct is varied first and the first parameter last,
   i.e., the first parameter in the parameter combination
   will stay constant longest!

\examples
   variable par = struct{ a = [0:1:#3],
                          b = ["hello","world"],
                          c = [ 2001, 2012]
                        };
   variable parcomb = get_par_combinations( par );
   print(parcomb);
\done

\function{get_rainbow_col}
\synopsis{get a rainbow color}
\usage{color = get_rainbow_col(Integer_Type value, Integer_Type num_colors);}
\description
  The value has to be within [0:num_colors-1], to get the correct
  color here.
\seealso{xfig_new_color}
\done

\function{get_ratio}
\synopsis{calculates data/model ratios}
\usage{Struct_Type rat[] = get_ratio(Integer_Type id[]);}
\description
    Each \code{rat[i]} is a \code{{ bin_lo, bin_hi, value, err }} structure
    where \code{rat[i].value} contains the (data counts)/(model counts)
    ratios obtained for the data set \code{id[i]}.\n
    If only a scalar \code{id} is given, only a single structure is returned.
\seealso{get_data_counts, get_model_counts, get_residuals}
\done

\function{get_residuals}
\synopsis{calculates (data-model)/error residuals}
\usage{Struct_Type res = get_residuals(Integer_Type id[]);}
\description
    Each \code{res[i]} is a \code{{ bin_lo, bin_hi, value, err }} structure
    where \code{res[i].value} contains the (data counts - model counts)/error
    residuals obtained for the data set \code{id[i]}.\n
    If only a scalar \code{id} is given, only a single structure is returned.
\qualifiers{
\qualifier{noticed}{restrict to noticed bins}
\qualifier{keV}{convert Angstrom-bins to keV-bins}
}
\seealso{get_data_counts, get_model_counts, get_ratio}
\done

\function{get_selected_data_flux_en}
\synopsis{returns the flux of a data set in energy units}
\usage{flux = get_selected_data_flux_en(id[, Emin, Emax[, alpha]]);}
\description
    \code{flux} is a \code{{ bin_lo, bin_hi, value, err}} structure
    containing energy bins of data set \code{id}
    and the spectral photon flux density (in 1/s/cm^2/keV, unless \code{alpha!=0}).\n
    If \code{Emin} and \code{Emax} are used, only values in this energy range are considered.\n
    If \code{alpha}!=0, the flux values are multiplied with E^\code{alpha}.
\example
    \code{hplot( get_selected_data_flux_en(1, 4, 20, 1) );}\n
    % plots the spectral energy flux density of data set 1 in the 4--20 keV range
\done

\function{get_simple_gpile_info}
\synopsis{retrieves pileup information within a simple_gpile2 model}
\usage{Struct_Type info = get_simple_gpile_info(Integer_Type id);}
\done

\function{get_simputfile_struct}
\synopsis{get a very basic SIMPUT structure for further editing}
\usage{get_simputfile_struct(filename, RA, Dec flux);}
\description
    Note: All parameters and there usage are according to the simputfile
    routine of the SIMPUT package. E.g., flux is given in erg/cm^2/s.
\qualifiers{
\qualifier{crab}{flux is given in units of Crab}
\qualifier{emin}{[2.0]: lower limit of the energy band of the given flux in keV}
\qualifier{emax}{[10.0]: lower limit of the energy band of the given flux in keV}
}
\seealso{create_basic_simputfile,eval_simputfile,set_simputfile_model_grid,set_simputfile_flux}
\done

\function{get_sixte_eventfile_statistic}
\synopsis{Get the top-level information of a SIXTE event file}
\usage{Struct_Type ret = get_sixte_eventfile_statistic(String_Type filename[, Double_Type flux]);}
\description
       This function returns the valid, invalid, fractional and
       piled-up pattern numbers of the event file, together with the 
       exposure and a possible flux entry (the argument is passed
       directly into the struct). This information is based on the
       event file keywords NVALID/NGRAD1 etc., written by SIXTE. 
       
       The pattern fraction errors are calculated assuming Poisson error.
       Furthermore, it is possible to calculate (energy-)resolved
       pattern fractions by giving the lo, hi, and field qualifiers.
       Hereby, currently no distinction between non-piled up and
       piled-up patterns is made (nvalids = TYPE>=0).
       
       If the given event file is an already loaded FITS file
       (Struct_Type), the reading of the relevant header keywords is
       skipped and only the energy/channel resolved patterns are
       returned (requires lo, hi, and field qualifiers).
\qualifiers{
\qualifier{lo}{low grid for calculation of pattern fractions (in
      unit of fields}
\qualifier{hi}{high grid}
\qualifier{field}{Name of event file column for filtering (for
      example "signal" or "pha")}
\qualifier{nphot}{Adds an additional struct field with number of
      photons in the SIXTE event file}
}
\seealso{ratio_error_prop}
\done

\function{get_sixte_xml_data}
\synopsis{Loads a SIXTE XML file and outputs their attributes and values}
\usage{Struct_Type xmlinfo = get_sixte_xml_data(String_Type filename);}
\notes
    This function currently only supports XMLs of the Athena WFI and
    eROSITA missions.
\done

\function{get_source_counts}
\synopsis{calculates source counts, background counts and background-subtracted source counts from given spectrum, with errors }
\usage{Double_Type get_source_counts(Integer_Type id, Integer_Type backid, Double_Type emin, Double_Type emax);}
\description
    - spectrum and background have to be already loaded (id, backid)
    - arf and rmf have to be already loaded and assigned
\seealso{load_data,assign_rmf,assign_arf}
\done

\function{get_struct_fields}
\synopsis{returns several fields of a structure}
\usage{(Any_Type val1, val2, ...) = get_struct_fields(Struct_Type s, String_Type fieldname1, fieldname2, ...);}
\qualifiers{
\qualifier{i}{indices used for filtering array fields}
}
\description
    Each value corresponds to the according field of the structure.
    If an \code{i} qualifier is given, array-typed field values
    are filtered with these indices, i.e.,
#v+
       val = get_struct_feld(s, fieldname)[i];
#v-
    It also possible to create lists of structure field values,
    by just passing lists (or, equivalently, arrays) as arguments.
\examples
#v+
    variable table = struct { x=[1:5], y=[1:5]^2, err1=[1:5], err2=[1:5]*2 };

    plot_with_err( get_struct_fields(table, "x", "y", "err1"; i=[1,0,4,2,3]) ; connect_points);
    % equivalent to:
    % plot_with_err( table.x[1,0,4,2,3], table.y[1,0,4,2,3], table.err1[1,0,4,2,3] ; connect_points);

    plot_with_err( get_struct_fields(table, "x", "y", {"err1", "err2"}) );
    % or (even less redundant):
    plot_with_err( get_struct_fields(table, "x", "y", "err"+["1", "2"]) );
    % equivalent to:
    % plot_with_err( table.x, table.y, {table.err1, table.err2} );
#v-
\seealso{get_struct_field}
\done

\function{get_unpiled_fit_fun}
\synopsis{return the currently defined fit function without simple_gpile*}
\usage{String_Type get_unpiled_fit_fun()}
\description
    \code{simple_gpile*(Isis_Active_Dataset, } is replaced by \code{(}.
\seealso{get_fit_fun}
\done

\function{get_variable_name}
\synopsis{returns the namespace and name of a given reference}
\usage{(namespace, name) = get_variable_name(&var);}
\description
    The name of the variable or function the given
    reference points to will be returned as a string.
    The namespace where it is defined is returned as
    well. In case of a private namespace, however,
    NULL will be returned instead.
\seealso{current_namespace}
\done

\function{get_warmabs_model}
\synopsis{computes the contribution of separate elements in a warmabs-model}
\usage{Struct_Type data = get_warmabs_model();}
\done

\function{get_xydata}
\synopsis{provides the xy-data, which has been defined with \code{define_xydata}}
#c%{{{
   \usage{(x[],         y[], yerr[]) = get_xydata(Integer_Type data_id);}
\altusage{(x[], xerr[], y[], yerr[]) = get_xydata(Integer_Type data_id);}
\description
    This function returns the xy-data of dataset # \code{data_id}
    previously defined with \code{define_xydata}.
    If no x-uncertainty was defined, no \code{xerr} is returned.
\seealso{define_xydata, get_xymodel}
\done

\function{get_xyfit_function}
\synopsis{returns xy-fit-function}
#c%{{{
\usage{String_Type get_xyfit_fun();}
\seealso{xyfit_fun, list_xypar}
\done

\function{get_xymodel}
\synopsis{provides the xy-model \code{(x_mdl[], y_mdl[])} given by an xy-fit}
#c%{{{
\usage{(x_mdl[], y_mdl[]) = get_xymodel(Integer_Type data_id);}
\description
    This function returns the xy-model (\code{(x_mdl[],y_mdl[])})
    provided by the last evaluation (by \code{fit_counts} / \code{eval_counts})
    of the xy-fit-function specified with \code{xyfit_fun}.
\seealso{xyfit_fun, get_xydata, define_xydata}
\done

\function{GhoshLamb79}
\synopsis{calculates the spin-up of a neutron star after Ghosh & Lamb (1979)}
\usage{Double_Type GhoshLamb79(Double_Type pulse_period; disk);
 or Double_Type GhoshLamb79(Double_Type pulse_period; wind);}
\qualifiers{
    \qualifier{fastness}{hook into the issue of a fastness parameter >=0.9. If
               this qualifier is set to "limit" then the fastness is
               set to 0.9 in this case. If set to a reference
               function (Ref_Type) then a hook function of the form
                 Double_Type fastness_hook(fastness, P, L, M, R, B)
               is called, which is given the computed 'fastness' and
               all neutrons star parameters. It has to return the new
               value for the fastness.}
    \qualifier{neutron star parameters:}{The qualifiers L, M, R, B, mu, V0,
      cs, Vorb, a, xi (see the description for details)}
}
\description
    This implements the accretion torque theory by
      Ghosh & Lamb, 1979, ApJ 234, 296
    (here after GL79) which calculates the spin-up of a neutron star
    spinning with a period of 'pulse_period' (given in seconds). The
    returned Pdot will be in seconds per seconds.

    The theory depends on the physical parameters of the neutron star
    and the accretion mode. All of these are controlled by qualifiers.

    Two accretion modes are available (set as corresponding qualifier):
      disk - accretion from an accretion disk (Eq. 15 of GL79)
      wind - accretion from a stellar wind (Eq. 24 of GL79)

    Three neutron star parameters are used independently from the mode:
      L - the X-ray luminosity (in 10^37 erg/s; default: 1)
      M - the neutron star mass (in solar masses; default: 1.4)
      R - the neutron star radius (in km; default: 11.5)

    For the disk-mode the magnetic field strength is needed. One of
    the following quantities has to be given:
      mu - magnetic moment (in 10^30 G cm^3; default: 1)
      B  - surface magnetic field strength (in 10^12 G; overwrites mu)
    For the wind-mode several parameters are needed:
      V0   - the capture velocity (see Eq. 20 of GL79) (in km/s;
             default: 1000)
      cs   - the sound speed at the capture radius (in km/s; default: 0)
      Vorb - the orbital velocity (in km/s; default related to:
             V0^2 = Vorb^2 + Vwind^2 + cs^2 assuming Vwind = Vorb)
      a    - the binary separation (in lt-s; default: 100)
      xi   - "on the order of unity" (see Eq. 22 of GL79; default: 1)

    The function should be vectorized, i.e., an array of pulse
    periods can be given or the qualifiers can be arrays.

    In case of disk accretion, the so-called fastness parameter is
    computed after Eq. 16 of GL79. This equation is accurate by 5% for
    fastnesses <0.9. For larger values a warning message is displayed.
    For fastnesses >=1.0 the solution is a complex number and therefore
    a _NaN value is returned and an error message is displayed. The
    'fastness' qualifier can be used to hook into this issue.
\seealso{pulsarGL79}
\done

\function{GMST}
\synopsis{Calculates the Greenwich sidereal time and the local siderial time}
\usage{Double_Type GST = GMST(Double_Type JD);}

\qualifiers{
\qualifier{hour}{Return the GMST in hours, not in seconds.}
\qualifier{dut1}{The difference UT1-UTC in seconds (see IERS Bull. A).}
\qualifier{mjd}{Argument is in MJD, not in JD.}
\qualifier{gast}{Return the Greenwhich Apparent Siderial Time, GAST.}
\qualifier{era}{Return the Earth rotation angle (in rotations).}
\qualifier{lst}{Return the local mean siderial time LST.}
\qualifier{lon}{longitude for the calculation of the LST (east is positive;
               in geodetic [ITRS] coordinates, for
               practical purposes these are identical to the
               WGS-84 coordinates given by GPS).}
\qualifier{lat}{latitude for the calculation of the LST (north is positive,
                  only needed if xp and yp are given)}
\qualifier{deg}{lon and lat are in degrees, not in radian.}
\qualifier{xp}{x-coordinate of the Celestial Intermediate Pole (CIP) 
               ALWAYS in arcseconds, as given by the IERS bulletin.}
\qualifier{yp}{y-coordinate of the CIP, ALWAYS in arcseconds.}
}
\description
    This code calculates the Greenwich Mean Siderial Time and the
    Greenwhich Apparent Siderial Time in seconds.

    The argument \code{JD} is the Julian Date in the UT1 time system.
    For most applications, one can take UT1=UTC. For the most precise
    applications, a correction term  DUT1=UT1-UTC can be looked up in
    IERS Bulletin A
    (see https://www.iers.org/IERS/EN/Publications/Bulletins/bulletins.html).

    The algorithms used are described by Kaplan (2005, USNO Circular
    179, Sect. 2.6.2).

    This routine is array safe.

    Note that contrary to the coordinate transform routines the
    deg qualifier only affects the interpretation of the lon, lat
    qualifiers and does NOT affect the other arguments or return
    arguments!
\seealso{equation_equinoxes}
\done

\function{gravitational_radius}
\synopsis{calculates the gravitational radius in Meters defined as r_g=GM/c^2,
    for a mass given in units of M_sol.}
\usage{gravitational_radius(mass_in_solar)}
\done

\function{greatcircle_coordinates}
\synopsis{calculates the coordinates of the greatcircle between two points on a sphere}
\usage{(Double_Type lambda[], phi[]) = greatcircle_coordinates(lambda1, phi1, lambda2, phi2);}
\qualifiers{
\qualifier{unit}{[\code{="deg"}] unit of the angular coordinates}
\qualifier{delta}{[\code{=0.5}] angular step in degrees}
}
\description
    (\code{lambda}i, \code{phi}i) are the spherical coordinates of point i.\n
    \code{unit="deg"}: \code{lambda}i and \code{phi}i are in degrees.\n
                They can be scalar values or arrays of the form
                \code{[deg, arcmin]}, \code{[deg, arcmin, arcsec]} or \code{[sign, deg, arcmin, arcsec]}.
    \code{unit="rad"}: \code{lambda}i and \code{phi}i are scalars in radian.\n
    \code{unit="hms"}: code{lambda}i are scalars hour angles (24h = 360deg)
                or arrays in h:m:s format, i.e., \code{[h, m]} or \code{[h, m, s]}.
                The \code{phi}i are nevertheless in degrees as above.
\seealso{greatcircle_distance}
\done

\function{greatcircle_distance}
\synopsis{calculates the angular distance between two points on a sphere in radians}
\usage{Double_Type greatcircle_distance(alpha1, delta1, alpha2, delta2)}
\qualifiers{
\qualifier{unit}{[\code{="deg"}] unit of the input angular coordinates}
\qualifier{alpha1_unit}{[\code{="deg"}] unit of the alpha1}
\qualifier{alpha2_unit}{[\code{="deg"}] unit of the alpha2}
\qualifier{delta1_unit}{[\code{="deg"}] unit of the delta1}
\qualifier{delta2_unit}{[\code{="deg"}] unit of the delta2}
}
\description

    DEPRECATED - please use angular_separation instead
    (\code{alpha}i, \code{delta}i) are the spherical coordinates of point i.\n
    \code{unit="deg"}: \code{alpha}i and \code{delta}i are in degrees.\n
                They can be scalar values or arrays of the form
                \code{[deg, arcmin]}, \code{[deg, arcmin, arcsec]} or \code{[sign, deg, arcmin, arcsec]}.
    \code{unit="rad"}: \code{alpha}i and \code{delta}i are scalars in radian.\n
    \code{unit="hms"}: \code{alpha}i are scalars hour angles (24h = 360deg)
                or arrays in h:m:s format, i.e., \code{[h, m]} or \code{[h, m, s]}.
                The \code{delta}i are in degrees as above.
                The units of each coordinate can be set independently.
    Note that independent of the unit setting, the returned great circle
    distance will always be in radian.
\seealso{greatcircle_coordinates}
\done

\function{GreenwichSiderealTime_from_MJD}
\synopsis{computes the Greenwich sidereal time}
\usage{Double_Type GST = GreenwichSiderealTime_from_MJD(Double_Type MJD);}
\description
    \code{MJD} is the Modified Julian Date (*in UT*).
    This routine is a wrapper around GMST(JD), and only
    kept for compatibility reasons.
    This function is DEPRECATED. Please do not use for future work.
\seealso{GMST}
\done

\function{greiner_hormann}
\synopsis{Clipping and logical intersection of two polygons}
\usage{List_Type polylist= greiner_hormann(src, clp);}
\altusage{List_Type polylist= greiner_hormann(sx,sy,cx,cy);}
\qualifiers{
\qualifier{intersection}{return the intersection of src and clp
    (i.e., clip src against clp), the default}
\qualifier{union}{return the union of src and clp}
\qualifier{without}{remove clp from src}
\qualifier{perturb}{slightly perturb src to reduce probability of
    failure of the algorithm (see description below)}
}
\description
 This function implements the Greiner-Hormann algorithm for polygon
 intersections (Greiner & Hormann, 1998, ACM Trans. Graph. 17(2), 71-83).
 The polygons are given as structures struct {x=[], y=[]}, other structure
 tags are ignored. The function returns a list of closed polygons of this
 type (which may be empty!). The returned polygons also include a tag
 id which is 0 if the point originates in src, 1 if the point originates in
 clp, and -1 if this is a newly inserted intersection point.

 Alternatively, the x- and y-coordinates of the polygon points can be
 given. In this case the return will still be a list of structs.

 The polygons must be closed, i.e., src.x[0]==src.x[-1], src.y[0]=src.y[-1],
 and the same for clp. The polygons can self-intersect, there is almost no
 limitation on their shape. In the case of intersection, the determination 
 that a point is inside a polygon is done using the winding number
 (see help for function point_in_polygon).

 The Greiner-Horman algorithm has an issue for polygons that have colinear
 overlapping sides. If the qualifier "perturb" is set, the coordinates
 in src are randomly perturbed at a level of 1e-8 to reduce the probability
 of this happening.
 
\example

 variable p=xfig_plot_new(15,15);
 p.world(-1.,1.,-1.,1.);

 variable src,clp;

 % a complex polygon
 src=struct {x=[-0.25,0.00,0.25,0.3,0.8,0.5,-0.25],
             y=[ 0.80,-0.40,0.8,-0.4,0.0,0.8,0.80]};

 % a square
 clp=struct {x=[-0.5,0.5,0.5,-0.5,-0.5]+0.2,
              y=[-0.5,-0.5,0.5,0.5,-0.5]+0.2};

 p.plot(src.x,src.y;color="blue",depth=150);
 p.plot(clp.x,clp.y;color="green",depth=150);

 variable res=greiner_hormann(src,clp;intersection);

 variable i;
 _for i(0,length(res)-1,1) {
     p.plot(res[i].x,res[i].y;color="red",depth=50);
 }

 p.render("polygons.pdf");
\seealso{point_in_polygon}
\done

\function{gridmapping}
\synopsis{computes a table of a 2d mapping}
\usage{Struct_Type data = gridmapping(&getxy, X, Xfine, Y, Yfine);}
\done

\function{group_noticed_data}
\synopsis{groups previously noticed spectral bins by an integer factor}
\usage{group_noticed_data(Integer_Type id, Integer_Type factor);}
\seealso{group_data}
\done

\function{group_pha}
\synopsis{Apply quality and grouping information from pha file}
\usage{group_pha(indx)}
\seealso{load_pha}
\done

\function{GTIoverlap}
\synopsis{computes the overlap of an interval [t1, t2] with a set of good time invervals}
\usage{Double_Type GTIopverlap(Double_Type t1, Double_Type t2, Struct_Type GTI}
\seealso{getCommonGTIs}
\done

\function{g_earth}
\synopsis{Calculates the acceleration at the Earth's surface as a function of latitude}
\usage{Double_Type g = g_earth(Double_Type phi);}

\qualifiers{
\qualifier{deg}{phi is in degrees, not in radian.}
\qualifier{altitude}{altitude of the observer above the geoid [m]}
\qualifier{mks}{return g in m/s^2, not in cm/s^2}
\qualifier{allen}{use the equation of Allen (1973, Astrophys. Quantities)}
\qualifier{almanac}{Urban & Seidelman (2013; Expl. Suppl. Astron. Almanac)}
}
\description
    This code calculates the surface acceleration on the earth as
    a function of the latitude (in rad). The equations used are 
    from a compilation of different approximations to g by
    Mangum & Wallace (2015), PASP 127, 74.

    The default returned is for the WGS84 geoid.

    This routine is array safe in phi or altitude.
\seealso{dms2deg}
\done

\function{Hammer_projection}
\synopsis{Computes the Hammer-Aitoff projection}
\usage{(Double_Type x, y) = Hammer_projection(Double_Type l, b);}
\qualifiers{
\qualifier{deg}{\code{l} and \code{b} are in degrees, not in radian}
\qualifier{normalized}{\code{x} and \code{y} are normalized (by \code{sqrt(2)})
                 such that \code{abs(x) <= 2} and \code{abs(y) <= 1}.}
\qualifier{astronomical}{flip x-axis for astronomical maps, where east is
                 to the left}
\qualifier{inverse}{calculate the inverse projection, interpreting l as the x- and
               b as the y-coordinate; all other qualifiers are 
               also interpreted as expected.}
}
\description
 This is the projection erroneously called the Aitoff projection by many
 astronomers. It is an equal area projection. The projection equations are

    \code{x = 2 * sqrt(2) * cos(b) * sin(l/2) / sqrt(1 + cos(b)*cos(l/2));}\n
    \code{y = sqrt(2) * sin(b) / sqrt(1 + cos(b)*cos(l/2));}\n

 The inverse function returns nan if the arguments given are not possible.

\seealso{Aitoff_projection, Lambert_Equal_Area_projection}
\done

\function{hardnessratio}
\synopsis{calculates various X-ray hardness ratio from given count rates}
\usage{Double_Type (HR , HR_err) = hardnessratio(Double_Type soft_count, Double_Type hard_count);}
\altusage{Double_Type (HR , HR_err_up , HR_err_down) = hardnessratio(Double_Type soft_count, Double_Type hard_count ; bayesian);}
\qualifiers{
  \qualifier{color}{calculate the hardness ratio according to color=log10(S/H)}
  \qualifier{hardness}{calculate the hardness ratio according to hardness=(H-S)/(H+S)}
  \qualifier{ratio}{calculate the hardness ratio according to ratio=S/H (the default)}
  \qualifier{bayesian}{returns hardness calculated using the bayesian estimation, calling 
                         the function behr()}
  \qualifier{back_s}{background counts in the soft band}
  \qualifier{back_h}{background counts in the hard band}
  \qualifier{backscale}{ratio between the extraction regions for the source and the background}
  \qualifier{exposure}{Exposure per bin in seconds. If given, interpret soft_count, hard_count and t
                         he backgrounds as rates. Multiply the rates with exposure to get the counts.}
  \qualifier{backexposure}{if given, the background exposure. Only taken into
               account if exposure is also set. If not given, it is assumed that
               the source and background exposures are identical.}
  \qualifier{ratio_type}{(DEPRECATED) Integer_Type, for hr=s/h choose 1,
             for hr=(h-s)/(h+s) choose 2, for hr=log(s/h) choose 3, default = 1}
  \qualifier{err_s}{Array containing the uncertainties of the soft light curve}
  \qualifier{err_h}{Array containing the uncertainties of the hard light curve}
      additional qualifiers are passed to 'behr' for bayesian estimation
}
\description
  This function calculates the so-called hardness ratio or color according to 
  the three common prescriptions used in X-ray astronomy: S/H, (H-S)/(H+S), and log10(S/H).
  If given, background counts are subtracted before the calculation, taking into account
  different source and background extraction regions and different source and background exposure
  times if necessary (if counts are given but the background exposure was different, set the
  source exposure to 1 and the background exposure to the ratio of the source and background exposure
  times. If rates are given, give the exposure times.

  Error bars are calculated using Gaussian error propagation. Contrary to earlier versions of
  this routine they are ALWAYS calculated!

  The bayesian approach or the function behr() should be used in low count regime.
  If they qualifier "bayesian" is present the hardness is calcualted using the bayesian estimation 
  implemented in the function behr(). If the background should be taken into account the function also requires 
  the background scaling factors used in behr().
  Requires gsl, make sure module is loaded!

\seealso{behr;}
\done

\function{hardnessratio_error_prop}
\synopsis{calculates a ratio and error propagation}
\usage{(hr, err) = hardnessratio_error_prop(h, h_err, s, s_err);}
\description
    \code{hr  = (h-s) / (h+s)}\n
    \code{err = sqrt[ (2s/(h+s)^2 * h_err)^2  + (2h/(h+s)^2 * s_err)^2 ]}
\seealso{ratio_error_prop}
\done

\function{hardnessratio_from_dataset}
\synopsis{calculates hardnessratios of the current model}
\usage{Struct_Type hardnessratio_from_dataset(
                  Integer_Type data-id, Integer_Type[2] soft_ch, hard_ch
                );}
\qualifiers{
\qualifier{soft_en}{Double_Type[2]}
\qualifier{hard_en}{Double_Type[2]}
\qualifier{get_counts}{set to '&get_model_counts' if needed, default: get_data_counts}
\qualifier{subtract_background}{subtract background from data. Only possible if get_counts is not set.}
    additional qualifiers are passed to 'hardnessratio'
}
\description
    - at least one dataset has to be defined (load_data)
    - if energy bands are given, channels have to be set to [0,0]
    - returns a structure containing: sc: soft counts, hc: hard counts, ratio: hardnessratio, err: error
\seealso{hardnessratio}
\done

\function{hardnessratio_from_spec}
\synopsis{calculates hardness ratio from given spectrum}
\usage{Struct_Type H = hardnessratio_from_spec(String_Type fits, freeze_model_comp)}
\description
    Calculates hardness ratio from a given spectrum using the model 'enflux'.
    First argument is a fits file with data and model (from fits_save_fit).
    Second argument is the model component of the continuum that has to be frozen (see enflux).
    It first determines the soft and hard energy flux densities of two variable energy bands
    and then derives the hardness ratio plus error.
    For the hardness ratio two different definitions can be chosen (either h/s or
    (h-s)/(h+s)).
\qualifiers{
\qualifier{hard_band}{Double_Type[2], hard energy band, default = [7.,10.]}
\qualifier{soft_band}{Double_Type[2], soft energy band, default = [2.,4.]}
\qualifier{hr_def}{Integer_Type, for hr=h/s choose 1, for hr=(h-s)/(h+s) choose 2, default = 1}
\qualifier{roc}{Integer_Type, RMF OGIP compliance, default = 2}
}
\seealso{hardnessratio_error_prop,enflux,fits_save_fit}
\done

\function{hardnessratio_simulate_grid}
\synopsis{calculates hardnessratios of the current model}
\usage{Struct_Type[] hardnessratio_simulate_grid(String_Type/Integer_Type par1name, par2name,
		     Integer_Type[2] soft_ch1, hard_ch1, soft_ch2, hard_ch2);
 or Struct_Type[] hardnessratio_simulate_grid(
      String_Type/Integer_Type par1name, Double_Type par1min, par1max, step1,
      String_Type/Integer_Type par2name, Double_Type par2min, par2max, step2,
      Integer_Type soft_ch1, hard_ch1, soft_ch2, hard_ch2
    );}
\qualifiers{
\qualifier{dataindex}{Integer_Type, dataset index to use, default = 1}
\qualifier{grid1scale}{0 = linear (default), 1 = logarithmic}
\qualifier{grid2scale}{0 = linear (default), 1 = logarithmic}
\qualifier{par1grid}{Double_Type[], override value grid of paramter 1}
\qualifier{par2grid}{Double_Type[], override value grid of paramter 2}
\qualifier{sample1}{Integer_Type, sampling-factor along each track, default 10}
\qualifier{sample2}{Integer_Type, sampling-factor along each track, default 10}
\qualifier{exposure}{Double_Type, default: 1e4}
\qualifier{arf}{String_Type, file path of arf}
\qualifier{rmf}{String_Type, file path of rmf}
\qualifier{rsp}{String_Type, file path of rsp (arf and rmf combined)}
\qualifier{soft_en1}{Double_Type[2], additional qualifier, soft band for all tracks in one direction}
\qualifier{hard_en1}{Double_Type[2], additional qualifier, hard band for all tracks in one direction}
\qualifier{soft_en2}{Double_Type[2], additional qualifier, soft band for all tracks in other direction}
\qualifier{hard_en2}{Double_Type[2], additional qualifier, hard band for all tracks in other direction}
    additional qualifiers are passed to 'hardnessratio_from_dataset' and 'hardnessratio'
}
\description
    - at least one dataset has to be defined (load_data)
    - a model must exist (fit_fun)
    - soft_ch1, hard_ch1, soft_ch2, hard_ch2 are passed to 'hardnessratio_from_dataset'; if energy-bands
      (soft_en1...)are given, the channels have to be set to [0,0] each.
    - if logarithmic gridscale is chosen, be careful 'parmin' is not <= zero (be aware of the default
      values of your model)
    - the output consists of an array of structures of tracks, where each track contains the values of par1, par2 and the 
     corresponding hardnessratios hr1 and hr2, and the errors hr1err and hr2err e.g. within one struct par1 is kept constant
    - either give arf AND rmf; or ONLY give rsp
\seealso{hardnessratio, hardnessratio_from_dataset, xfigplot_hardnessratio_grid}
\done

\function{hardnessratio_simulate_grid_load}
\synopsis{load tracks saved with hardnessratio_simulate_grid_save}
\usage{Struct_Type = hardnessratio_simulate_grid_save(String_Type filename);}
\seealso{hardnessratio, hardnessratio_from_dataset,
    xfigplot_hardnessratio_grid, xfigplot_hardnessratio_grid_save}
\done

\function{hardnessratio_simulate_grid_save}
\synopsis{saves the output of hardnessratio_simulate_grid into a fits
    file}
\usage{hardnessratio_simulate_grid_save(Struct_Type tracks,
    String_Type filename);}
\description
    - tracks is the output strcutute of hardnessratio_simulate_grid
\seealso{hardnessratio, hardnessratio_from_dataset,
    xfigplot_hardnessratio_grid, xfigplot_hardnessratio_grid_load}
\done

\function{hex2color}
\synopsis{converts hex color values to color string.}
\usage{Integer_Type hex2color(UInt_Type color)}
\seealso{color2hex}
\done

\function{hist1d_confidence}
#c%{{{
\synopsis{Find the confidence interval for histogram}
\usage{(Min,Max) = hist1d_confidence(lo, hi, hist);}
\qualifiers{
\qualifier{conf}{[=0.9] confidence limit (between 0 and 1)}
\qualifier{index}{if given, return the indices of lo and hi bin}
}
\description
    Find the bin boundary values such that they enclose the smallest volume
    equal to \code{conf}. The result is guaranteed to have at least \code{conf}
    integral. Works only for unimodal dsitributions.
\done

\function{hist2d_confidence}
#c%{{{
\done

\function{histogram2d_min_max}
\synopsis{computes a 2d histogram between minimum and maximum data values}
\usage{h2 = histogram2d_min_max(Double_Type Y, X);}
\qualifiers{
\qualifier{xmin}{[\code{=min(X)}]: first value of \code{Xlo}-grid}
\qualifier{xmax}{[\code{=max(X)}]: last value of \code{Xhi}-grid}
\qualifier{ymin}{[\code{=min(Y)}]: first value of \code{Ylo}-grid}
\qualifier{ymax}{[\code{=max(Y)}]: last value of \code{Yhi}-grid}
\qualifier{Nx}{[=50]: number of bins of (linear) \code{Xlo}-grid}
\qualifier{Ny}{[=50]: number of bins of (linear) \code{Ylo}-grid}
\qualifier{Xlo}{reference to a variable to store the \code{Xlo} array}
\qualifier{Ylo}{reference to a variable to store the \code{Ylo} array}
}
\description
    For 2d arrays, the order of the indices matters.
    Almost all ISIS and related functions use \code{h2[iy, ix]}
    with the first index corresponding to y, and the second to x.
    For this reason, \code{histogram2d_min_max} -- just as
    \code{histogram2d} (though not explicitly documented) --
    needs to get the array \code{Y} of y-coordinates as first argument
    and the array \code{X} of x-coordinates only as second argument,
    if the resulting 2d array \code{h2} shall be used with functions
    like \code{plot_image}, \code{png_write}, \code{ds9_view}, etc..

    The x-grid \code{Xlo} starts at \code{xmin}, but ends before xmax,
    such that \code{Xhi = make_hi_grid(Xlo)} would end at \code{xmax}.
    The same is true for the y-grid \code{Ylo} with \code{ymin} and \code{ymax}.

    Unlike \code{histogram2d(Y, X, Ylo, Xlo)}, \code{h2} will not contain
    overflow bins in the last column and the last row, i.e.,
    \code{h2[iy,ix]} corresponds to the number of pairs (\code{X}, \code{Y})
    where \code{Xlo[ix] <= X < Xhi[ix]} and \code{Ylo[iy] <= Y < Xhi[iy]}.
\example
    variable n=10000; x=2*grand(n), y=grand(n), Xlo, Ylo;
    variable h2 = histogram2d_min_max(y, x; Xlo=&Xlo, Ylo=&Ylo);
    plot_image(h2, 0, Xlo+(Xlo[1]-Xlo[0])/2., Ylo+(Ylo[1]-Ylo[0])/2.);
\seealso{histogram2d}
\done

\function{histogram3d}
\synopsis{bins scatter data into a 3d histogram}
\usage{Integer_Type[,,] histogram3d(Double_Type x[], y[], z[], Xgrid[], Ygrid[], Zgrid[])}
\description
    \code{histogram3d} computes the number \code{N[i,j,k]} of points \code{(x[m], y[m], z[m])}
    that fall into the 3d-cell with \code{Xgrid[i] <= x < Xgrid[i+1]},
    \code{Ygrid[j] <= y < Ygrid[j+1]} and \code{Zgrid[k] <= z < Xgrid[k+1]}.
    The last bin in each dimension is an overflow bin,
    such that its upper limit is at infinity.
\seealso{histogram, histogram2d}
\done

\function{histogram_gaussian_probability}
\usage{Double_Type[] histogram_gaussian_probability(Double_Type[] x, sigma, lo[, hi])}
\description
    While \code{histogram(x, lo[, hi])} increases the histogram bin \code{j}
    where \code{lo[j] <= x[i] < hi[j]} by one for every \code{x[i]},
    \code{histogram_gaussian_probability(x, lo[, hi])} adds the
    the Gaussian proability of events with mean \code{x[i]}
    and standard deviation \code{sigma[i]} to all bins.

    Note that this function acts like a convolution
    and therefore introduces an additional broadening.
\example
    variable n=10000, x=grand(n), sigma=ones(n);
    variable lo=[-3:3:0.05],  hi=make_hi_grid(lo);
    hplot(lo, hi, histogram(x, lo, hi));
    ohplot(lo, hi, histogram_gaussian_probability(x, sigma, lo, hi));
    % The first distribution follows N(0, 1),
    % but the second one follows N(0, sqrt(1^2 + sigma^2)) = N(0, sqrt(2)).
\seealso{histogram}
\done

\function{histogram_min_max}
\synopsis{computes a histogram between minimum and maximum value}
\usage{Struct_Type h = histogram_min_max(Double_Type X[, Double_Type dx]);}
\qualifiers{
\qualifier{log}{use a logarithmic grid with the following number of bins:}
\qualifier{N}{[=100] number of bins of logarithmic or linear grid}
}
\description
    The return value is a \code{{ bin_lo, bin_hi, value, err }} structure which
    can directly be used with, e.g., \code{hplot_with_err}, \code{define_counts}, etc.
\seealso{histogram}
\done

\function{history}
\synopsis{shows the history of commands on the interactive ISIS-shell}
\usage{history();}
\seealso{save_input}
\done

\function{hist_fwhm_index}
\usage{(lo,hi) = hist_fwhm_index(hist [, max_index]);}
\synopsis{Compute index bounds spaning FWHM of histogram}
\description
    When given a single array hist_fwhm_index tries to find the FWHM
    range from the maximum. This works much better if the second argument
    is given which has to be the index of the maximum (or at least close to
    it). By interative stepping of the boundaries the function will stop
    when the lo and hi enclose the true fwhm or has hit the array boundaries.

\done

\function{hms2deg}
\synopsis{Convert angle in hour, minute, seconds to degrees or radian}
\usage{degree = hms2deg(h,m,s);}
\qualifiers{
\qualifier{radian}{If set, return angle in radians, not degrees}
}
\description
This is a convenience routine to convert astronomical coordinates given
in hours, minutes, and seconds into a floating point number.

The routine is equivalent to calling dms2deg with the hours qualifier.

This routine is array safe (as long as h, m, s are arrays of equal length).

\seealso{dms2deg}
\done

\function{horizon2equatorial}
\synopsis{convert horizon (azi,ele) coordinates to equatorial (alpha,delta)}
\usage{(alpha,delta)=horizon2equatorial(azi,ele;qualifiers)}
\altusage{Vector_Type eqp=horizon2equatorial(hor;qualifiers)}
\qualifiers{
 \qualifier{JD}{JD for which the calculation is to be performed. Mandatory.}
 \qualifier{lon}{geographic longitude of the observer, positive towards the east. Mandatory.}
 \qualifier{lat}{geographic latitude of the observer, positive towards the north. Mandatory.}
 \qualifier{deg}{interpret all angular arguments in degrees (default is radians!)
                 applies also to the return value.}
 \qualifier{mjd}{interpret date as a MJD (default: JD)}
}

\description
 The function  takes coordinates in the horizon (azimuth,elevation) system
 and converts them into the equatorial system for the ecliptic and
 elevation of the date. Alternatively the routine also accepts a
 Vector_Type and then returns avector in equatorial coordinates.

 See the description of the function equatorial2horizon for
 references and further information. This routine is just a
 wrapper around that function, with the qualifier "inverse" set.

\seealso{Vector_Type, equatorial2horizon,dms2deg,hms2deg}
\done

\function{hplot_filled}
\synopsis{plot a filled histogram defined by slang arrays}
\usage{hplot_filled(Array_Type bin_lo, Array_Type bin_hi, Array_Type values)
}
\qualifiers{
\qualifier{fill_style}{[\code{=1}] set the fill style:\n
                       \code{FS = 1} \code{=>} solid (default)\n
                       \code{FS = 2} \code{=>} outline\n
                       \code{FS = 3} \code{=>} hatched (cannot be used with ylog;)\n
                       \code{FS = 4} \code{=>} cross-hatched (cannot be used with ylog;)\n
}
\qualifier{ticks}{[\code{=""}] set to \code{"I"} or \code{"P"} to invert or project the ticks
                            of both axis (additional to initial plot options)}
\qualifier{xopt}{[\code{=get_plot_options.xopt}] set the plot options of the x-axis directly}
\qualifier{yopt}{[\code{=get_plot_options.yopt}] set the plot options of the y-axis directly}
\qualifier{ymin}{[\code{=min(values)}] set the lower y-value to which the areas are filled}
\qualifier{angle}{[\code{=degrees}] sets the angle of the hatched lines for FS=3 [default = 45] }
}
\description
    This function plots a histogram described by three 1-D S-Lang arrays of size N.
    The area below the histogram is filled. The fill style can be selected with a qualifier.
\examples
    \code{hplot_filled([1:5],[2:6],[1:5]);}\n

    \code{%} invert the ticks\n
    \code{hplot_filled([1:5],[2:6],[1:5]; ticks="I");}\n
        
    \code{%} change the fill style\n
    \code{hplot_filled([1:5],[2:6],[1:5] ; fill_style=4);}\n
\seealso{ohplot_filled, hplot}
\done

\function{hplot_with_err}
\synopsis{plots histogram data points with errorbars}
\description
    This function passes all its arguments and qualifiers to the
    \code{plot_with_err} function, but adds the \code{histogram} and \code{xminmax} qualifiers.
\seealso{[o][h]plot_with_err, [o][h]plot}
\done

\function{hpluv2rgb}
\usage{Int_Type r, g, b = hpluv2rgb(Double_Type h, s, l);}
\synopsis{Calculate RGB triplet from HPLuv space}
\description
  This function returns the RGB colors (as 8 bit values) of
  the corresponding HPLuv color (see https://www.hsluv.org/).
  The inputs range from 0 to 1.

  This colorpsace is useful to generate custom color maps and
  palettes as it attemps to give the same perceived lightness (l)
  and saturation (s) for different hues (h). Compared to HSLuv
  it tries to maximize the used color range.

\seealso{rgb2hpluv, hsluv2rgb}
\done

\function{hsl2rgb}
\synopsis{converts a (hue, saturation, lightness) color to (red, green, blue)}
\usage{Integer_Type rgb = hsl2rgb(Double_Type h, s, l);}
\description
    The hue \code{h} is an angle in color-space. Here, \code{h} is between 0 and 1.
    \code{h} of red is 0, \code{h} of green is 1/3, and \code{h} of blue is 2/3.
    The lightness \code{l} of black is 0, and \code{l} of white is 1.
    Like the hue, the saturation \code{0<=s<=1} matters only for \code{0<l<1}.

    The return value is a(n array of) 24 bit RGB value(s).
\seealso{rgb2hsl}
\done

\function{hsluv2rgb}
\usage{Int_Type r, g, b = hsluv2rgb(Double_Type h, s, l);}
\synopsis{Calculate RGB triplet from HSLuv space}
\description
  This function returns the RGB colors (as 8 bit values) of
  the corresponding HSLuv color (see https://www.hsluv.org/).
  The inputs range from 0 to 1.

  This colorpsace is useful to generate custom color maps and
  palettes as it attemps to give the same perceived lightness (l)
  and saturation (s) for different hues (h). It does not contain
  the full RGB space, however.

\seealso{rgb2hsluv}
\done

\function{hsv2rgb}
\synopsis{Convert HSV color to RGB}
\usage{(Int_Type r,g,b) = hsv2rgb(Double_Type h,s,v);}
\description
   HSV (Hue, Saturation, Value) as doubles (0-1) are
   converted to RGB values (defined as [0-255] 8 bit
   values). Works for array values two (lengths must
   match).
\done

\function{ignore_large_bins}
\synopsis{ignores those bins of a spectral dataset exceeding a maximal size}
\usage{ignore_large_bis(Integer_Type id[], Double_Type maxsize);}
\qualifiers{
\qualifier{verbose}{}
\qualifier{unit}{[=\code{"A"}]: maxsize may be in A or keV}
}
\done

\function{ignore_xy}
\synopsis{ignore points from xy-dataset}
c#%{{{
\usage{ignore_xy (index [, low, high]);}
\description
    Wraper function of the ignore function for datasets defined with
    define_xydata. Ignores data points of dataset \code{index}
    (in the range low to high) for fitting.
\seealso{notice_xy, ignore, ignore_en, define_xydata}
\done

\function{image2rgb}
\synopsis{converts an image (2d array) to a 24bit RGB image}
\usage{Integer_Type rgb[] = image2rgb(Double_Type img[]);
\altusage{Integer_Type rgb[] = image2rgb(Double_Type R[], G[], B[]);}
}
\description
    \code{min(img)} will be mapped to black, \code{max(img)} to white,
    and other values to their linear gray scale.\n
    This function can be used for \code{png_write}(\code{_flipped}).
\seealso{png_write}
\done

\function{init_histo}
\synopsis{initialize a struct with histogram fields}
\usage{init_histo([Integer_Type len]);}
\description
     Return a new struct with the typical fields of a histogram:
     bin_lo, bin_hi, value, err. If the optional argument is present,
     the fields are initialized with a Double_Type array of that
     length. Else the fields are empty. 
\seealso{read_histo, init_histo, add_hist, shift_hist, 
           scale_hist, stretch_hist}
\done

\function{init_jet_fit}
\synopsis{inititialize the fit function for the speed of jet components}
\usage{init_jet_fit (Ref_Type jet_component_structure);}
\qualifiers{
\qualifier{pa}{[=90] position angle of the jet in degrees (required)}
\qualifier{recalc_distance}{set to 1 in order to recalculate the distance
                 of each component based on deltax and deltay}
}
\description
    This function initializes the fit function \code{jet_speed}, which
    can fit a linear function of the form:
        dist (t)  = (t - t_0) * v
    to each component, where t_0 is the ejection time of the component in
    MJD and v is its speed in mas/year. For each epoch an offset (in mas)
    can be fitted, allowing to correct for shifts of the image (along the
    line in which the distance is measured).
    For the initialization a structure with the name "jet_speed_struct"
    is REQUIRED, which has to contain the fields \code{time} (date of epoch in MJD),
    \code{distance} (in mas),  \code{derr} (uncertainty of the distance), and
    \code{component} (integer values identifying the components).
    The function extends this structure, and sets the initialized fit function.
\example
    variable jet_speed_struct = struct {
      mjd       = [55500, 55550, 55600, 55650, 55500, 55600, 55650],
      deltax    = [  0.2,   0.4,   0.5,   0.7,   0.6,   1.2,   1.5],
      deltay    = [  0.0,   0.0,   0.0,   0.0,   0.0,   0.0,   0.0],
      derr      = [ 0.01,  0.05,  0.01,   0.1,  0.02,   0.1,   0.1],
      component = [    1,     1,     1,     1,     2,     2,     2],
      };
    init_jet_fit(&jet_speed_struct; pa = 90);
    ()=fit_counts;
    list_par;
    plot_jet_speed ( jet_speed_struct );
\seealso{plot_jet_speed}
\done

\function{init_plot}
\synopsis{starts a new plot, such that other plots can be overplotted}
\usage{init_plot();}
\description
    plot is used to plot a single point in the lower left corner.
    Before init_plot is called, x- and yrange have to be set.
\seealso{plot, get_plot_options}
\done

\function{init_time_structure}
\synopsis{creates a time structure}
\usage{Struct_Type init_time_structure(Integer_Type Y[, m[, d[, H[, M[, S]]]]])}
\description
    The \code{tm_year} field of the returned structure
    is \code{Y-1900}, and \code{tm_mon} is \code{m-1}.
\seealso{localtime, mktime, strftime}
\done

\function{inner_product}
\synopsis{}
\usage{ M[n,...,j] = inner_product( A[n,...,m], B[m,...,j] );}
\description
    In ISIS the operator # calculates the inner product of two arrays
    A, B, i.e., contracts the last and first dimension, respectively.
    A and B can have an arbitrary number of dimensions, it is just required
    that the last dimension of A and first dimension of B is of the same
    size!
\example
    A = _reshape( [1:2*3], [2,3] );
    B = _reshape( [1:3*4], [3,4] );
    M = inner_product( A, B );
    N = A # B;
    vmessage("M\n"); print(M);
    vmessage("N\n"); print(N);
\seealso{operator #}
\done

\function{inner_products}
\synopsis{}
\usage{ M[n,...,l] = inner_products( A[n,...,m], B[m,...,j] [, ..., Z[k,...,l] ] );}
\description
    In ISIS the operator # calculates the inner product of two or more arrays.
    This function is basically nesting the inner_product function!
\seealso{inner_product, operator #}
\done

\function{integral_cat2ds9}
\synopsis{a cat2ds9 like function for INTEGRAL catalogs}
\usage{ ()=integral_cat2ds9(String_Type inputfile,
                      String_Type outputfile)}
\qualifiers{                     
\qualifier{source_name}{name of the column with source names
                        (default: NAME)}
\qualifier{ra_name}{name of the column with right ascension of the
                      sources (default: RA_OBJ)}
\qualifier{dec_name}{:name of the column with declination of the
                      sources (default: DEC_OBJ)}
\qualifier{color}{color to be used for the ds9 region (default:
                    black)}
\qualifier{symbol}{form of the ds9 region (default:box)}
\qualifier{noname}{only boxes, no text with source names}
}
\done

\function{integrate2d}
\synopsis{numerical computation of an 2-dim. integral}
\usage{Double_Type I = integrate2d( Ref_Type func, y1, y2,
                                    Double_Type x1, x2
                                  );
}
\qualifiers{
\qualifier{intfun[&qromb]:}{Reference to the integrator}
\qualifier{NOTE}{Qualifiers are passed to all sub-functions!}
}
\description
    This function numerically computes the 2-dimensional integral 'I' over the
    integrant-function 'func' of the form:

       I = int_x1^x2 int_y1(x)^y2(x) func(x,y) dx dy

   The given limits 'x1' and 'x2' are the lower and upper integration limits
   for the 'x' variable, respectively. 'y1' and 'y2', on the other hand, are
   references to functions, which calculates the lower and upper limit y1(x)
   and y2(x) for the 'y' variable depending on 'x'.

   The implemented solution is adopted from the 'Numerical Recipes in C'
   Chap. 4.6. !
   
   The integrator can be changed by the qualifier 'intfun', which is set to
   'qromb' as default (see 'help qromb). Qualifiers given to 'integrate2d'
   are passed to the 'intfun' function!

\example
   % Calculation of the area of an rectangle:
   %    I = int_0^1 dx int_0^2 dy
   %      = [ x ]_0^1 * [ y ]_0^2
   %      = 2
   
   variable x1 = 0., x2 = 1.;
   define y1(x){ return 0.; };
   define y2(x){ return 2.; };
   define func( x, y ){ return 1.; };
   variable I = integrate2d( &func, &y1, &y2, x1, x2 );
   vmessage("Area of rectangle is A = %g",I);

   % Calculation of the area of the unit circle in cartesian coordinates:
   %   I = int_-1^1 int_-sqrt(1-sqr(x))^sqrt(1-sqr(x)) dx dy
   %     = int_-1^1 2 * sqrt(1-sqr(x)) dx
   
   define tfun( x, y ){ return 1; }
   define y1(x){ return -sqrt(1-sqr(x)); }
   define y2(x){ return  sqrt(1-sqr(x)); }
   variable I = integrate2d( &tfun, &y1, &y2, -1., 1. ; qromb_max_iter=24, qromb_eps=1e-3 );
   vmessage("Area of unite circle is A = %g, abs(A-PI)/PI = %g", I, abs(I-PI)/PI );
   
%\seealso{qromb, qsimp, integrate2d_test}
\done

\function{integrateAB5}
\synopsis{integrates an ordinary differential equation with the 5th order Adams-Bashforth algorithm}
\usage{x = integrateAB5(&f, t1, t2, dt[, x0]);}
\description
    The integral of the ordinary differential equation\n
      dx/dt = f(x(t), t)  with  x(t0) = x0\n
    reads\n
      x(t) = x0 + int_{t0}^{t} f(x(t'), t') dt'

    \code{&f} is a reference to a function with two arguments:
       \code{define f(x, t)}\n
       \code{\{}\n
       \code{  return ...;}\n
       \code{\}}

    \code{x} may be be a scalar as well as an array.
\seealso{integrateRK4}
\done

\function{integrateGK}
\synopsis{Integrate function with Gauss Kronrod method (G7K15)}
\usage{Double_Type integrateGK(Ref_Typ fun, Double_Type a, b, ...);}
\qualifiers{
  \qualifier{max_intervals}{[=5]: Maximum number of intervals to use}
  \qualifier{quiet}{: If given, do not message about non-convergence}
  \qualifier{tolerance}{[=1e-8]: Convergence tolerance}
  \qualifier{recursive}{If given, integration is done recursively. max_intervals does not apply.}
  \qualifier{qualifier}{[=NULL]: qualifier structure. Will be passed to function.}
}
\description
  Calculate the integral of a function \code{fun} from \code{a}
  to \code{b}. It is expected that the first argument of \code{fun}
  is the integration parameter. Additional arguments can be passed
  after the integration boundaries. The second return value is the
  estimated error from the integral value.

\example
  (val,err) = integrateGK(&sin, 0, PI);
\seealso{integrateRK4}
\done

\function{integrateRK4}
\synopsis{integrates an ordinary differential equation with the 4th order Runge-Kutta algorithm}
\usage{x = integrateRK4(&f, t1, t2, dt[, x0]);}
\description
    The integral of the ordinary differential equation\n
      dx/dt = f(x(t), t)  with  x(t0) = x0\n
    reads\n
      x(t) = x0 + int_{t0}^{t} f(x(t'), t') dt'

    \code{&f} is a reference to a function with two arguments:
       \code{define f(x, t)}\n
       \code{\{}\n
       \code{  return ...;}\n
       \code{\}}

    \code{x} may be be a scalar as well as an array.
\seealso{integrateAB5}
\done

\function{integrateRK45_adaptive}
\synopsis{Integrate an ODE with an adaptive 4th/5th order Runge-Kutta algorithm}
\usage{(t, x(t)) = integrateRK45_adaptive(&f, t0, t, [, x0]);}
\description
    Given an ordinary differential equation of the form\n
      dx/dt = f(t, x(t))
    (with \code{x} either a scalar or an array)\n
    this function numerically computes the solution\n
      x(t) = int_{t0}^{t} f(t', x(t')) dt' [ + x0 ]\n
    by means of a 4th/5th order Runge-Kutta (RK) algorithm.

    \code{&f} is a reference to a function with two arguments:\n
       \code{define f(t, x)}\n
       \code{\{}\n
       \code{  return ...;}\n
       \code{\}}

    If x is an array, the adaptive stepsize is chosen according\n
    to the needs of the "worst-offender" equation implying that\n
    the desired accuracy is reached by each individual component.

    Note: All qualifiers are also passed to the function f.
\qualifiers{
\qualifier{eps}{[\code{=1e-12}] absolut error control tolerance; lower limit: 1e-15}
\qualifier{method}{[\code{="RKCK"}] choose among three different RK45 methods:
      "RKF": RK-Fehlberg, "RKCK": RK-Cash-Karp, "RKDP": RK-Dormand-Prince}
\qualifier{path}{return the entire path and not only the final result}
\qualifier{verbose}{show intermediate "times" \code{t}}
\qualifier{plus all qualifiers of the function f}{}
}
\example
    % d/dt x(t) = -sin (t^2)*2t; -> analytical solution: x(t) = cos(t^2) for x(0)=1
    define f(t,x)
    {
      return -sin(t^2)*2*t;
    };
    (t, x) = integrateRK45_adaptive(&f, 0, 10, 1; path);
    plot(t,x-cos(t^2));

    % d/dt [x1(t),x2(t)] = [x2(t),-x1(t)]; -> analytical solution: x1(t) = cos(t) for x1(0)=1, x2(0)=0
    define f(t,x)
    {
      [x[1],-x[0]];
    }
    (t, x) = integrateRK45_adaptive(&f, 0, PI, [1,0]; path);
    plot(t,x[*,0]-cos(t));
    
\seealso{integrateRKF45, integrateRK4, integrateAB5}
\done

\function{integrateRKF45}
\synopsis{integrates an ODE with the adaptive 4th/5th order Runge-Kutta-Fehlberg algorithm}
\usage{x = integrateRKF45(&f, t1, t2, dt[, x0]);}
\qualifiers{
\qualifier{eps}{[\code{=1e-12}] error control tolerance}
\qualifier{verbose}{show intermediate "times" \code{t}}
}
\description
    This implementation of the adaptive Runge-Kutta-Fehlberg algorithm
    is still considered EXPERIMENTAL and MAY REQUIRE FURTHER TESTING!

    The integral of the ordinary differential equation\n
      dx/dt = f(x(t), t)  with  x(t0) = x0\n
    reads\n
      x(t) = x0 + int_{t0}^{t} f(x(t'), t') dt'

    \code{&f} is a reference to a function with two arguments:
       \code{define f(x, t)}\n
       \code{\{}\n
       \code{  return ...;}\n
       \code{\}}

    \code{x} may be be a scalar as well as an array.
\seealso{integrateRK4, integrateAB5}
\done

\function{integrate_trapez}
\synopsis{numerical integration with the trapezoidal rule}
\usage{Double_Type int = integrate_trapez(Double_Type x[], Double_Type y[]);
    alternative:
    Double_Type int = integrate_trapez(Ref_Type function, Double_Type min, Double_Type max);}
\description
    If two arguments (\code{x[]} and \code{y[] = function(x[])}) are given the integral is calculated via:
    \code{int = sum_i (y[i-1] + y[i])/2 * (x[i]-x[i-1])}
    An alternative usage with three arguments requires a reference to
    the function and the integration limits. The function is evaluated
    on an equidistant grid. The number of steps can be set with a qualifier.
    The second case allows a forth argument specifying a previous result
    with a smaller number of steps, which is meant for iterative usage by
    the function \code{qsimp}.
\qualifiers{
\qualifier{steps [=100]}{:    number of equidistant grid points,
                       on which the function is evaluated}
}
\example
    % Integration of the sin function from 0 to PI/3
    % The result should be 0.5
    variable x = [0:PI/3:#100];

    % The first method: providing grid and corresponding function values
    integrate_trapez (x, sin(x));

    % Alternative method using reference to the function and integration limits
    integrate_trapez (&sin, 0, PI/3);
    integrate_trapez (&sin, 0, PI/3; steps = 5000);    
\seealso{integrateRK4, qsimp}
\done

\function{interpol2D}
\synopsis{}
\usage{img2 = interpol2D(X2, Y2,  X, Y, img);}
\description
    \code{img = Double_Type[ny, nx];}\n
    \code{X  = Double_Type[nx];}\n
    \code{Y  = Double_Type[ny];}\n
    \code{min(X) <= X2 <= max(X)}\n
    \code{min(Y) <= Y2 <= max(Y)}\n
\done

\function{interpol_2d_table}
\usage{Double_Type interpol_2d_table(Double_Type x, y, Struct_Type table)}
\done

\function{interpol_polynomial}
\synopsis{interpolates a polynomial function between data points}
\usage{Double_Type y = interpol_polynomial(Double_Type x, Double_Type X[], Double_Type Y[]);}
\description
    \code{X} and \code{Y} are arrays of same length \code{n}.
    \code{y} is the Lagrange polynomial (of degree \code{n-1}) interpolating
    the data points (\code{X[i]}, \code{Y[i]}), evaluated at \code{x}:\n
      \code{     n-1           n-1      x - X[j]  }\n
      \code{y =  sum  Y[i] * product  ----------- }\n
      \code{     i=0        j=0,j!=i   X[i]-X[j]  }

    If \code{x} is an array (which doesn't need to be ordered), \code{y} will also be an array.
\seealso{interpol_points}
\done

\function{inversemapping}
\synopsis{computes the inverse of a 2d mapping numerically}
\usage{(x, y) = inversemapping(&getxy, cache, x_, y_);}
\description
   (cache.x_, cache.y_) = getxy(cache.x, cache.y);
   (x_, y_) = getxy(x, y);
\done

\function{ionabs (fit-function)}
\synopsis{multiplicative fit-function for ionized photoabsorption}
\description
    Currently, the cross section is only evaluated at the center of each bin!

    The first fit parameter, \code{ionabs.N_H}, is a dummy parameter,
    which is not taken into account for the model. One can, however, use it
    to relate the ionized column densities to an equivalent hydrogen column density.
\seealso{ionized_phabs_sigma}
\done

\function{ionized_phabs_sigma}
\synopsis{calculates the photoabsorption cross section for ionized material}
\usage{Double_Type sigma = ionized_phabs_sigma(Double_Type E);}
\description
    \code{E} is the energy in keV (may be a scalar or an array),
    \code{sigma} is the corresponding cross section in cm^2.
    The ion of interest is specified by the following
\qualifiers{
\qualifier{Z}{atomic number (1 <= \code{Z} <= 30)}
\qualifier{Nel}{number of electrons (\code{Z} >= \code{Nel} > 0)}
\qualifier{n}{principal quantum number for the selection of a subshell (together with l)}
\qualifier{l}{orbital angular momentum quantum number for the selection of a subshell}
}
\seealso{Verner & Yakovlev (1995)}
\done

\function{JD2MJD}
\synopsis{Convert Julian Dates to Modified Julian Dates.}
\usage{JD2MJD(JD);}
\description
    Helper function to convert Julian Dates to MJD by
    subtracting off 2400000.5.
\seealso{j2dmjd, mjd2jd}
\done

\function{jd2mjd}
\synopsis{Convert Julian Dates to Modified Julian Dates.}
\usage{jd2mjd(JD);}
\description
    Helper function to convert Julian Dates to MJD by
    subtracting off 2400000.5.
\seealso{mjd2jd}
\done

\function{jd2year}
\synopsis{transform a JD into a fractional year}
\usage{Double_Type year = jd2year(Double_Type JD);}
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
}
\description
  NOTE: This routine takes into account leap years. This is good for
  plotting, but this means that there are slight phase shifts that could be
  problems when doing time series analysis. For such purposes best use
  EpochofJD!
\seealso{EpochofJD}
\done

\function{JDofDate}
\synopsis{Calculates the (modified) Julian Date}
\usage{Double_Type JD = JDofDate( struct { year, month, day, hour, minute, second } );
\altusage{Double_Type JD = JDofDate(year, month, day[, hour[, minute[, second]]]);}
}
\qualifiers{
\qualifier{mjd}{return the MJD, i.e., JD-2400000.5}
\qualifier{julian_calendar}{return the JD/MJD assuming the
       Julian calendar rather than the Gegorian one}
}
\description
    This routine calculates the Julian Date for a given Gregorian date
    following the routine by Fliegel and van Flandern (1968, Comm. ACM 11, 657)
    and further optimized as discussed by Pulkkinen and van Flandern (1979,
    ApJ Suppl 41: 391). With the julian_calendar qualifier the date is
    calculated for the Julian calendar (should be done for dates before
    15 October 1582 in Germany, see
    https://en.wikipedia.org/wiki/Gregorian_calendar
    for a rough table and
    https://de.wikipedia.org/wiki/Gregorianischer_Kalender
    for a table containing the exact dates for the switch over).

    The date is either given by a structure or by at least 3 integer arguments.
    Fractional parts for the year and month are ignored. The default values for
    hour, minute and second are 0, fractional days, hours, etc. are properly
    taken into account.

    The year must be larger than -4716 (4717 BC).
    This function is array safe.

\seealso{MJDofDate,dateOfMJD}
\done

\function{JDofEpoch}
\synopsis{Returns the (M)JD of a Julian or Besselian epoch such as J2000.0}
\usage{jd=JDofEpoch(epochstring;qualifiers)}
\qualifiers{
\qualifier{mjd}{Return the modified julian date (default: JD)}
}
\description
   This function returns the julian date corresponding to a
   Julian or Besselian epoch (e.g., J2000, B1950.0 etc.). For highest
   precision, the most commonly used values are hardcoded, other values
   are calculated using the formulae given by Lieske (1979, A&A 73, 282)
   for Besselian dates and the standard definition of the Julian epoch
   (based on Julian years with 365.25 days).

   The epoch string must start with a "B" for a Besselian epoch and
   "J" for a Julian epoch, followed by a number that designates the year
   fractional values such as 2017.25 are possible.

   Note that the time system in which the dates are defined is TT, for most
   applications that do not require microsecond accuracy for the conversion
   of positions the precise time system won't matter.

   If the routine is called with a floating point number, it is assumed
   that this number is the epoch and this number is returned. This greatly
   simplifies functions in which the epoch is either a string or
   a number.

\seealso{EpochofJD,tt2tai,tt2tdb}
\done

\function{jpl_bineph2fits}
\synopsis{convert a binary JPL ephemeris to FITS}
\usage{jpl_bineph2fits(ephfile,fitsfile);}
\description
 This function reads the JPL binary ephemeris in ephfile
 and writes it to the fitsfile.

 The JPL ephemeris must be in the "old-style" format.
 SPK files are NOT supported.

 The function has to be run on a machine with the same byte
 sex as the ephemeris (JPL typically distributes ephemerides
 for all common types of byte sex). It does not do a lot of
 checking, however, it has been used to successfully
 convert ALL available JPL old-style ephemerides into FITS.

 Which also means you should probably ask Joern Wilms for
 the location of the relevant FITS file rather than using this
 function...

\seealso{jpl_eph,jpl_eph_vec}
\done

\function{jpl_eph}
\synopsis{calculate planet positions using the JPL ephemeris}
\usage{Struct_Type results = jpl_eph(jd,ta,ce);}
\description
    This function returns the ephemeris  of target body ta
    as seen from center body ce for the time jd (the JD in the
    TDB system). 

    NOTE: To calculate apparent planet positions for observing,
    use the higher level function planetpos.

    NOTE2: If you want a routine that is compatible with
    function vsop87, uses TT and is vector safe, use  jpl_eph_vec

    jd is either a Double or a 2-element array (to avoid roundoff
    errors the array can include, e.g., the integer and fractional
    parts of the JD). It is given in in TDB. If you need to calculate
    positions for a time TT (=TAI+32.184s), convert with function
    tt2tdb.
    Note, however, that for all practical applications except for ms
    pulsar timing, you can assume that TT=TDB. 

    The target and center bodies ta and ce are identified as outlined
    below.

    By default, the routine initializes to DE430 as distributed with HEASOFT
    (and found either in  $LHEA_DATA/JPLEPH.430 or in
    $LHEASOFT/refdata/JPLEPH.430).
    The ephemeris in HEASOFT is only a subset of DE430, valid for about 1950--2050.
    For these ephemerides, the precision for the decades around AD2000 is on
    the order of meters to tens of meters, and milliarcseconds (see below). 
    Joern provides full files for DE430, valid for the time between
    AD1550 and AD2650, and for DE431, which covers years -13200 to +17191
    (with lower precision for the moon).

    Use the ephemeris qualifier if you want to use another ephemeris.

    For planets, the function returns an array containing the xyz-position,
    and the velocity components in the ICRF. For angle variables
    (only contained in a few of the JPL ephemerides), the array elements are
    the angles and their  derivatives, and for the difference TT-TDB, the time
    difference and its derivative. Units are km and km/s, rad and deg/day [!],
    or s and s/day [!]. 

    Target bodies or angles are either identified by a number as listed
    below or by the indicated case-insensitive string. Note that not all
    ephemeris files contain all of the possibilities!

     1 - Mercury
     2 - Venus 
     3 - Earth
     4 - Mars
     5 - Jupiter
     6 - Saturn
     7 - Uranus
     8 - Neptune
     9 - Pluto
    10 - Moon
    11 - Sun
    12 - Solar System Barycenter
    13 - Earth-Moon Barycenter
    14 - 1980 IAU nutation angles
    15 - Lunar libration (Euler) angles
    16 - Lunar angular velocity
    17 - TT-TDB

    where 3 is the Earth's geocenter. For planets with moons, the 
    position returned is with respect to the barycenter of the respective
    planetary system.
 
    TT-TDB is measured at the geocenter.

    The following abbreviations are also allowed:
    12: SSB
    13: Earth-Moon or EMV
    14: IAU1980
    15: Lunar Libration

 For targets 14-17 the center is ignored
 
 See Folkner et al., 2014, IPN Progress Report 42-196 for a description
 of the precision that is reached with DE430/DE431. The figures in
 this publication indicate that for the timeframe from about 1970 onwards,
 where precise astrometry and space probe data exist, a precision
 on the order of several 10s of meters is reached (and apparent
 angular positions are precise to milli-arcseconds).

 This function is NOT array safe, and it CANNOT be used in
 a parallel_map or other parallelized functions. This is
 because global caches are used that we cannot avoid if we
 want decent performance for single calls. Sorry

 \qualifiers{
    \qualifier{ephemeris}{structure defining the ephemeris, initialized with
                            jpl_initeph (see there).}
 }
 
\seealso{jpl_eph_vec,jpl_initeph, planetpos}
\done

\function{jpl_eph_vec}
\synopsis{high precision planetary positions from the JPL ephemerides, vsop87 compatible}
\usage{Struct_Type jpl_eph_vec(JD,planet;qualifiers)}
\description
 This routine provides an interface to jpl_eph which allows to
 calculate heliocentric ephemerides from the JPL ephemerides.
 It is compatible with a subset of the functions that are also
 available from function vsop87.

 Use planetpos to calculate apparent planetary positions.

 The routine returns high precision planetary positions for
 all planets for a given (array) of times (in TT).

 Depending on the qualifiers the routine can return rectangular
 coordinates (X,Y,Z) or ecliptical coordinates
 (longitude, latitude, distance) in a heliocentric coordinate system
 for epoch J2000 (the ICRF).

 Contrary to vsop87, orbital elements cannot be calculated. 

 By default the function returns rectangular heliocentric
 coordinates in a structure with elements x, y, z, vx, vy, vz.
 Here x, y, z are in an ecliptical(!) coordinate system for
 the epoch J2000 (but see the equatorial qualifier), where the x-axis
 is in the direction of the vernal equinox, the z-axis points
 to the ecliptical North pole, and the y-axis forms
 a right handed coordinate system with the x- and z-axes.
 The default units are AU and AU/day (but see the mks and cgs qualifiers).

 For heliocentric coordinates the planet can have the values
 "Mercury", "Venus", "Earth", "Mars", "Jupiter", "Saturn",
 "Uranus", "Neptune", and "Pluto" (the latter is not included
 in vsop87!). The value "Earth-Moon" will return the position
 of the Earth-Moon barycenter.

 The default unit of time is the Julian Date in TT, to be
 compatible with vsop87. Use the qualifier tdb to avoid the
 conversion TT->TDB, which for pretty much all practical
 applications except for ms pulsar timing results in no
 appreciable difference in the results but is slightly faster.

 Use the mjd qualifier to use MJD rather than JD as input.

 The default function uses DE430 from HEASOFT, which is valid
 for the time between 1950 and 2050. Joern can provide ephemeris
 files for the full DE430, valid AD1550 and AD2650, and
 also for DE431, which covers years -13200 to +17191
 (with lower precision for the moon). See the ephemeris qualifier.

 When using JPL ephemerides, do NOT use this function in
 a parallel_map or other parallelized computations. This is
 because global caches are used that we cannot avoid if we
 want decent performance for single calls. Sorry.
  
\qualifiers{
 \qualifier{tdb}{The time argument is in TDB, not TT}
 \qualifier{mjd}{The time argument is in MJD, not in JD}
 \qualifier{barycentric}{Return coordinates with respect to the solar system barycenter rather
                    than heliocentric coordinates.}
 \qualifier{spherical}{Return spherical coordinates in the ecliptical coordinate system.
                       In this case the structure returned contains the elements
                       lat, lon, r, latdot, londot, rdot, where lat and lon are the ecliptical
                       latitude and longitude (in rad), r is the distance in AU, and
                       latdot, londot, rdot are their time derivatives (in rad/day or
                       AU/day, respectively.)}
 \qualifier{equatorial}{return coordinates in the ICRS2 coordinate system, not the
                       equatorial one (vsop87 does not have this option)}
 \qualifier{mean_equinox}{Coordinates are given in the frame defined by the
                   mean equinox and ecliptic of the date.
                   The default is to return J2000 coordinates.}
 \qualifier{deg}{All angle quantities to be returned are in deg, or deg/day.}
 \qualifier{mks}{All distances returned by the function are given in meters or m/s, depending
                 on the deg qualifier, angles are returned in rad and rad/s or deg and deg/s.}
 \qualifier{cgs}{Dito, but use centimeters instead of meters.}
 \qualifier{ephemeris}{Ephemeris structure initialized with jpl_initeph.}
 \qualifier{forcearray}{Force the tags in the returned struct to be arrays.
                          The default is that they are arrays only if JD
                          is an array.}
}
\seealso{vsop87,jpl_initeph,jpl_eph}
\done

\function{jpl_initeph}
\synopsis{initialize a JPL ephemeris}
\usage{Struct_Type results = jpl_initeph(file);}
\description
 This function initializes a JPL ephemeris for later use with
 function jpl_eph.

 For an empty argument, the function defaults to DE430 as distributed
 with HEASOFT. This is probably what you want (and since jpl_eph
 initializes to that ephemeris, this means that you probably do NOT
 want to call this function in the first place).

 The ephemeris file must be in FITS format, using the HEASOFT
 convention (see function jpl_bineph2fits if you need to generate
 such a file from one of the older JPL ephemeris distributions).

 The function first searches for the ephemeris file as given,
 and then searches for the file by prepending ${LHEA_DATA}/
 or ${LHEASOFT}/refdata/. 

\qualifiers{
  \qualifier{nocache}{Do not cache the ephemeris data in memory,
                        i.e., keep the FITS file open and seek on it.
                        This is faster if calculating just a few positions
                        (e.g., for a few weeks), otherwise, reading the
                        whole ephemeris first [the default] is faster,
                        but a memory hog, i.e., it won't work for DE431.}
}
\seealso{jpl_eph,jpl_bineph2fits}

\done

\function{j_ff}
\synopsis{Calculates the emission coefficient for free-free emission (bremsstrahlung)}
\usage{Double_Type j = j_ff(nu,T);}

\qualifiers{
\qualifier{Z}{average nuclear charge (default=1)}
\qualifier{ne}{electron particle density (cm^-3; default: 1e10)}
\qualifier{np}{proton particle density (cm^-3; default: 1e10)}
}
\description
    This function returns the emission coefficient for free-free radiation
    (bremsstrahlung). The frequency, nu, is measured in Hz and can be an array,
    the temperature T is measured in K.

    Note: per Kirchhoff's law the total bremsstrahlung spectrum from
    a slab of size R is
    B_nu(nu,T)*(1.-exp(-R*alpha_ff(nu,T)))

\seealso{alpha_ff}
\done

\function{kaastra_cstat_goodness}
\synopsis{computes the theoretical Cash-statistics for a Poisson process}
\usage{(Array_Type ce, Array_Type cn) = kaastra_cstat_goodness(Array_Type mu);}
\description
 Given an array of predicted counts, mu, this function uses the
 expressions given by Kaastra (2017, A&A 605, A51) to compute the expected
 value of the contribution to the C-statistic for this bin, ce, and its
 variance, cn.

 This can be used to evaluate the expected value of the C-statistic of
 a fit (see function cstat_goodness()), or to show the expected model
 contribution in a plot of the cash statistics residuals.

 The function uses Kaastra's approximation to the exact formulae (his
 Eqs. 8-22), which are better than a few times 1e-4. Use the exact
 qualifier if you cannot live with this (it is unclear why you would
 want to do this).
 
\qualifiers{
\qualifier{exact}{use Kaastra's exact equations 4-6 (slow; use only
                    for testing, not necessary for practical work)}
}
 
\seealso{cstat_goodness}
\done

\function{kendall_tau_censored}
\synopsis{calculation of Kendall rank correlation coefficient}
\usage{Struct_Type cc = kendall_tau(Double_Type x[], Double_Type y[]);}
\altusage{Struct_Type cc = kendall_tau(Double_Type x[], Double_Type y[], Char_Type y_is_upper_limit[]);}
\altusage{Struct_Type cc = kendall_tau(Double_Type x[], Char_Type x_is_upper_limit[], Double_Type y[], Char_Type y_is_upper_limit[]);}
\description
    This function calculates the Kendall rank correlation coefficient \code{tau},
    as defined by M. G. Kendall (1938, Biometrika, 30, 81), between two
    arrays of equal length.
    The result \code{tau} is within the interval [-1,1], where 1 means a perfect
    agreement between both rankings, -1 means perfect disagreement, and values
    around 0 indicate that both arrays are independent.
    The value of \code{tau} is the difference of concordant and discordant pairs
    divided by the total number of pairs.
    In the definition of Helser (Dennis R. Helsel, "Nondetects and Data Analysis",
    Wiley, 2005) only the number of determined (valid) pairs is used instead of
    the total number. The structure returned by this function includes the values
    of \code{tau}, \code{sigma}, and \code{pval} in both ways.

    The value \code{sigma} characterizes the expected width of the distribution of \code{tau}
    for random data. (If the "gsl" module is not available, the p-value is not calculated.
    It can be obtained from \code{tau} and \code{sigma}.)

    See statistic module for further functions: require("stats");
\examples
    variable x = [1:2:#100];
    variable y = grand(100)+x;
    variable cc_no_limits   = kendall_tau_censored(x,y);
    variable cc_incl_limits = kendall_tau_censored(x,y,nint(urand(100)));
\seealso{kendall_tau, pearson_r, spearman_r, spearmanrho, correlation_coefficient}
\done

\function{KeplerEquation}
\synopsis{solves Kepler's Equation  E - e sin E  =  M }
\usage{Double_Type E = KeplerEquation(Double_Type M, e)}
\description
    Solve Kepler's Equation by using the result of the method
    by S. Mikkola (1987) Celestial Mechanics, 40, 329-334
    as starting value for a Newton-Raphson iteration
    to extend the applicability of this function to higher eccentricities.
\qualifiers{
\qualifier{threshold}{stopping criterion for the Newton Raphson iteration, default = 1e-5}
}
\done

\function{kerr_lp_energyshift_observer}
\synopsis{calculates the energyshift from a Lamp Post source (point-like)
 on the rotational axis of a spining black hole (spin a) to the
 observer. It follows the relxill definition that if the height is in
 negative units, it is interpreted as given in units of the event
 horizon. }
\usage{kerr_lp_energyshift_observer(height, a)}
\description
   g = E_obs / E_emit =  sqrt( 1 - 2*h/(h^2 + a^2)
\done

\function{kerr_lp_redshift}
\synopsis{calculates the redshift from a Lamp Post source (point-like)
 on the rotational axis of a spining black hole (spin a) to the  observer}
\usage{kerr_lp_redshift(height, a)}
\description
   z_obs = 1 / ( sqrt( 1 - 2*h/(h^2 + a^2) )  - 1 
\done

\function{kerr_rms}
\synopsis{calculates the radius of marginal stability of a black hole
    in units of GM/c^2}
\usage{kerr_rms(a)}
\done

\function{kerr_rplus}
\synopsis{calculates the event horizon of a black hole in units of GM/c^2}
\usage{kerr_rplus(a)}
\done

\function{keV2erg}
\synopsis{convert flux to erg}
\usage{Double_Type new_value = kev2erg(Double_Type old_value)}
\qualifiers{
 \qualifier{y_fac}{: divide the value by 10^{y_fac} }
}
\seealso{plot_unfold}
\done

\function{keyinput}
\synopsis{reads input from the keyboard}
\usage{String_Type = keyinput([String_Type message]);}
\qualifiers{
    \qualifier{silent}{characters are not echoed, e.g. useful for password input}
    \qualifier{time}{read timout after given seconds. Returns an empty string}
    \qualifier{nchr}{returns automatically after given number
              of chars have been read}
    \qualifier{prompt}{output the given string before reading}
    \qualifier{default}{initial text to be read}
}
\description
    Prompts the user to input a line, which is ended
    with return. If a limited number of chars is given
    using the `nchr' qualifier, the line is automatically
    ended after reaching this number without pressing return. 
    It is also possbile to limit the time for an input with the 
    `time' qualifier and to disable echoing the input using 
    the `silent' qualifier. The latter is useful, e.g., for password 
    inputs or for single key events.
    If `nchr==1', the returned keycode is
    checked against special keys such as the arrow keys.
    If one of these is detected, the returned string contains
    the name if the pressed key.
\done

\function{keynote_size}
\synopsis{Set the pgplot output size to something suitable for Keynoe presentations (isis_fancy_plots package)}
\usage{apj_size;}
\description

   Use as:
   isis> id = open_print("fig1.ps/vcps"); keynote_size; nice_width;
   isis> plot(x,y);
   isis> close_print(id,"gv");
\seealso{sov, apj_size, nice_width, open_print, close_print, pg_color, pg_info}
\done

\function{KS_test}
\synopsis{computes the test statistics of a two sample Kolmogorov-Smirnov test}
\usage{test_statistics = KS_test(Double_Type x1, Double_Type x2);}
\description
 The null hypothesis of the KS test is that two samples are
 distributed according to the same distribution. It is rejected if
 the test statistic D=max(F_1(x),F_2(x)) is greater than a certain
 value
\done

\function{lagrange_poly}
\synopsis{Interpolate points with Lagrange polynomial}
\usage{Double_Type[] lagrange_poly(Double_Type[] x, x0, y0);}
\qualifiers{
  \qualifier{w}{calculated weights can be passed directly}
}
\description
  Calculate the Lagrange interpolation at points \code{x} given
  points \code{x0},\code{y0}.

  If multiple invocations of the interpolation for the same pair
  \code{x0},\code{y0} are required it is better to calculate the
  weights beforehand (see \code{lagrange_weights}) and pass them
  via the \code{w} qualifier.

  The Lagrange interpolation is quite powerfull, however, as any
  polynomial interpolation scheme it suffers from Runges phenomenon.
  It can be shown that the interpolation of a function on a uniform
  grid is close to the worst case, while the evaluation on a grid
  given by the Chebyshev nodes is close to optimal. Therefore, when
  interpolating a function not restircted to a specific grid you'd
  be best advised to use \code{chebyshev_nodes} and
  \code{chebyshev_lagrange_weights}.

\example
  variable x = chebyshev_nodes(20; min=-2, max=2);
  variable w = chebyshev_lagrange_weights(20);
  variable f = sin(x);
  % we can extrapolate, but this will diverge quickly
  variable p = lagrange_poly([-5:5:#100], x, f; w=w);

\seealso{lagrange_weights,chebyshev_nodes,chebyshev_lagrange_weights}
\done

\function{lagrange_poly_deriv}
\synopsis{Calculate y values of the derivative of a Lagrange polynomial}
\usage{dy = lagrange_poly_deriv(x, y);}
\qualifiers{
  \qualifier{w}{calculated weights can be passed directly}
}
\description
  Returns the y values of the derivative of the lagrange polynomial at the
  given points x according to Berrut & Trefethen 2004. Interpolating those
  points with the same weights gives the full derivative function.

\example
  variable x = chebyshev_nodes(20; min=-2, max=2);
  variable w = chebyshev_lagrange_weights(20);
  variable f = sin(x);
  variable p = lagrange_poly([-5:5:#100], x, f; w=w);
  variable df = lagrange_poly_deriv(x, f);
  variable dp = lagrange_poly([-5:5:#100], x, df; w=w);

\seealso{lagrange_poly,lagrange_weights}
\done

\function{lagrange_weights}
\synopsis{Get Lagrange polynomial weights}
\usage{Double_Type[] lagrange_weights(Double_Type[] x0);}
\description
  Given an array of evaluation points \code{x0} this function
  calculates the weights for the barycentric Lagrange polynomial.
\seealso{lagrange_poly}
\done

\function{lag_energy_spectrum}
\synopsis{Calculates the time lag of energy-resolved lightcurves}
\usage{Struct_Type les = lag_energy_spectrum(String_Type[lcs], dimseg)}
\description
    * lc_list: Array of file names of the energy-resolved
               lightcurves (must have same length and time binning)
    * dimseg:  Segmentation length in bins, needed to segment the
               lightcurve and do the Fourier calculation
    
    returns: lag-energy-spectrum as a struct with fields lag, errlag
\qualifiers{
    \qualifier{dt}{Time resolution of the lightcurve in seconds
                [default: TIMEDEL keyword]}
    \qualifier{deadtime}{Detector deadtime in seconds [default: 0.0s]}
    \qualifier{f_lo}{Array_Type. For frequency-resolved lag-energy
                spectra. Default: lowest sampled frequency [an array
                containing only fmin=1/(dt*dimseg)]. Unit: Hz}
    \qualifier{f_hi}{See f_lo. Default: Nyquist frequency [1/(2*dt)]. Unit: Hz}
    \qualifier{verbose}{Increase verbosity [default=0]}
    \qualifier{outfile}{String_Type. If set, write various timing products
                into a FITS file.}
    \qualifier{elo, ehi}{Double_Type: Energy grid}
    \qualifier{elo_ref, ehi_ref}{Boundaries of reference lightcurve}
}
\notes
    * The default error computation is Nowak et al., 1999, ApJ, 510,
      874 (Sect. 4).
    * All input lightcurves are accounted for their gaps and segmented
      by segment_lc_for_psd. 
    * The cross-spectrum (CPD) and lags are calculated with foucalc.
    * The PSD normalization is fixed to Miyamoto.
    * The average time lag in the frequency interval is calculated by
      the normal mean on the imaginary and real part of the
      cross-spectrum: 
      atan2(mean(Imag(CPD)), mean(Real(CPD)))/(PI*(fmin+fmax))
    * Reference lightcurve: For each energy bin, the function takes the
      summed lightcurves of all but the current energy bands (always
      excluding the lightcurve for the cross-spectrum).
    * The function does *not* use any information about the energy
      grid. This has to be created by yourself (and must be the same as
      the lightcurves in lc_list)! The elo, ehi qualifiers are only
      for the records in the output file
    
    If you want to add energy information to the output FITS file,
    you can pass two Double_Type arrays (same length as lc_list) via
    the elo, ehi qualifiers. The columns of the created extension,
    called BAND_<f_lo>_<f_hi>Hz, are:
  
    ENERGY_LO           - Energy channel (keV), taken from elo qualifier
    ENERGY_HI           - Energy channel (keV), taken from ehi qualifier
    MEAN_PHASE          - Phase (radians), computed as atan2(mean(Im(CPD)), mean(Re(CPD)))
    LAG                 - Time lag (s), computed as phase/(PI*(fmin+fmax))
    LAG_ERR             - Error computed as in Nowak et al., 1999, ApJ, 510, 874 Eq. 16
    LAG_ERR_PROPERR     - Error computed from Gaussian error propagation
    MEAN_REAL/IMAG_CPD  - real/imaginary part of the mean of the
                          segment-averaged cross-spectrum
    MEAN_REAL/IMAG_CPD_ERR - standard error on the mean of the
                          real/imaginary segment-averaged cross-spectrum 
    RMS/RMS_ERR         - Root Mean Squared variability of the channel-of-interest 
                          lightcurve. The error is calculated using
                          Vaughan et al., MNRAS 345, 1271, 2003 Eq. 11.
                          
    Following keywords are written into each extension:
    
    N_SEGS              - Number of segments averaged over (foucalc.numavgall)
    N_FREQS             - Number of frequencies averaged over (as in
                          "mean of the CPD")
    F_MIN/F_MAX         - Minimum and maximum frequency as given by
                          f_lo, f_hi qualifiers
    FMIN_DFT/FMAX_DFT   - Minimal/Maximal frequency of the frequency-filtered
                          Fourier products - this can be different from
                          F_MIN/F_MAX due to the discretization of the
                          *Discrete* Fourier Transform
                         
    If you find any bugs or have questions, please contact ole.koenig@fau.de

    The function also accepts all qualifiers of segment_lc_for_psd.

\example
    %% Example for frequency-resolved lag-energy spectrum
    (f_lo, f_hi) = log_grid(1, 50, n_freqs);  % (Hz)
    % Energy grid must be the same as extracted lightcurves
    (elo, ehi) = log_grid(0.5, 10, n_energies);  % (keV)
    variable les = lag_energy_spectrum(lc_names, dimseg
                                       ; f_lo = f_lo, f_hi=f_hi,
                                       verbose=2, outfile="test.fits", elo=elo, ehi=ehi);
    _for ii (0, n_freqs-1, 1) {
      ohplot_with_err(elo, ehi, les.lag[*,ii], les.errlag[*,ii]);   
    }
\seealso{segment_lc_for_psd, foucalc, colacal}
\done

\function{Lamberg_Equal_Area_projection}
\synopsis{Computes the Lambert_Equal_Area_projection}
\usage{(Double_Type x, y) = Lambert_Equal_Area_projection(Double_Type phi,theta);}
\qualifiers{
\qualifier{deg}{\code{phi} and \code{theta} are in degrees, not in radian}
\qualifier{inverse}{calculate the inverse projection, interpreting phi as the x- and
               theta as the y-coordinate; all other qualifiers are 
               also interpreted as expected.}
}
\description
 This is an equal area projection centered on the North Pole (when plotting
 southern objects, flip the sign of theta!). phi is the angle along the 
 equator, theta is measured from the equator.

    \code{R =sin((pi/2 - theta)/2);}\n
    \code{x = R * sin(phi);}\n
    \code{y = R * cos(phi);}\n

 The inverse function returns nan if the arguments given are not possible.

\seealso{Aitoff_projection, Hammer_projection}
\done

\function{lc_from_events}
\synopsis{extracts light curves from an event list}
\usage{Struct_Type dat = lc_from_events(Struct_Type evts [, Array_Type bands]);}
\qualifiers{
\qualifier{dt}{time resolution in sec [default: 100]}
\qualifier{back}{Struct_Type b_evts : subtract background events}
\qualifier{gti}{struct { Double_Type start, stop } : GTIs for events}
\qualifier{minfracexp}{minimum allowed fractional exposure of a time bin, requires GTIs to be set}
}
\description
    This function extracts light curves from an event list given as the
    following structure:
      Double_Type[] time - the event arrival times (in seconds)
      Double_Type[] pi   - the associated event energies (in eV) 
    If \code{bands}
    is given, an array of structures containing light curves of each band
    is returned. The energy bands have to be given in eV. To receive
    two lightcurves with energies between 1-2keV and 2-3keV, e.g., 

    If GTIs are provided, the fractional exposure 'fracexp' will be added
    to the output structure.
    
    \code{dat = lc_from_events(evts, [1000,2000,3000]);}
\seealso{color_color_data, histogram}
\done

\function{leapyear}
\synopsis{tells whether a year is a leapyear in the Gregorian calendar}
\usage{Char_Type leapyear(Integer_Type y)}
\description
    For a leapyear in the Gregorian calendar, \code{y} is either
    divisible by 4 but not by 100,
    or \code{y} is divisible by 400.
\seealso{daysPerMonth}
\done

\function{linear_fit_xerr_yerr_data}
\synopsis{fits a linear function to data points with both x- and y-errors}
\usage{(Double_Type a, b) = linear_fit_xerr_yerr_data(Double_Type X[], Xerr[], Y[], Yerr[][, Double_Type b0]);
\altusage{(Double_Type a, aerr, b, berr) = linear_fit_xerr_yerr_data(Double_Type X[], Xerr[], Y[], Yerr[][, Double_Type b0]; with_errors);}
}
\qualifiers{
\qualifier{verbose}{}
\qualifier{n}{number of iterations}
\qualifier{with_errors}{The 90% confidence interval (Delta chi^2 = 2.7) is computed as well.}
}
\description
    The data points \code{(X[i]+-Xerr[i], Y[i]+-Yerr[i])}
    are iteratively fitted with the linear function \code{y = a + b*x}.
    If no initial value for the slope b is specified,
       \code{b0 = s * [max(Y)-min(Y)]/[max(X)-min(X)]}
    is used.
    The best fit is found iteratively after the algorithm
    by Fasano & Vio (1988), BICDS 35, p.191.
\seealso{linear_regression}
\done

\function{linear_regression}
\synopsis{computes a linear regression fit}
\usage{Struct_Type = linear_regression(Double_Type[] x, y[, err]);}
\description
    \code{err} is the uncertainty of the \code{y} values.
    If \code{err} is not specified, all data points are weighted equally.

    The parameters \code{a} and \code{b} of the best fit \code{y = a + b*x}
    are   \code{a = (Sy*Sxx - Sx*Sxy)/D}\n
      and \code{b = (S *Sxy - Sx*Sy )/D}\n
    where \code{S  = sum(1/err^2)}\n
          \code{Sx = sum(x/err^2)},  \code{Sxx = sum(x*x/err^2)}\n
          \code{Sy = sum(y/err^2)},  \code{Sxy = sum(x*y/err^2)}\n
      and \code{D = S*Sxx - Sx^2}.

    The returned structure contains the fields \code{a} and \code{b},
    and the errors \code{da} and \code{db}, respectively,
    as well as the reduced chi square \code{chisq} of the fit.
\seealso{linear_fit_xerr_yerr_data}
\done

\function{linear_xyfit}
\synopsis{linear xy fit function to be used with xyfit_fun}
\usage{xyfit_fun ("linear");}
\description
    This function is not meant to be called directly!
    
    Calling \code{xyfit_fun ("linear");} sets up a linear fit
    function for xy-data. It has the form \code{y = a*x + b},
    where \code{a} and \code{b} are the two fit parameters.
\seealso{xyfit_fun, define_xydata, plot_xyfit, linear_regression}
\done

\function{lines}
\synopsis{model for gaussian line profiles of highly ionized ions' transitions}
\usage{fit_fun("lines(id)");}
\description
     The return values are the bin-averages of  1 + sum_i gauss_i(lambda) ,
     i.e., lines is a multiplicative model.
\seealso{gauss, set_lines_par_fun}
\done

\function{line_intersect}
\synopsis{Calculate the intersection of two lines defined by
          two points each}
\usage{res=line_intersect(P0,P1,P2,P3,&alphaP,&alphaQ);}
\altusage{res=line_intersect(P0x,P0y,P1x,P1y,P2x,P2y,P3x,P3y,&alphaP,&alphaQ);}
\description
 This function implements a fast line intersection algorithm
 based on 
 https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-line-segments-intersect
 between the line segments P=P0--P1 and Q=P2--P3, where the points Pi 
 are defined as structures of the type struct {x,y} or through 
 coordinates P0x,P0y, etc.

 The function returns 1 if an intersection exists and 0 if not.

 If an intersection exists, the function returns the relative position
 of the intersection point on the segments in variables alphaP and alphaQ.
\done

\function{lissajous_pattern}
\synopsis{calculates a Lissajous pattern for 3:4 ratio with PI/4 offset}
\usage{variable li = lissajous_pattern();}
\qualifiers{
\qualifier{amplitude}{[1.0]: amplitude by default from -1 to 1}
\qualifier{tstart}{[0.0]: tstart}
\qualifier{tstop}{[1.0]: tstop}
\qualifier{x0}{[0.0]: x0}
\qualifier{y0}{[0.0]: y0}
}
\description
    An ARF is built from a mirror area, filter-,
    support grid-, detector layer transmission
    information. The transmission files-parameters
    have to be file names of transmission tables 
    in the format:

\done

\function{lists_all_lines}
\synopsis{shows the parameters of a lines-model's lines in a given wavelength range}
\usage{list_all_lines([ids][, lMin, lMax]);}
\seealso{lines, get_lambda_parameters_of_all_lines, list_lines}
\done

\function{list_copy}
\synopsis{makes a copy of a nested list}
\usage{List_Type copy = list_copy(List_Type list);}
\description
    \code{list_copy} returns a properly dereferenced copy of an
    arbitrarily nested \code{List_Type} with all its entries. 
    Depending on the Data_Type of the entries, \code{list_copy}
    iteratively calls the according sub-function
    (\code{struct_copy}, \code{array_copy} or \code{list_copy}).
    If the Data_Type of one of its entries is not
    one of privouse mentioned ones, \code{list_copy} returns
    @(Data_Type) or respectively @(entry).
\example
     s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
     s[0].a[[0]]=[0:9];
     l = [{ array_copy(s), 1., [1:10] }, {"abs"}];
     copy = COPY(l); copy[0][0][0].a[[0]] = ["modified"];
     print(l[0][0][0].a);
\seealso{COPY, struct_copy, array_copy, assoc_copy}
\done

\function{list_free_bounds}
\synopsis{List parameter range bounds for free parameters}
\usage{list_free_bounds();}
\qualifiers{
  \qualifier{tolerance}{[=0.01] Boundary hit tolerance}
}
\description
  This function reports the parameter boundaries for all free parameters.
  For more details see help for \code{list_par_bounds}.
\seealso{list_par_bounds, par_bounds}
\done

\function{list_lines}
\synopsis{shows the parameters of a lines-model's used lines in a given wavelength range}
\usage{list_lines([ids][, lMin, lMax]);}
\seealso{lines, get_lambda_parameters_of_lines, list_all_lis}
\done

\function{List_par}
\synopsis{list current fit function and parameters}
\usage{List_par ([arg])}
\altusage{List_par ( String_Type[] pattern )}
\description
   The optional argument is used to redirect the output.  If arg
   is omitted, the output goes to stdout.  If arg is of type
   Ref_Type, it the output string is stored in the referenced
   variable.  If arg is a file name, the output is stored in that
   file.  If arg is a file pointer (File_Type) the output is
   written to the corresponding file.

   If the given argument is a String pattern, only parameter matching
   this pattern are listed!

   The parameter listing looks like this:

   gauss(1) + poly(1)
     idx  param        tie-to  freeze    value      min          max
      1  gauss(1).area     0     0       103.6        0            0
      2  gauss(1).center   0     0        12.1       10           13
      3  gauss(1).sigma    0     0       0.022    0.001          0.1
      4  poly(1).a0        0     0       1.2e4        0            0
      5  poly(1).a1        0     1           0        0            0
      6  poly(1).a2        0     1           0        0            0

    The first line defines the form of the fit-function. The
    parameter index idx may be used to refer to individual fit
    parameters (see set_par).  freeze = 1 (0) indicates that the
    corresponding parameter value is frozen (variable).  If two
    parameter values are tied together, the connection is indicated
    in the tie-to column.  For example, if parameter 1 has tie-to =
    5, that means the value of parameter 1 is tied to the value of
    parameter 5; if parameter 5 changes, parameter 1 will follow
    the change exactly.  If min=max=0, the corresponding parameter
    value is unconstrained.

    In input parameter files (see load_par), lines beginning with a
    '#' are mostly ignored and may be used to include comments.
    Exceptions to this rule are "special" comment lines which are
    used to support additional functionality such as, e.g. writing
    some parameters as functions of other parameters (see
    set_par_fun).  Note that, aside from these special cases,
    comment lines are not loaded by load_par and will not be
    preserved if file is later overwritten by save_par.

\seealso{list_free, edit_par, set_par, get_par, save_par, set_par_fun}
\done

\function{list_Par}
\synopsis{lists parameters of the current fit-function as ISIS-commands}
\usage{list_Par([ pars[] ]);}
\qualifiers{
\qualifier{fit_fun}{lists also the fit-function}
\qualifier{fmt}{format statement for min/max/value}
}
\done

\function{list_par_bounds}
\synopsis{List parameter range bounds}
\usage{list_par_bounds();}
\qualifiers{
  \qualifier{tolerance}{[=0.01] Boundary hit tolerance}
}
\description
  This function reports the values of the fit parameters relative to the
  allowed parameter range. The value is indicated in a graphical indicator
  where the parameter lies (from -0.5 to +0.5 around the center). If the
  parameter value is within <tolerance> close to the range minimum or maximum
  the indicator at the correspongind limit changes from '<' or '>' to '|'.
  In the case of min == max both ends are displayed as '|'.
\seealso{list_free_bounds, par_bounds}
\done

\function{list_xypar}
\synopsis{list current xy-parameter and xy-function}
#c%{{{
\usage{list_xypar();}
\seealso{get_xyfit_fun, save_xypar, xyfit_fun}
\done

\function{loadDataset}
\synopsis{loads and assigns one spectrum and its detector response}
\usage{Integer_Type id = loadDataset(phaFile[, rmfFile[, arfFile[, backFile[, row]]]]);}
\description

\done

\function{loadHETGlcs}
\synopsis{Load Chandra HETG lightcurves}
\usage{lcs = loadHETGlcs();}
\qualifiers{
\qualifier{dir [="."]}{     : path to the extracted spectra}
\qualifier{heg [=1]}{       : load HEG spectra (=0: don't load)}
\qualifier{meg [=1]}{       : load MEG spectra (=0: don't load)}
\qualifier{order [=[-1,1]]}{: diffraction orders to load. Sign matters.}
\qualifier{energyband [=[500,10000]]}{: extracted energy bands. See
                                 description below for details.}
\qualifier{unit [="eV"]}{   : Unit for the energyband: eV, keV, A}
}
\description   
   Load Chandra HETG lightcurves extracted with the (new) Remeis
   extraction scripts and return a structure with the results.
   
   The new (summer 2014) Remeis Chandra gratings extraction scripts
   do not longer specify observation / extraction specific
   information in the file name, but only the arm, diffraction order,
   and energyband of the lightcurve, e.g., heg_m1_500-1000.lc or
   meg_p1_3500-10000.lc. 
   
   loadHETGlcs reads the corresponding lightcurve for each specified arm,
   diffraction order, and enegeryband and stores it as a field in a
   structure. 
   The returned structure is then in the form (in case of the defaults):
      struct{
        dir = ".",
        orders = [-1,1],
        arms = ["heg","meg"],
        energyband = [500, 10000],
        unit = "eV",
        heg_m1_500_10000 = Struct_Type, % content of the fits file
        heg_p1_500_10000 = Struct_Type,
        meg_m1_500_10000 = Struct_Type,
        ...
      }
   The function returns -1 if both arms are ignored, i.e., no
   lightcurves chosen.
   
   The energyband can be specified in multiple ways:
     * a list or array of limits: [0.5,1.5,3,10] keV. In this case, the
       energybands for the lightcurves are assumed to be 0.5-1.5,
       1.5-3, and 3-10 keV, i.e., there are no gaps.
     * a list or an array of energy pairs: {[0.5,1.5],[3,10]} or
       [{0.5,1.5},{3,10}] keV. Then the bands are taken as they are,
       i.e., it is possible to have gaps between energy bands. 
  
\seealso{loadHETGlc_sum, loadHETGspec}
\done

\function{loadHETGlc_sum}
\synopsis{ADD Chandra HETG lightcurves}
\usage{Struct_Type lcsum = loadHETGlcs( [lcs] );}
\qualifiers{
\qualifier{dir [="."]}{     : path to the extracted spectra}
\qualifier{heg [=1]}{       : load HEG spectra (=0: don't load)}
\qualifier{meg [=1]}{       : load MEG spectra (=0: don't load)}
\qualifier{order [=[-1,1]]}{: diffraction orders to load. Sign matters.}
\qualifier{energyband [=[500,10000]]}{: extracted energy bands. See
                                 description below for details.}
\qualifier{unit [="eV"]}{   : Unit for the energyband: eV, keV, A}
}
\description   
   Sum multiple Chandra HETG lightcurves extracted with the (new) Remeis
   extraction scripts and return a structure with the result.
   
   The new (summer 2014) Remeis Chandra gratings extraction scripts
   do not longer specify observation / extraction specific
   information in the file name, but only the arm, diffraction order,
   and energyband of the lightcurve, e.g., heg_m1_500-1000.lc or
   meg_p1_3500-10000.lc, and only extract lightcurves on a per
   arm/order basis. 
   
   Often, we are more interested in the combined lightcurve of
   multiple arms / orders. 
   loadHETGlc_sum can be used in 2 ways:
     - call the function with no arguments and use the qualifiers to
       specify which lightcurves you are interested in. The
       qualifiers are passed to loadHETGlcs to load the lightcurves
       first.
     - call the function with 1 argument: the lightcurves produced by
       a previous call of loadHETGlcs. In this case, all lightcurves
       contained in the structure will be summed, and the qualifiers
       will have no effect [in the future, they could be used to
       choose a subset of the given lightcurves, but this is not
       implemented yet].
   
   The returned structure has the same fields as simply reading a
   single lightcurve file would produce. If reading of the
   lightcurves fails, the return value is -1. 
   
   The function assumes that the lightcurves have not been tempered
   with, i.e., that they are correctly (Chandra) formatted
   lightcurves and have the same length, time grid, etc. No checks
   are performed. Fields like time and exposure are taken from the
   first lightcurve in the structure, since they are expected to be
   the same for various extractions of the same ObsID. 
   
   The energyband can be specified in multiple ways:
     * a list or array of limits: [0.5,1.5,3,10] keV. In this case, the
       energybands for the lightcurves are assumed to be 0.5-1.5,
       1.5-3, and 3-10 keV, i.e., there are no gaps.
     * a list or an array of energy pairs: {[0.5,1.5],[3,10]} or
       [{0.5,1.5},{3,10}] keV. Then the bands are taken as they are,
       i.e., it is possible to have gaps between energy bands. 
  
\seealso{loadHETGlcs, loadHETGspec}
\done

\function{loadHETGspec}
\synopsis{Load locally extracted Chandra HETG spectra and responses}
\usage{Struct_Type ids = loadHETGspec();}
\qualifiers{
\qualifier{dir [="."]}{     : path to the extracted spectra}
\qualifier{heg [=1]}{       : load HEG spectra (=0: don't load)}
\qualifier{meg [=1]}{       : load MEG spectra (=0: don't load)}
\qualifier{order [=[-1,1]]}{: diffraction orders to load. Sign matters.}
}
\description
   Load Chandra HETG spectra extracted with the (new) Remeis
   extraction scripts and return a structure with the dataset indices.
   
   The new (summer 2014) Remeis Chandra gratings extraction scripts
   do not longer specify observation / extraction specific information 
   in the file name, but only the arm and diffraction order of the
   spectrum, e.g., heg_m1.pha or meg_p1.pha. The old extraction
   scripts used to generate a loaddata0.sl file for convenience. With
   the more generic file names, this function replaces the load
   script. To ensure backwards compatibility with dataset-index
   sensitive par files, the function loads the arms and orders in the
   exact same order as the old load scripts used to: 
      heg_m1 heg_p1 meg_m1 meg_p1 heg_m2 heg_p2 meg_m2 meg_p2 ...
   
   The returned structure is then in the form (in case of the defaults):
      struct{
        dir = ".",
        orders = [-1,1],
        arms = ["heg","meg"],
        heg_m1 = 1,
        heg_p1 = 2,
        meg_m1 = 3,
        ...
      }
   The function returns -1 if both arms are ignored, i.e., no spectra
   chosen.
      
   Negative and positive diffraction orders can be chosen independently
   of each other. However, all selected orders are applied to all
   selected grating arms: You can load HEG without MEG and vice versa. 
   But if you load them both in a single call of loadHETGspec, the
   same orders will be loaded for both of them. 
      
   If you would like to load configurations like [heg_m1, meg_p1],
   where different orders are loaded for each grating arms, you have
   to call the function multiple times or load the spectra by hand. 
   
\seealso{loadHETGlcs, loadHETGlc_sum}
\done

\function{load_1storderHETGS_datasets}
\synopsis{loads MEG+-1 & HEG+-1 Chandra HETGS spectra}
\usage{Integer_Type ids[] = load_1storderHETGS_datasets(String_Type specpath, RMFpath);}
\done

\function{load_atime}
\synopsis{loads a structure containing arrival times from a FITS-file}
\usage{Struct_Type load_atime(String_Type filename, String_Type extname);}
\description
    The arrival times, which were stored previously by
    'save_atime', are loaded and returned as a structure.
    This structure has the fields described in the
    'atime_det' function. The extension table containing
    the arrival times must be specified as well as the
    FITS-file itself.
\seealso{save_atime, atime_det}
\done

\function{load_data_combined}
\synopsis{reads and combines the given rows from a Fits Type II pha file}
\usage{Integer_Type load_data_combined(String_Type, pha_filename, Integer_Type[] rows);}
\description
    A combination of load_data and combine_datasets, with
    the exception that the rows appear as a single dataset.

    All qualifiers are passed to load_data.

\seealso{load_data, combine_datasets}
\done

\function{load_data_integral}
\synopsis{reads an INTEGRAL spectrum from a PHA file and performs the necessary tweaks}
\usage{Integer_Type id = load_data_integral(String_Type pha_filename);}
\description
    ISIS's load_data often does not work with INTEGRAL spectra for the following reasons:\n
    - The extension of the PHA file is not called 'SPECTRUM'.\n
    - The extensions of the RMF file are not called 'MATRIX' and 'EBOUNDS'.\n
    - The extension of the ARF file is not called 'SPECRESP'.\n
    - The columns ENERG_LO and ENERG_HI of the ARF file contain only zeros.\n
    The function load_data_integral tries to make the appropriate changes
    (modify the extension names, write a new ARF file) and finally uses load_data.
\qualifiers{
\qualifier{verbose}{[=1] show changes}
}
\seealso{load_data}
\done

\function{load_fermi}
\synopsis{loads fermi data produced by the "Fermi-SED Script"@Remeis}
\usage{Integer_Type data_id = load_fermi(String_Type filename);}
\qualifiers{
\qualifier{rmf_factor}{[=10]: define a fine RMF to properly evaluate the model}
}
\description
  Loads fermi data produced by the "Fermi-SED Script"@Remeis. Note
  that special care is taken here, as the energy bins are relatively
  large. Therefore the function set_bin_corr_factor is used to
  calculate a correction factor. This factor is used automatically
  for loading the data. In order to plot the data properly,
  plot_data/unfold has to be used.
  WARNING: Use clear_all to delete the correction factors (as well 
  as the data). This is necessary as otherwise the correction factors 
  are applied to a newly loaded dataset with the same ID.

  This function is not working if you have computed a Fermi spectrum
  with Fermipy!

\seealso{load_fermi_spectrum,load_fermi_catalog}
\done

\function{load_fermi_catalog}
\synopsis{loads Fermi data from a catalog for a given source}
\usage{Integer_Type data_id = load_fermi_catalog(String_Type sourcename);
}
\qualifiers{
\qualifier{list}{return list of available sources}
\qualifier{ul}{Load includes upper limits}
\qualifier{catalog}{specify the catalog (default: 2FGL_point_source@Remeis) }
\qualifier{fgl3}{Use this when reading in the 3FGL catalog, either with 
                   the catalog qualifier or without }
}
\description
  WARNING: Use clear_all to delete the correction factors (as well 
  as the data). This is necessary as otherwise the correction factors 
  are applied to a newly loaded dataset with the same ID.

\seealso{load_fermi}
\done

\function{load_fermi_spectrum}
\synopsis{loads fermi data produced by any fermipy script}
\usage{Integer_Type data_id = load_fermi_spectrum(String_Type filename);}
\qualifiers{
\qualifier{rmf_factor}{[=10]: define a fine RMF to properly evaluate the model}
\qualifier{ts_min}{[=9]: set TS threshold for spectral bins to be loaded in as data points.}
}
\description
  Loads fermi data produced by any Fermi spectrum produced with fermipy. Note
  that special care is taken here, as the energy bins are relatively
  large. Therefore the function set_bin_corr_factor is used to
  calculate a correction factor. This factor is used automatically
  for loading the data. In order to plot the data properly,
  plot_data/unfold has to be used.

  A note regarding upper limits: you can set yourself at which significance
  you consider a spectral to be valid and not use the 2 sigma upper limit (UL).
  The default value is TS = 9, which is ~3 sigma. The SED plot produced by 
  the Fermi-LAT analysis script has a lower threshold, which is why you might
  get less data points in your spectrum, when you load it in isis with the
  default value of 'ts_min'. 
  Spectral bins with a lower TS value will be treated as upper limits and 
  not loaded in isis. For plotting purposes, the upper limits need to be 
  added from the [*]_sed.fits file (column e2dnde_ul) and multiplied by a factor
  of 1.60218e-6 to convert from MeV to erg.

  This function supersedes the previous function 'load_fermi', which can only
  be used to load the Fermi Spectra produced by the original "Fermi-SED"
  script.

  WARNING: Use clear_all to delete the correction factors (as well 
  as the data). This is necessary as otherwise the correction factors 
  are applied to a newly loaded dataset with the same ID.



\seealso{load_fermi}
\done

\function{load_grouped_xmm_data}
\synopsis{wrapper for load_data, which sets already a few basic options}
\usage{Integer_Type Index = load_grouped_xmm_data([String_Type Path], [E_lo, E_hi]);}
\qualifiers{
\qualifier{min_sn[=5]}{[\code{=5}]: Set the minimal S/N ratio (see help("group");) }
\qualifier{min_chan[=2]}{[\code{=2}]: Set the minimal number of Channels do be rebinned (see help("group");) }
\qualifier{delete_data}{If set, any data sets loaded previously will be deleted.}
}
\description
   This functions loads data (by default "src_sd.pha"), groups it,
   and notices the energy bins between E_lo[=1keV] and E_hi[=10keV]. 
   When changing all default values, it can be easily used for any
   other satellite.
\done

\function{load_HETGS_datasets}
\synopsis{loads Chandra HETGS spectra from a type II pha file}
\usage{Integer_Type ids[] = load_HETGS_datasets(String_Type specpath, RMFpath[, Integer_Type ms[]]);}
\description
    \code{ids} is an array containing negative/positive MEG/HEG spectra
\done

\function{load_par_from_FITS_header}
\synopsis{loads the fit-function and parameters from a FITS file}
\usage{load_par_from_FITS_header(String_Type filename);}
\description
    This function reads keywords from a FITS header
    created with \code{save_par_to_FITS_header_struct}.
\seealso{save_par_to_FITS_header_struct, save_par, load_par}
\done

\function{load_par_tbnew}
\synopsis{load a parameter file after decreasing the upper limit for tbnew.PL}
\usage{load_par_tbnew(String_Type filename);}
\description
    Early versions of the \code{TBnew} absorption model allowed
    for a too large upper limit of the \code{PL} parameter of 5.
    This function reduces the \code{PL} parameter to 3.999,
    rewrites the parameter file, and loads it with \code{load_par}.
\seealso{load_par}
\done

\function{load_pha}
\synopsis{Load pha file with grouping and quality information}
\usage{load_pha("pha_filename")}
\seealso{group_pha}
\done

\function{load_piled1storderHETGS_datasets}
\synopsis{loads 1st order Chandra-HETGS data and sets up the simple_gpile model}
\usage{Integer_Type ids[] = load_1storderHETGS_dataset(specpath, RMFpath);}
\done

\function{load_piledHETGS_datasets}
\synopsis{loads Chandra HETGS spectral and sets up the simple_gpile model}
\usage{Integer_Type ids[] = load_1storderHETGS_dataset(specpath, RMFpath);}
\seealso{load_HETGS_datasets, use_simple_gpile}
\done

\function{load_radio2}

\synopsis{reads and loads radio data}
\usage{Integer_Type data_id = load_radio (String_Type filename);}

\description
    Takes data an ASCII file , comprised of three  columns: 
    Frequency[Hz], Flux density [mJy], Error [mJy], and loads
    the data as a dataset.
    WARNING: The standard bin width is set to 10%

    It can be changed with the qualifier binwidth (given in
    fraction of the frequency). Since a log frequency grid is
    used, it will not be exactly the size given.
    WARNING: Bins cannot overlap, as grid needs to be continuous!
    
\qualifiers{
\qualifier{binwidth}{   specify bin width}
\qualifier{use_struct}{ use an ISIS structure with the
                          following fields: freq[Hz],
                          bandwidth[Hz], flux[mJy], err[mJy];
                          with full bandwidth given}
}

\example
    isis> variable data_file = "/some/path/radio_file.txt";
    isis> variable radio_data = load_radio2(data_file);

    example using a textfile that includes bin widths:

    isis> variable abc = ascii_read_table ("radio_dat.txt",[{"%F","freq"},{"%F", "bandwidth"},{"%F","flux"},{"%F","err"}]);
    isis> variable radio_data = load_radio2(abc; use_struct);

\seealso{read_radio, load_radio}
\done

\function{load_scaled_data}
\synopsis{Loads a dataset with a non-unity AREASCAL keyword or column}
\usage{id = load_scaled_data( String_Type file);}
\qualifiers{
\qualifier{rmf}{[="rmf.fits", response file if not in data header]}
\qualifier{arf}{[="arf.fits", effective area file if not in data header]}
\qualifier{bkg}{[="bkg.fits", background file if not in data header]}
}
\description
    Load a data file with a non-unity AREASCAL, rewrite the
    data/background to undo the ISIS default application of this value,
    create a post_model_hook to properly apply it, and rewrite the data
    and background backscales to properly apply it.  The data itself
    must either reference a set of responses/backgrounds, or these must
    be entered via qualifiers upon initial loading of the data.
\seealso{load_data, _define_back, set_data_backscale}
\done

\function{load_xypar}
\synopsis{load xy-parameter and xy-function from parameter file}
#c%{{{
\usage{load_xypar(String_Type file);}
\description
    Load parameter file saved with \code{save_xypar("file.par")}. To display
    the load parameters one should use \code{list_xypar} instead of
    \code{list_par}.
\seealso{save_xypar, load_par}
\done

\function{logging}

\synopsis{write a log file}
\usage{logging ("filename.log", "Log message");}

\description
    This function writes output to a log file (which will be created if it does
    not exist. It can also handle input in the sprintf format (see example).
    By default all input to logging is also printed to stdout.
    Repeated calls to the function add the new message at the end of the file. It
    is therefore recommended to specify the date and time in the name of the log
    file.
    
\qualifiers{
\qualifier{v}{verbosity: [default: 1] print output also to stdout, set this
                              qualifier to 0 for no output to stdout}
}

\example
    isis> variable cutime = strftime("%Y-%m-%d_%H:%M:%S");
    isis> variable l = cutime+".log";
    isis> logging(l, "* Found %.u observations for %s", length(src_info), src);
    isis> logging(l, "***Error in function %s", _function_name() );

\seealso{strftime, _function_name()}
\done

\function{log_grid}
\synopsis{generate a logarithmic histogram grid}
\usage{(bin_lo[], bin_hi[]) = log_grid(min, max, nbins);}
\description
    This function is an shorthand form of:\n
       \code{(log_bin_lo, log_bin_lo) = linear_grid( log(min), log(max), nbins );}\n
       \code{bin_lo = exp( log_bin_lo );}\n
       \code{bin_hi = exp( log_bin_hi );}
\seealso{linear_grid}
\done

\function{log_par}
\synopsis{log-log parabolic fit function}
\description
    Fit function of the form F(x)= exp ( a*(log(x) - center)^2 + peak );
    For fitting the function is implemented in integrated from.
    \code{center} has to be specified in the unit in which the bins are given.
    The function is a parabola in the log-log space of the unit of the bins.
    Currently the function only works for negative curvature \code{a<0}.

    For a log-parabola in the energy/frequency regime the function \code{log_par_en}
    has to be used.
    
\seealso{log_par_en}
\done

\function{log_par_en}
\synopsis{log-log parabolic fit function in the energy/frequency regime}
\description
    Fit function of the form F(nu)= exp ( a*(log(nu) - center)^2 + peak );
    For fitting the function is implemented in integrated from.
    \code{center} has to be specified in Hz. The bins are expected to be
    given in Angstrom (as it is typically the case in ISIS).
    Currently the function only works for negative curvature \code{a<0}.
    
\seealso{log_par}
\done

\function{lorentzmb (fit-function)}
\synopsis{implements a Lorentzian profile}
\description
    \code{L(f)  =  [2 rms^2 Q f0]/[pi/2 + atan(2*Q)] * 1/[f0^2 + 4 Q^2 (f-f0)^2]}\n
    [see also Eq. (1) in Pottschmidt et al. (2003), A&A 407, 1039-1058]\n
    where:\n
    \code{rms} = contribution to the root mean square variability\n
    \code{ Q } = quality factor\n
    \code{ f0} = resonance frequency = \code{nu0/sqrt(1 + 1/[4 Q^2])}\n
    \code{nu0} = peak frequency (maximum of \code{f * L(f)})

    The parameters of the lorentzmb fit-function are
    "rms" (= \code{rms}), "peakfr" (= \code{nu0}) and "quality" (= \code{Q}).

    The integrated fit-function is:\n
    \code{int L(f) df  =  rms^2/[pi/2 + atan(2*Q)] * arctan{2Q/pi*(f/f0-1)}}
\done

\function{lorentzmb_old (fit-function)}
\synopsis{implements a Lorentzian profile}
\description
    \code{L(f)  =  1/pi * [2 R^2 Q f0] / [f0^2 + 4 Q^2 (f-f0)^2]}\n
    [Eq. (1) in Pottschmidt et al. (2003), A&A 407, 1039-1058]\n
    where:\n
    \code{ R } = normalization constant\n
    \code{ Q } = quality factor\n
    \code{ f0} = resonance frequency = \code{nu0/sqrt(1 + 1/[4 Q^2])}\n
    \code{nu0} = peak frequency (maximum of \code{f * L(f)})

    The parameters of the lorentzmb fit-function are
    "norm" (= \code{R}), "peakfr" (= \code{nu0}) and "quality" (= \code{Q}).

    The integrated fit-function is:\n
    \code{int L(f) df  =  R^2/pi * arctan{2Q/pi*(f/f0-1)}}
\done

\function{Lorentz_complex}
\synopsis{Compute complex Lorentz profile}
\usage{Complex_Type[] = Lorentz_complex(z, z0, gamma);}
\done

\function{lorentz_trafo}
\synopsis{lorentz transformation for a boost in any direction}
\usage{lorentz_trafo( Double_Type ct,
                      Vector_Type x,
                      Vector_Type B
                    );
}
\description
    This function performs a Lorentz transformation:
      ( ct, x ) -> ( ct', x')
      ct' = gamma ( ct - B*x )
      x'  = x + ( (gamma-1)/B^2 * B*x - gamma * ct ) * B
    where x is the spartial vector, gamma the Lorentz-factor
    and B=v/c the velocity vector.
    
\seealso{Vector_Type}
\done

\function{lorentz_xyfit}
\synopsis{Lorentzian xy fit function to be used with xyfit_fun}
\usage{xyfit_fun ("lorentz");}
\description
    Calling \code{xyfit_fun ("lorentz");} sets up a Lorentzian fit
    function for xy-data. It has the form 
    \code{y = norm * sigma/(2pi) * 1/((x-center)^2 + (sigma/2)^2)}
\example
    variable id = define_xydata(x, y, yerr);
    xyfit_fun("lorentz");
    () = fit_counts;
    variable xfit, yfit;
    (xfit, yfit) = eval_xyfun([min(x) : max(x) : #1000]);
    plot(xfit, yfit);
\seealso{xyfit_fun, define_xydata, plot_xyfit, linear_regression}
\done

\function{lumdist}
\synopsis{Calculate luminosity distance (in Mpc) of an object given its redshift}
\usage{result = lumdist(z);}
\qualifiers{
\qualifier{silent}{If set, the program will not display adopted
                       cosmological parameters at the terminal.}
\qualifier{h0}{[=70] Hubble parameter in km/s/Mpc}
\qualifier{omega_k}{curvature constant, normalized to the closure density.
                       Default is 0, indicating a flat universe.}
\qualifier{omega_m}{Matter density, normalized to the closure density,
                       default is 0.3. Must be non-negative.}
\qualifier{omega_lambda}{: Cosmological constant, normalized to the
                       closure density (default: 0.7).}
\qualifier{q0}{[=-0.55] Deceleration parameter, numeric scalar = -R*(R'')/(R')^2}
}
\description
    INPUTS:    z = redshift (positive scalar or vector)
    OUTPUTS:   The result of the function is the luminosity distance (in Mpc)
               for each input value of z.
    The luminosity distance in the Friedmann-Robertson-Walker model is 
    taken from  Caroll, Press, and Turner (1992, ARAA, 30, 499), p. 511.
    Uses a closed form (Mattig equation) to compute the distance when the 
    cosmological constant is zero.   Otherwise integrates the function using
    QSIMP.	
    See help for cosmo_param for a description of the cosmological parameters,
    note that only two out of the four parameters should be given!

    The routine can fail to converge at high redshift for closed universes with
    non-zero lambda. This can presumably be fixed by replacing QSIMP with
    an integrator that can handle a singularity.

\example
    %  Plot the distance of a galaxy in Mpc as a function of redshift out 
    %  to z = 5.0, assuming the default cosmology (Omega_m=0.3, Lambda = 0.7,
    %  H0 = 70 km/s/Mpc)
    variable z = [0:5:0.1];
    plot (z, lumdist(z));

    %  Now overplot the relation for zero cosmological constant and 
    %  Omega_m=0.3
    oplot (z, lumdist(z ; omega_lambda=0 , omega_m=0.3) );
\seealso{cosmo_param, qsimp}
\done

\function{luminosity}
\synopsis{computes a source luminosity assuming the current fit-function and a distance}
\usage{Double_Type luminosity(Double_Type Emin, Emax, d);}
\qualifiers{
\qualifier{factor}{[=\code{1.001}] step of logarithmic energy grid}
\qualifier{Emin}{minimum energy of (extended) grid}
\qualifier{Emax}{maximum energy of (extended) grid}
}
\description
    returns \code{4pi (d kpc)^2 * int_Emin^Emax E*S_E(E) dE}  (in erg/s)

    Use flux2lum to calculate luminosities for extragalactic sources.

\seealso{energyflux, flux2lum}
\done

\function{makepsd}
\synopsis{Calculate the power density spectrum of a single energy band.}
\usage{(psd, nipsd) = makepsd(rate, timeseg, dimseg);}
\description
    Input:
      rate - input rate array
      timeseg - real time of one segment of rate
      dimseg  - number of bins in each segment of rate

    Output:
      psd   - unnormalized, averaged PSD
      nipsd - averaged PSD, individual segments normalized
\qualifiers{
\qualifier{avgbkg}{Average background count rate for Miyamoto normalization}
\qualifier{normtype}{"Miyamoto", "Leahy", "Schlittgen" normalization type}
}
\done

\function{make_fine_rmf}         
\synopsis{creates a fine RMF }
\usage{make_fine_rmf (Data_Id [Integer], Factor [Integer]);}
\description
   Creates a fine RMF. Therefore each data bin will be split into 
   a certain number of bins, specified by the variable "Factor".
\seealso{load_slang_rmf}   
\done

\function{make_movie_from_evtList}
\synopsis{makes a simple movie from a given event list}
\usage{Double_Type total_img = make_movie_from_evtList(string eventFile, double dt, string movieFile);}
\description
    Make a movie from the eventlist "eventFile". The time (in
    seconds) of each frame is adjusted by "dt". The output file is
    written to "movieFile" (best use a mp4 file). 
\qualifiers{
\qualifier{cmap}{["hot"] color map}
\qualifier{thres}{[0.0] threshold of counts/frame, when a frame should be discarded}
\qualifier{fps}{frames per secondswitch on the scale}
\qualifier{size}{[10,10] size of the image in cm}
\qualifier{tstart}{start time of the movie (sec)}
\qualifier{tstop}{end time of the movie (sec)}
\qualifier{labels}{draw some labels (white frame and time in ksec)}
}
\seealso{xfig_plot_new,fitswcs_get_img_wcs}
\done

\function{make_path}
\synopsis{Recursively create multiple directories}
\usage{make_path (dirs)}
\description
   This function creates multiple directory paths. It is
   similar to mkdir_rec, but does not internally change
   directories, allows multiple path specifications, and
   individual modes.

   The function throws an IO error if a path component exists
   and is not a directory.

\qualifiers{
\qualifier{mode}{mode for the paths to be generated. If mode is an 
                 array, it contains the mode for each individual path
                 specified. Default: 0777}
\qualifier{separator}{path separator. Default is /, so you will
                  probably not have to use it...}
}
\example
 make_path(["./test1/test2","./test1/../test3"];mode=[0777,0700]);
\seealso{mkdir,mkdir_rec}
\done

\function{make_spix}
\synopsis{creates a spectral index map using two .fits files (provided by DIFMAP)}
\usage{make_spix(String_Type \code{name_lo}, String_Type \code{name_hi});}
\qualifiers{
\qualifier{iterations}{[=1] set number of translations to average over}
\qualifier{cc}{[=cc.fits] name of the cross correlation map.  Will generate
                              one if the file does not exist.}
\qualifier{lothreshold}{[=NULL] if image has a pixel below this value it's set to
                              this value. also criterion for showing spectral index if
                              req_both_bands is set to 1}
\qualifier{hithreshold}{[=NULL] if image has a pixel below this value it's set to
                              this value. also criterion for showing spectral index if
                              req_both_bands is set to 1}
\qualifier{shift}{[={NULL,NULL}] define the shift of the second image wrt the first
                              in format {x,y} in mas}
\qualifier{crpix_shift}{[=0.] Extra shift for converting mas in pixel. In the past, 0.5 was used, 
					but default is now  0., as only a relative is needed}
\qualifier{dec_step}{[=NULL] pixel size in declination (value>0). 
				 	Default value is the largest pixel size of the two images}
\qualifier{dec_size}{[=NULL] mapsize size in declination. 
					Default value is the largest map size of the two images}
\qualifier{ra_step}{[=NULL] pixel size in right ascension (value>0). 
					Default value is the largest pixel size of the two images}
\qualifier{ra_size}{[=NULL] mapsize size in right ascension
					Default value is the largest map size of the two images}
\qualifier{beam}{[=[NULL,NULL,NULL]] defines beam for restoring the two images 
						[semi-major axis, semi-minor axis, position angle] 
						in mas (major and minor axis) and degree (position angle)}
\qualifier{req_both_bands}{[=0] only make calculations if both bands are above the
                              noise level}
\qualifier{fit_noise}{[=1] set to 0 to use fits header for noise information}
\qualifier{lo_nsigma}{[=3] number of standard deviations from mean to define
                              noise limit.  Only matters if fit_noise==1,
                              overrides lothreshold}
\qualifier{hi_nsigma}{[=3] number of standard deviations from mean to define
                              noise limit.  Only matters if fit_noise==1,
                              overrides hithreshold}
\qualifier{n_beams}{[=3] size of beam to exclude from core on correlation
                              calculation}
\qualifier{excl_core}{[=1] set to 0 if you do not want the program to
                              automatically exclude the core from correlation calculations}
\qualifier{overwrite}{overwrite the restore fits files (if already existing)}
}
\description
    This function creates a spectral index (F=v^a) map from two fits files provided by
    DIFMAP.  A cross correlation fits image is generated and the images are shifted
    by the best (iterations) translations and the spectral index, etc. are
    calculated for each translations. The values are averaged with the weights
    given by the corresponding value in the cc image.
    If the provided images do not have the same size, resolution and beam, the
    functions difmap_restore and enclosing_ellipse are used to obtain images with
    these properties. It is also possible to specify a desired map size and 
    pixel size for both axes individually, otherwise  the default value from the comparison 
    of both images will be used. Note, that changing the pixel size may require changing the maps size as well.

    It returns a structure that holds the information for the following values\n
    Use the structure as an input to\n
    struct_name.spec_map      ->   matrix of calculated spectral index values\n
    struct_name.stdev         ->   matrix of weighted stdevs of calculated values\n
    struct_name.avg_lum       ->   matrix of weighted average of the summed BRIGHTNESS of the images\n
    struct_name.weights       ->   matrix of sum of weights used for calculations in each pixel\n
    struct_name.avg_shift     ->   average shifts in a 2-element array [x_shift,y_shift]\n
    struct_name.shift_weight  ->   weights for calculating avg shift [x_weight,y_weight]\n
    struct_name.avg_shift_pixel -> average shift in pixel [x_shift_pixel, y_shift_pixel]\n
    struct_name.ra_px_center  ->   x index of center pixel\n
    struct_name.ra_steps      ->   x mas per pixel\n
    struct_name.dec_px_center ->   y index of center pixel\n
    struct_name.dec_steps     ->   y mas per pixel\n
    struct_name.major         ->   semi major axis of beam in mas
    struct_name.minor         ->   semi minor axis of beam in mas
    struct_name.source        ->   source name
    struct_name.date          ->   dates of images
    struct_name.posang        ->   position angle of beam in degrees
\seealso{plot_spix, write_spix, read_spix, difmap_restore, enclosing_ellipse}
\done

\function{mass_function}
\synopsis{Calculates the mass function of a binary.}
\usage{Double_Type   mass = mass_function(Double_Type porb, asini[, error_porb, error_asini]);
 or Double_Type[]    i = mass_function(Double_Type mass, Double_Type[] Mopt[, error_mass, error_Mopt]; i);}
\qualifiers{
    \qualifier{i}{calculate and return the inclination (case 2a)}
    \qualifier{Mx}{neutron star mass in units of the solar mass
             (default: 1.4)}
    \qualifier{rad}{return the inclination angle in radian
             (default: degrees)}
    \qualifier{chatty}{set to zero to suppress output messages}
}
\description
    The mass function of a binary is given by
    (see, e.g., Hilditch, 2001)

    f(M) = (M_opt sin i)^3 / (M_x + M_opt)^2
         = (4 Pi^2 (a sin i)^3) / (G P_orb^2)

    with the masses of the neutron star, M_x, and its optical
    ompanion star, M_opt, the inclination, i, the semi-major axis,
    a, the orbital period, P_orb, and the gravitational constant, G.

    There are two ways this function can be used:
    1)  The orbital period (in days) and the semi-major axis (in
        lt-s) are provided, which then is used to calculate the
        right hand side of the above equation. The returned mass is
        given in units of the solar mass.
    2)  The value of the mass function, mass, and the companion
        mass(es), Mopt, are provided and the i-qualifier is set.
        The returned value(s) is (are) the orbital inclination(s), i.
    In case of 2) the neutron star mass is assumed to be Mx = 1.4
    solar masses by default, but can be changed by the Mx-qualifier.
    The thirs possible case, that the companion mass is returned,
    is formally solving a third order polynomial, which is yet not
    implemented here.

    If uncertainties for the given parameters are provided the error
    propagated values are returned as well for the specific case via
      (value,error) = mass_funtion(..., error(s));
\done

\function{matrix2array}
\synopsis{Transforms a matrix to an Array}
\usage{Array_Type[] matrix2array( Any_Type[...,d,...] M, d );}
\description
     From the given dimension d of a matrix M an array is created,
     i.e., the given dimension of the matrix is shifted into the
     array which then contains matrices without this dimension.
\example
     variable M = _reshape( [1:2*3*4], [2,3,4] );
     variable A0 = matrix2array(M,0);
     variable A2 = matrix2array(M,2);
     vmessage("A0");print(A0);
     vmessage("A2");print(A2);
\done

\function{matrix33_as_array}
\synopsis{return the contents of the matrix as a 3x3 matrix}
\usage{ arr=matrix33_as_array(m);}
\description
  Return the contents of the matrix as a 3x3 array

\seealso{vector, Vector_Type, matrix33_new}
\done

\function{matrix33_determinant}
\synopsis{return the determinant of a 3x3 matrix}
\usage{ det=matrix33_determinant(m);}
\description
  Calculates the determinant of a 3x3 matrix. 

\seealso{vector, Vector_Type, Matrix33_Type}
\done

\function{matrix33_diag}
\synopsis{return a diagonal-matrix}
\usage{ Matrix33_Type=matrix33_diag(m11,m22,m33);}
\description
  Returns a 3x3 diagonal matrix. If m11,m22,m33 are given,
  then the three diagonal values are initialized to these
  three values. If only m11 is given, then a scalar
  matrix where all three elements are equal to m11 is
  returned.

\seealso{Vector_Type, Matrix33_Type}
\done

\function{matrix33_identity}
\synopsis{return a identity-matrix}
\usage{ Matrix33_Type=matrix33_identity();}
\description
  Returns a 3x3 identity matrix.

\seealso{Matrix33_Type}
\done

\function{matrix33_new}
\synopsis{instantiate a 3x3 matrix}
\usage{ Matrix33_Type=matrix33_new();}
\description
 Instantiates a new 3x3 matrix object.
  The following initializers are available:
    * No arguments: a zero-Matrix is returned
    * One argument:
        If the argument is of type Matrix33_Type: a copy of the argument
           is returned
        If the argument is a scalar: all matrix elements are initialized to
           this scalar (use matrix33_diag to initialize a diagonal matrix!)
        If the argument is an array with 9 elements: the matrix is initialized
           to these elements
    * Nine arguments: matrix elements m11,m12,m13,m21,m22,m23,m31,m32,m33,
      i.e., the elements are in row order and the matrix is:

               m11 m12 m13
          M =  m21 m22 m23
               m31 m32 m33

\seealso{Matrix33_Type, Vector_Type}
\done

\function{matrix33_null}
\synopsis{return a null-matrix}
\usage{ Matrix33_Type=matrix33_null();}
\description
  Returns a 3x3 null-matrix

\seealso{Matrix33_Type}
\done

\function{matrix33_reflect}
\synopsis{return a 3x3 reflection matrix to change the handedness of the ith axis}
\usage{ Matrix33_Type=matrix33_reflect(i);}
\description
 Returns a reflection matrix, i.e., a matrix that changes the handedness
 of the ith axis (where i=1: x-axis, i=2: y-axis, and i=3: z-axis)
 when multipliying it with a vector.

\seealso{vector, Vector_Type, matrix33_new}
\done

\function{matrix33_rot}
\synopsis{return a standard rotation matrix about the x-, y-, or z-axis}
\usage{ Matrix33_Type=matrix33_rot(i,angle;qualifiers);}
\qualifiers{
 \qualifier{deg}{angle is given in deg [default: radians]}
}
\description
 Returns a rotation matrix to transform a column-3 vector from
 one cartesian coordinate system to another. The new coordinate
 system is given by rotating the original system in a counter
 clockwise way around the ith axis (where i=1: x-axis,
 i=2: y-axis, and i=3: z-axis)

 The nomenclature follows Kaplan et al, The IAU Resolutions
 on Astronomical Reference Systems, Time Scales, and Earth
 Rotation Models, USNO circular 179, 2005.

\seealso{vector, Vector_Type, matrix33_new}
\done

\function{matrix33_scalar}
\synopsis{return a scalar-matrix}
\usage{ Matrix33_Type=matrix33_scala(m11);}
\description
  Returns a 3x3 scalar matrix (a matrix where all diagonal elements have the
  same value and where all other elements are zero)

\seealso{Vector_Type, Matrix33_Type}
\done

\function{matrix33_transpose}
\synopsis{return the transpose of a 3x3 matrix}
\usage{ Matrix33_Type=matrix33_transpose(m);}
\description
  Returns the transpose of a 3x3 matrix

\seealso{vector, Vector_Type, matrix33_new}
\done

\datatype{Matrix33_Type}
\synopsis{3x3 matrix type}
\description
  3x3 Matrix type that is compatible with Vector_Type

  The following operations are defined for the Matrix33_Type:
    * M1+M2: Addition and subtraction of matrices
    * -M1: Changing sign of a matrix
    * M1*M2: Matrix - Matrix multiplication
    * M1*V: Matrix - Vector multiplication (V is a Vector_Type)
    * M1*f and f*M1: Matrix - real multiplication
    * M1/M2: Multiply M1 with the inverse of M2. Do NOT use this...

  Objects of type Matrix33_Type can be instantiated with the 
  following functions (see there for detailed descriptions):
    * matrix33_new: general initialization
    * matrix33_diag: return a diagonal matrix
    * matrix33_scalar: return a scalar matrix
    * matrix33_null: return a null matrix
    * matrix33_identity: return an identity matrix
    * matrix33_rot: return a rotation matrix for the x-, y-, or z-axis
    * matrix33_reflect: return a reflection matrix for the x-, y-, or z-axis

  Other functions operating on matrices (functions marked with + are also 
      available through accessor functions):
    * matrix33_determinant: calculate the determinant of the matrix (+)
    * matrix33_get_diag: return the diagonal elements of the matrix (+)
    * matrix33_get_trace: return the sum of the diagonal elements (+)
    * matrix33_transpose: return the transpose of the matrix (+)
    * matrix33_adjoint: return the adjoint of the matrix
    * matrix33_cofactors: return the matrix of cofactors

  The Matrix33_Type object has the following accessors:
    * m.determinant(): return the determinant of the matrix
    * m.as_array(): return the elements of the matrix as a 3x3 array
    * m.transpose(): return the transpose of the matrix (not in place!)
    * m.diag(): return the diagonal elements as a vector
    * m.trace(): return the trace of the matrix
    * m.inverse(): return the inverse of the matrix (not in place!)

\done

\function{matrixmul}
\synopsis{Matrix multiplication}
\usage{ M[n,m] = matrixmul( A[n,j], B[j,m] );}
\description
    (Efficient) Calculation of the product of two matrices A and B.
    If A is a (n x j)-Matrix and B a (j x m)-Matrix the resulting
    Matrix has the dimensions (n x m).

    If A or B is one dimensional, the neccessary second dimension
    is assumed to be of length 1,
    i.e., A[n]*B[m] =  A[n,1]*B[1,m] = M[n,m]

    This function is identical to the ISIS intrinsic operator #
    or the inner product and only available for backwards
    compatibility!

\seealso{transpose, inner_product}
\done

\function{_maximum}
\synopsis{computes the maximum of >=2 values}
\usage{Array_Type _maximum(Array_Type x1, x2, ...)}
\description
    \code{x1}, \code{x2}, ... have to be arrays of the same length, or scalars.
    The \code{_maximum} function returns an array of the maximum values
    by successively calling the \code{_max} function.
    If all \code{x1}, \code{x2}, ... are scalars, \code{_maximum} returns a scalar value,
    in this case, \code{_maximum(x1, x2, ...)} is equivalent to \code{max([x1, x2, ...])}.
\seealso{_max, max, _minimum, _min, min}
\done

\function{MAXI_lightcurve}
\usage{Struct_Type MAXI_lightcurve(String_Type source);}
\description
    The returned structure has the following fields:\n
    - \code{time}: Modified Julian Date\n
    - \code{rate}  , \code{err}  :  2-20 keV light curve [c/s/cm^2]\n
    - \code{rate_a}, \code{err_a}:  2- 4 keV light curve [c/s/cm^2]\n
    - \code{rate_b}, \code{err_b}:  4-10 keV light curve [c/s/cm^2]\n
    - \code{rate_c}, \code{err_c}: 10-20 keV light curve [c/s/cm^2]\n
\seealso{http://maxi.riken.jp/top/index.php?cid=000000000036, http://ads.nao.ac.jp/abs/2009PASJ...61..999M}
\done

\function{max_time}
\synopsis{finds the latest time in an array of structures with a time field}
\usage{Double_Type tmax = max_time(Struct_Type structs[]);}
\description
    \code{structs} is an array of structures containing the field \code{time}.
    \code{tmax} is computed in the following way:\n
    \code{tmax = max( array_map(Double_Type, &max, array_map(Array_Type, &get_struct_field, [structs], "time")) );}
\seealso{min_time, min_struct_field, max_struct_field}
\done

\function{mbknpo_fit}
\synopsis{Fitting a multiplicative broken powerlaw to data in energy space}
\usage{fit_fun("mknpo");}
\description
    This function can be used to cut a relxill spectrum off at low
    energies (e.g., Steiner et al., ApJL 969, L30, 2024, Ubach et
    al., ApJ 976, 38, 2024).
\seealso{relxill}
\done

\function{mc_sig}
\synopsis{calculates the significance of a spectral component doing a Monte Carlo (MC) simulation}
\usage{Struct_Type mc_sig(String_Type withComponent.fits, String_Type withoutComponent.fits);
 or Struct_Type mc_sig(String_Type torqueDir);}
\qualifiers{
    \qualifier{mcruns}{number of MC loops (default: 10)}
    \qualifier{beforeData}{script to be called before any data is loaded (default: NULL)}
    \qualifier{afterData}{script to be called after the data have been loaded (default: NULL)}
    \qualifier{beforeFit}{array of scripts to be called right before a fit of faked data to
                 the model without [0] and with [1] the component (default: NULL)}
    \qualifier{id}{override the dataset ID(s) after fits_load_fit with the given one(s)}
    \qualifier{ignbinning}{do not apply the same binning after faking the data}
    \qualifier{ignnotice}{do not use the same energy ranges after faking the data}
    \qualifier{torque}{calculate significance using given number of torque jobs, i.e., total
                 number of runs = mcruns*torque (default: 0, i.e., don't use torque)}
    \qualifier{walltime}{walltime of each torque job in minutes (default: 3 minutes times
                 number of MC runs per job)}
    \qualifier{dontSubmit}{do not submit the torque job-file (implies qualifier 'dontwaint')
                 (default: submit)}
    \qualifier{dontWait}{do not wait on the the torque jobs to complete (implies qualifier
                 'dontClean') (default: wait)}
    \qualifier{torqueDir}{temporary directory to save necessary torque files
                 NOTE: this directory will be deleted afterwards completely!
                 (default: ~/.isis_mc_sig/)}
    \qualifier{dontClean}{do not delete temporary torque directory afterwards}
    \qualifier{chatty}{a number >0 means more chatty (default: 0)}
    \qualifier{ }{additional qualifiers are passed to fits_load_fit}
}
\description
    This function calculates the significance of a spectral component
    found in real data. During each Monte Carlo loop, spectra data
    without the component are simulated (for each detector) and this
    data are then fitted with a model containing the component (that
    needs to be tested for) and separately fitted with a model without
    it.
    The resulting simulated differences in chi square between these
    fits are returned and compared to the measured difference: the
    number of simulated chi squares below the measured one corresponds
    to the significance, that the spectral component is real (i.e. in
    80 cases out of 100 runs the simulated chi square difference is
    below the measured chi square difference, the significance is 80%).

    If two FITS-files created with fits_save_fit are provided, the
    first includes the model "with" the component to be tested and
    the second one "without" it.

    If only one argument is given, this has to be a temporary directory
    with results of a previous torque run (e.g. if "dontWait" was set).
    The function tries to collect and return the results as usual.

    The returned structure is defined as follows:
      readdchisqr  - the measured difference in chi square
      fakedchisqr  - an array of simulated differences in chi square
      nfalse       - the number of detected false positives
      significance - the resulting significance as defined above
\example
    % FITS-files created by fitting a cutoffpl and iron line to
    % RXTE-PCA, -HEXTE and Swift-XRT data (Rmf_OGIP_Compliance = 0 to
    % load XRT data, see help of fits_load_fit)
    sig = mc_sig("rxte_swift_cutoffpl_ironline.fits",
                 "rxte_swift_cutoffpl.fits";
                 mcruns = 1, ROC = [2,2,0],
                 beforeData = "defineMyModels.sl",
                 afterData = "setDataHooks.sl",
                 chatty = 2);
\seealso{fits_save_fit, fits_load_fit, fakeit}
\done

\function{median_2d}
\synopsis{computes the median image of a list of images (2d arrays)}
\usage{med_img = median_2d(img1, ...);}
\description
\done

\function{merge_struct_arrays}
\synopsis{creates a structure whose fields are merged from all structures' fields}
\usage{Struct_Type merged_s = merge_struct_arrays(Struct_Type s[]);}
\description
    All elements of s have to be structures with the same fields.
    The return value merged_s is another structure of this kind, and
    merged_s.field = [s[0].field, s[1].field, ..., s[-1].field];
    holds for every field. (If s[i].field is NULL, it is skipped
    unless "keep_null" qualifier is set.)
\qualifiers{
\qualifier{remove_excess_fields}{: remove fields not present in all
       structures.}
\qualifier{reshape = Integer_Type dim}{: reshape the merged fields to the
       dimensions of the original fields, using the sum of dimensiom 'dim'
       to account for the increased array. Care has to be taken that the
       other dimension need to have the same length in all structures. }
\qualifier{keep_null}{: keep fields which contain NULL.}
}
\seealso{append_struct_arrays, get_intersection, reshape}
\done

\function{message_system}
\usage{Integer_Type message_system(String_Type cmd)}
\description
    \code{message(cmd);}\n
    \code{system(cmd);}
\done

\function{midas_bary_helio_corr}
\usage{Struct_Type midas_bary_helio_corr(Y, m, d, H, M, S,  RAh, RAm, RAs,  decdeg, decmin, decsec)}
\description
    Y, m, d, H, M, S  specify the date in UT.

    The fields of the returned structure have the following meaning:\n
     bary_corr_time: Barycentric correction time, in days\n
    helio_corr_time: Heliocentric correction time, in days\n
       bary_RV_corr: Total barycentric RV correction, in km/s\n
      helio_RV_corr: Total heliocentric RV correction, in km/s\n
    diurnal_RV_corr: (incl.) diurnal RV correction, in km/s
\done

\function{_minimum}
\synopsis{computes the minimum of >=2 values}
\usage{Array_Type _minimum(Array_Type x1, x2, ...)}
\description
    \code{x1}, \code{x2}, ... have to be arrays of the same length, or scalars.
    The \code{_minimum} function returns an array of the minimum values
    by successively calling the \code{_min} function.
    If all \code{x1}, \code{x2}, ... are scalars, \code{_minimum} returns a scalar value, too,
    in this case, \code{_minimum(x1, x2, ...)} is equivalent to \code{min([x1, x2, ...])}.
\seealso{_min, min, _maximum, _max, max}
\done

\function{min_max}
\synopsis{returns the minimum and maximum value of an array}
\usage{(mn, mx) = min_max(Double_Type a[]);
\altusage{(mn, mx) = min_max(Double_Type a1[], a2[], ...);}
}
\qualifiers{
\qualifier{pad}{[=0] additive fraction to be padded on both sides:\n
                   \code{-dmin = dmax = (max-min) * pad}\n
                This qualifier may be overwritten by the following ones:}
\qualifier{dmin}{[=0] difference added to the minimum}
\qualifier{dmax}{[=0] difference added to the maximum}

\qualifier{logpad}{[=0] multiplicative fraction to be padded on both sides:\n
                      \code{fmin^-1 = fmax = (max/min)^logpad}\n
                   All array elements must be either positive or negative.
                   (For negative arrays \code{-A},  \code{[min_max(-A; logpad=lp)]}
                    is the same as  \code{array_reverse([min_max(A; logpad=lp))} .)
                   This qualifier may be overwritten by the following ones:}
\qualifier{fmin}{[=1] factor to multiply the minimum}
\qualifier{fmax}{[=1] factor to multiply the maximum}
}
\description
     \code{mn = min(a) * fmin + dmin;}\n
     \code{mx = max(a) * fmax + dmax;}
\seealso{min, max, moment}
\done

\function{min_struct_field}
\synopsis{returns the minimal field value of an array of structures}
\usage{Double_Type mn = min_struct_field(Struct_Type structs[], String_Type fieldname);}
\description
    \code{structs} is an array of structures containing the field called \code{fieldname}.
    \code{mn} can be computed in the following way:\n
    \code{mn = min( array_map(Double_Type, &min, array_map(Array_Type, &get_struct_field, [structs], fieldname)) );}
\seealso{max_struct_field, min_time, max_time}
\done

\function{min_time}
\synopsis{finds the earliest time in an array of structures with a time field}
\usage{Double_Type tmin = min_time(Struct_Type structs[]);}
\description
    \code{structs} is an array of structures containing the field \code{time}.
    \code{tmin} is computed in the following way:\n
    \code{tmin = min( array_map(Double_Type, &min, array_map(Array_Type, &get_struct_field, [structs], "time")) );}
\seealso{max_time, min_struct_field, max_struct_field}
\done

\function{missing_contour_trq}
\synopsis{calculates remaining contour points from function
    contour_trq. It is used in such cases where countour_trq does
    finnished before all point are calculated due to torque
    issues.} 
\usage{missing_contour_trq(String_Type inputDir);}
\description
    -\code{inputDir}   directory where all torque job files are,
                       created by function contour_trq

    It may be required to run the function several times.
    The output (number of files) needs to be checked manualy.
\example           
    missing_contour_trq("torqueFiles");
          
\seealso{contour_trq}
\done

\function{MJD2fermi}
\synopsis{calculate Fermi MET used for LC and spectral analysis}
\usage{Double_Type = MJD2fermi (Double_Type);}
\description
	Calculates the Fermi Mission Elapsed Time (MET)
	in seconds for a given MJD(UTC).
\example
	variable m = 54900.345;
	variable fermi_s = MJD2fermi(m);
\seealso{fermi2MJD, MJDref_satellite}
\done

\function{MJD2JD}
\synopsis{Convert Modified Julian Dates to Julian Dates.}
\usage{MJD2JD(MJD);}
\description
    Helper function to convert MJD to Julian Dates 
    by adding 2400000.5.
\seealso{jd2mjd, JD2MJD, MJD2JD}
\done

\function{mjd2jd}
\synopsis{Convert Modified Julian Dates to Julian Dates.}
\usage{mjd2jd(MJD);}
\description
    Helper function to convert MJD to Julian Dates 
    by adding 2400000.5.
\seealso{jd2mjd}
\done

\function{MJD2UNIXtime}
\synopsis{converts modified Julian date to UNIX time}
\usage{Long_Type MJD2UNIXtime(Double_Type MJD)}
\description
    Time formats and conversion functions:
      Long_Type secs = MJD2UNIXtime(Double_Type MJD);
      Double_Type MJD = UNIXtime2MJD(Long_Type secs);

      Struct_Type tm = localtime(Long_Type secs);  % or gmtime
      Long_Type secs = mktime(Struct_Type tm);

      String_Type str = strftime(String_Type fmt, Struct_Type tm);

    Current time and date:
      Long_Type secs = _time();
      String_Type str = time();
\seealso{UNIXtime2MJD, localtime, gmtime, mktime}
\done

\function{MJDofDate}
\synopsis{calculates the modified Julian date}
\usage{Double_Type MJD = MJDofDate( struct { year, month, day, hour, minute, second } );
\altusage{Double_Type MJD = MJDofDate(year, month, day[, hour[, minute[, second]]]);}
}
\description
    The Gregorian or Julian date is either given by a structure or by at
    least 3 integer arguments. Fractional parts are ignored. 
    The default values for hour, minute and second are 0, fractional
    hours etc. are properly taken into account.

    This function is array safe and equivalent to calling JDofDate with the
    mjd qualifier.
\qualifiers{
\qualifier{julian_calendar}{return the JD/MJD for the date in the Julian calendar rather
       the Gegorian date. This does not make much sense for the MJD, which was negative
       for the time when the Julian calendar was used.}
}
\seealso{JDofDate,dateOfMJD}
\done

\function{MJDofDateString}                                                                                         
\synopsis{calculates the MJD from a string including year, month, day [, hour [, minute [, second]]]}
\usage{Double_Type \code{mjd} = MJDofDateString(String_Type \code{date_string});}
\qualifiers{
\qualifier{verbose}{prints the found values for YYYY-MM-DD hh:mm:ss.sss}
\qualifier{no_2digit_correction}{see `parseDateString`}
}
\description
    This function calculates the Modified Julian Date from a String
    using the `parseDateString` function.
\example
    MJDofDateString("69/07/21");
    MJDofDateString("1969-07-21 02:56:30.44");
    MJDofDateString("sdf1969--X07..21QQ_02/56((30,,44xyz");
\seealso{parseDateString}
\done

\function{MJDref_satellite}
\synopsis{returns the reference date (MJD) of a satellite's time}
\usage{Double_Type MJDref_satellite(String_Type satellite)}
\done

\function{MJDstrftime}
\synopsis{formats a modified Julian date}
\usage{String_Type MJDstrftime(String_Type fmt, Double_Type MJD);}
\seealso{strftime, MJD2UNIXtime, localtime}
\done

\function{mkdir_rec}
\synopsis{Create a new directory (recursively)}
\usage{Int_Type mkdir_rec (String_Type dir [,Int_Type mode])}
\description
   Does the same as 'mkdir' with the exception that this
   function creates also subdirectories.
   See the help of 'mkdir' for a detailed description.
\seealso{mkdir}
\done

\function{MN_NFW}
\synopsis{Evaluate equations of motion, total energy, or circular velocity derived from a
    potential with a Miyamoto & Nagai bulge and disk component and a Navarro, Frenk,
    & White dark matter halo}
\usage{MN_NFW(Double_Types t, m[6,n]; qualifiers)}
\qualifiers{
\qualifier{coords}{[\code{="cyl"}] Use cylindrical ("cyl") or cartesian ("cart") coordinates.}
\qualifier{eomecd}{[\code{="eom"}] Return equations of motion ("eom"), total energy ("energy"),
      circular velocity ("circ"), or Sun-Galactic center distance ("sgcd").}
\qualifier{Mb}{[\code{=439}] Mass of bulge in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{Md}{[\code{=3096}] Mass of disc in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{Mh}{[\code{=142200}] Mass scale factor of halo in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{bb}{[\code{=0.236}] Bulge scale length, see Irrgang et al. 2013.}
\qualifier{ad}{[\code{=3.262}] Disc scale length, see Irrgang et al. 2013.}
\qualifier{bd}{[\code{=0.289}] Disc scale length, see Irrgang et al. 2013.}
\qualifier{ah}{[\code{=45.02}] Halo scale length, see Irrgang et al. 2013.}
}
\description
    Evaluate the equations of motion, the total energy, or the circular velocity at time 't'
    derived from a potential with a Miyamoto & Nagai bulge and disk component and a Navarro,
    Frenk, & White dark matter halo (see Model III in Irrgang et al., 2013, A&A, 549, A137)
    using either cylindrical coordinates (r [kpc], phi [rad], z [kpc]) and their canonical
    momenta vr [kpc/Myr], Lz [kpc^2/Myr], vz [kpc/Myr]) or cartesian coordinates (x [kpc],
    y [kpc], z [kpc], vx [kpc/Myr], vy [kpc/Myr], vz [kpc/Myr]), see qualifier 'coords'.
    Conservation of angular momentum Lz is implemented in the equations of motion for
    cylindrical coordinates only. The total energy E_total [kpc^2/Myr^2] is not used to
    integrate the equations of motion although being a conserved quantity, too. Therefore,
    conservation of energy, i.e., of E_total, is a measure for the precision of the numerical
    methods applied.

    For computing orbits with n different initial conditions, the input parameter m is
    a [6,n]-matrix with (qualifier("coords")=="cyl")   or (qualifier("coords")=="cart")
       m[0,*] = r;                                        m[0,*] = x;
       m[1,*] = phi;                                      m[1,*] = y;
       m[2,*] = z;                                        m[2,*] = z;
       m[3,*] = vr;                                       m[3,*] = vx;
       m[4,*] = Lz;                                       m[4,*] = vy;
       m[5,*] = vz;                                       m[5,*] = vz;
    If the qualifier 'eomecd' is set to "eom", the function returns a [6,n]-matrix delta with
       delta[0,*] = vr;                                   delta[0,*] = vx;
       delta[1,*] = Lz/r^2; % = vphi                      delta[1,*] = vy;
       delta[2,*] = vz;                                   delta[2,*] = vz;
       delta[3,*] = -d/dr (Potential(r,z) + Lz^2/r^2);    delta[3,*] = -d/dx Potential(x,y,z);
       delta[4,*] = 0; % -d/dphi Potential(r,z)           delta[4,*] = -d/dy Potential(x,y,z);
       delta[5,*] = -d/dz Potential(r,z);                 delta[5,*] = -d/dz Potential(x,y,z);
    If the qualifier 'eomecd' is set to "energy", the function returns a [1,n]-array storing
    the total energy for each orbit:
       E_total(r,z,vr,vz,Lz) = Double_Type[n] = 0.5*(vr^2+vz^2+Lz^2/r^2) + Potential(r,z)
    or
       E_total(x,y,z,vx,vy,vz) = Double_Type[n] = 0.5*(vx^2+vy^2+vz^2) + Potential(x,y,z)
    If the qualifier 'eomecd' is set to "circ", the function returns a [1,n]-array storing
    the circular velocity for each orbit:
       v_circ(r,z) = Double_Type[n] = sqrt( r * d/dr Potential(r,z) )
    If the qualifier 'eomecd' is set to "sgcd", the function returns the Sun-Galactic center
    distance found to fit best to this potential.
\example
    delta = MN_NFW(0, m);
    energy = MN_NFW(0, m; eomecd="energy");
    v_circ = MN_NFW(0, m; eomecd="circ");
    sgcd = MN_NFW(; eomecd="sgcd");
\seealso{orbit_calculator, AS, MN_TF, plummer_MW}
\done

\function{MN_TF}
\synopsis{Evaluate equations of motion, total energy, or circular velocity derived from a
    potential with a Miyamoto & Nagai bulge and disk component and a truncated, flat
    rotation curve halo model}
\usage{MN_TF(Double_Types t, m[6,n]; qualifiers)}
\qualifiers{
\qualifier{coords}{[\code{="cyl"}] Use cylindrical ("cyl") or cartesian ("cart") coordinates.}
\qualifier{eomecd}{[\code{="eom"}] Return equations of motion ("eom"), total energy ("energy"),
      circular velocity ("circ"), or Sun-Galactic center distance ("sgcd").}
\qualifier{Mb}{[\code{=175}] Mass of bulge in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{Md}{[\code{=2829}] Mass of disc in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{Mh}{[\code{=69725}] Mass of halo in Galactic mass units, see Irrgang et al. 2013.}
\qualifier{bb}{[\code{=0.184}] Bulge scale length, see Irrgang et al. 2013.}
\qualifier{ad}{[\code{=4.85}] Disc scale length, see Irrgang et al. 2013.}
\qualifier{bd}{[\code{=0.305}] Disc scale length, see Irrgang et al. 2013.}
\qualifier{ah}{[\code{=200}] Halo scale length, see Irrgang et al. 2013.}
}
\description
    Evaluate the equations of motion, the total energy, or the circular velocity at time 't'
    derived from a potential with a Miyamoto & Nagai bulge and disk component and a truncated,
    flat rotation curve dark matter halo model (see Model II in Irrgang et al., 2013, A&A,
    549, A137) using either cylindrical coordinates (r [kpc], phi [rad], z [kpc]) and their
    canonical momenta vr [kpc/Myr], Lz [kpc^2/Myr], vz [kpc/Myr]) or cartesian coordinates
    (x [kpc], y [kpc], z [kpc], vx [kpc/Myr], vy [kpc/Myr], vz [kpc/Myr]), see qualifier
    'coords'. Conservation of angular momentum Lz is implemented in the equations of motion
    for cylindrical coordinates only. The total energy E_total [kpc^2/Myr^2] is not used to
    integrate the equations of motion although being a conserved quantity, too. Therefore,
    conservation of energy, i.e., of E_total, is a measure for the precision of the numerical
    methods applied.

    For computing orbits with n different initial conditions, the input parameter m is
    a [6,n]-matrix with (qualifier("coords")=="cyl")   or (qualifier("coords")=="cart")
       m[0,*] = r;                                        m[0,*] = x;
       m[1,*] = phi;                                      m[1,*] = y;
       m[2,*] = z;                                        m[2,*] = z;
       m[3,*] = vr;                                       m[3,*] = vx;
       m[4,*] = Lz;                                       m[4,*] = vy;
       m[5,*] = vz;                                       m[5,*] = vz;
    If the qualifier 'eomecd' is set to "eom", the function returns a [6,n]-matrix delta with
       delta[0,*] = vr;                                   delta[0,*] = vx;
       delta[1,*] = Lz/r^2; % = vphi                      delta[1,*] = vy;
       delta[2,*] = vz;                                   delta[2,*] = vz;
       delta[3,*] = -d/dr (Potential(r,z) + Lz^2/r^2);    delta[3,*] = -d/dx Potential(x,y,z);
       delta[4,*] = 0; % -d/dphi Potential(r,z)           delta[4,*] = -d/dy Potential(x,y,z);
       delta[5,*] = -d/dz Potential(r,z);                 delta[5,*] = -d/dz Potential(x,y,z);
    If the qualifier 'eomecd' is set to "energy", the function returns a [1,n]-array storing
    the total energy for each orbit:
       E_total(r,z,vr,vz,Lz) = Double_Type[n] = 0.5*(vr^2+vz^2+Lz^2/r^2) + Potential(r,z)
    or
       E_total(x,y,z,vx,vy,vz) = Double_Type[n] = 0.5*(vx^2+vy^2+vz^2) + Potential(x,y,z)
    If the qualifier 'eomecd' is set to "circ", the function returns a [1,n]-array storing
    the circular velocity for each orbit:
       v_circ(r,z) = Double_Type[n] = sqrt( r * d/dr Potential(r,z) )
    If the qualifier 'eomecd' is set to "sgcd", the function returns the Sun-Galactic center
    distance found to fit best to this potential.
\example
    delta = MN_TF(0, m);
    energy = MN_TF(0, m; eomecd="energy");
    v_circ = MN_TF(0, m; eomecd="circ");
    sgcd = MN_TF(; eomecd="sgcd");
\seealso{orbit_calculator, AS, MN_NFW, plummer_MW}
\done

\function{mpi_fit_pars}
\synopsis{computes confidence intervals using MPI}
\usage{Struct_Type results = mpi_fit_pars([Integer_Type pars[]]);}
\description
    The function \code{mpi_fit_pars} is designed to provide a similar
    interface then \code{pvm_fit_pars}, written by M. Hanke for the
    isis-scripts. Please pay special attention to the notes marked IMPORTANT
    below.

    \code{pars} is an array of parameters, for which the confidence levels are to be fitted.
    If \code{pars} is not specified, all free parameters of the current model are used.
    The best fit which is eventually found is always saved in
    <\code{dir}>/<\code{basefilename}>\code{_best.par}; the confidence limits are
    written in plain-text to <\code{dir}>/<\code{basefilename}>\code{_conf.txt} and as a
    FITS table to <\code{dir}>/<\code{basefilename}>\code{_conf.fits}.

    The verbosity of \code{mpi_fit_pars} is controlled by the intrinsic variable \code{Fit_Verbose}.

    The return value \code{results = struct { index, name, value, min, max, conf_min, conf_max, buf_below, buf_above, tex }}
    is a table with the following information for each parameter:\n
    \code{min} and \code{max} are the minimum/maximum values allowed.
    \code{conf_min} and \code{conf_max} are the confidence limits.
    \code{buf_below} (\code{buf_above}) is the fraction of the allowed range \code{[min:max]}
    which seperates the lower (upper) confidence limit from \code{min} (\code{max}).
    If one of these buffers is 0, your confidence interval has bounced.

    Perhaps more usefully, this information is written to the files
    <\code{dir}>/<\code{basefilename}>\code{_conf.txt} and
    <\code{dir}>/<\code{basefilename}>\code{_conf.fits}.  In case of any
    error, the return value is \code{NULL}. If run via slurm/torque, these
    files will only be written by the host node once all threads have
    completed - i.e., there's no danger of the results being written by
    several machines at the same time.

    NOTE: The function "mpi_new_best_fit_hook()" can be defined by the
    user. This function will be called whenever a new best fit is found. This
    can be used to, e.g, store the newly found best fit (although the usual
    caveats about writing files to disk while running via MPI still apply...).

    IMPORTANT: Rember that the whole script containing the call
    "mpi_fit_pars()" is typically run on N > 1 computers. Therefore only the
    *minimum* amount of code necessary to run mpi_fit_pars() should be
    included in the script.

    IMPORTANT: This function relies on MPI. This means that it only
    works when started externally via, e.g., \code{mpiexec isis-script
    my_script.sl} or via a slurm jobfile. Slurm or mpiexec must be able to
    start as many processes as you have parameters for which you want to
    calculate confidence limits (e.g., if you want error bars on four
    parameters, you need four tasks - no more, no less).

    IMPORTANT: This code is still in <beta> test. It might happen
    that the function ends unexpectedly.
\qualifiers{
\qualifier{level}{confidence level to be computed.
             As for conf, 0 means 68%, 1 means 90% [default], and 2 means 99% confidence level.}
\qualifier{tolerance}{the tolerance for chi^2 improvements without interrupting the search
                 for the confidence intervals, see \code{help("conf");}. The default is \code{1e-3}.}
\qualifier{fitmethod}{fit-method to be used, see \code{help("set_fit_method");}
                 Default is the currently used fit-method returned by \code{get_fit_method()}.}
\qualifier{dir}{[="."] specifies the directory in which the logfiles shall be stored.
                  It may be a relative path to the current working directory.}
\qualifier{basefilename}{[=startdate]}
\qualifier{verbose}{Files with the stdout and the inital parameters are kept after program
                     is finished. }
\qualifier{forked}{[=0] Use a separate (forked) process to
                  calculate the confidence levels. It speeds up the calculation, but
                  might cause troubles for models which write files to disk, as this
                  process is killed  immediatelly when asked to restart
                  calculation and not stopped smoothly. This option can also
                  cause your job to be killed if run via slurm, as it requests
                  additional threads beyond those allocated by slurm.  }
\qualifier{do_not_finalize}{if set, "mpi_finalize()" is not called
                  within the routine. In this case the code following the
                  mpi_fit_pars() call will be executed on *all* nodes. Only the
                  master process will return the confidence structure (see above);
                  all other processes will return NULL. This can be used to start
                  more than one mpi_fit_pars() in one script. Only use it for very
                  quick evaluations and if you *know that you're doing*. At the end
                  of such an script, it is important to call "mpi_finalize()" manually.}
\example
     % EXAMPLE 1
     % Simple script for confidence level calcualtion with mpi_fit_pars()
     variable id = load_data("my_data.pha");   % load the data
     xnotice_en(id,0.5,16);                    % initialize data
     load_par("my_best_fit.par");

     variable result = mpi_fit_pars(pars);     % do calculation for pars[]
     fits_save_fit("result.fits",result);      % save all information
                                               % to disk
                                               % Note that mpi_fit_pars() alone will save information to disk,
                                               % by default in files named <date>_best.par, <date>_conf.txt,
                                               % and <date>_conf.fits

     % EXAMPLE 2 - Uses slurm to schedule jobs on the Remeis cluster.
     % This requires two files - mpi_fit.sl, containing the ISIS code to be
     % executed, and mpi_fit.slurm, containing the job specification for slurm.
     % This would be run by calling "sbatch mpi_fit.slurm" at the command line.
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     % ISIS script (mpi_fit.sl)
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     variable id = load_data("my_data.pha");   % load the data
     xnotice_en(id,0.5,16);                    % initialize data
     load_par("my_best_fit.par");
     variable pars = [1,2];                    % obviously you will pick whatever parameters you need here
     () = mpi_fit_pars(pars;                   % Do the confidence limit calculations. Results will be saved to
       basefilename="my_fit");                 % "my_fit_conf.fits","my_fit_conf.txt", and "my_fit_best.par".
                                               % Note that unlike the above example, we do not manually save
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%            % the results to a file - mpi_fit_pars does this on its own.
     % Jobfile (mpi_fit.slurm)
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     #!/bin/bash
     #SBATCH --job-name my_fit                 # name of the job to be run by slurm
     #SBATCH --time 00:10:00                   # You might need more (or less) time.
     #SBATCH --ntasks 2                        # we want erors on two parameters (see above), so we need 2 cores
     #SBATCH --mem-per-cpu 1G                  # 1 gig of memory per core (tweak to your liking)
     srun /usr/bin/nice -n +19 isis mpi_fit.sl # run the script with srun (and nice +19 because you don't 
                                               # want to bog down someone else's machine)
                                               
}
\seealso{fit_pars; conf, set_fit_method, pvm_fit_pars}
\done

\function{mpi_fit_pars_trqscript_gen}
\synopsis{Creates torque scripts for derivation of confidence intervals}
\usage{mpi_fit_pars_trqscript_gen(String_Type fitfile, String_Type resultfile, String_Type scriptfile)}
\description
    This function generates 1) an executable S-Lang file
    that is loading a previous fit saved with \code{fits_save_fit}, 
    calling \code{mpi_fit_pars} with the current number of free parameters
    and saving the result via \code{fits_save_fit} and \code{save_pars},
    2) writing the .cmd and 3) the .cmd.job file
    that can be submitted to Torque via 'qsub *.cmd.job'.

    If only two arguments are given
    (the resultfile and the scriptfile), the function expects
    that you want to calculate the uncertainties out of isis
    In that case the function will itself save the fit to a
    file in the results-directory. When mpi_fit_pars is finished,
    you have to delete it by your own.

    ATTENTION: make sure that the min/max parameter borders
    are set wide enough in your previous fit but still with
    a reasonable width to allow mpi to find the best fit and
    ncertainties in a reasonable amount of time.

    NOTE: All given qualifiers are passed to the function
    fits_load_fit. 
\qualifiers{
\qualifier{walltime}{[="00:30:00"]: String_Type, wallime for the torque}
\qualifier{ROC}{Rmf OGIP Compliance (see fits_load_fit}
\qualifier{submit}{1-submit to torque, 0-only create job files}%
}
\example
    isis>

    isis> mpi_fit_pars_trqscript_gen("input.fits","result.fits","trq_scripts/mpi_script.sl";
                                      walltime="01:00:00")
    OR 
    isis> mpi_fit_pars_trqscript_gen("result.fits","trq_scripts/mpi_script.sl";
                                      walltime="01:00:00")

    THEN
    
    qsub trq_scripts/mpi_script.cmd.job

\seealso{fits_save_fit, mpi_fit_pars, fits_list_fit_pars}
\done

\function{mpi_master_only}
\synopsis{Use function with on one host only in an MPI job}
\usage{mpi_master_only (Reference_Type function, arg1, arg2, ...)}
\qualifiers{
\qualifier{verbose}{show name of host which acts as MPI master}
\qualifier{veryverbose}{show name of host which acts as MPI master, PID and parent PID (overwrites verbose)}
}
\description
	Sometimes, for example by saving files, it is required that a function is only called once
	in an MPI job. mpi_master_only does this for a function with an arbitrary number of arguments.
	The function has to be passed by reference, the arguments simply as arguments.
\example
	mpi_master_only(&sprintf, "Pi is approximately %f and Eulers number is %f", PI, E);
	output:
		Pi is approximately 3.141593 and Eulers number is 2.718281828459045
	But this only once
\done

\function{mpi_mc_sig}
\synopsis{calculates the significance of a spectral component doing a Monte Carlo (MC) simulation}
\usage{Struct_Type mpi_mc_sig(String_Type withComponent.fits, String_Type withoutComponent.fits [, String_Type results.fits]);}
\qualifiers{
    \qualifier{mcruns}{number of MC loops (default: 10)}
    \qualifier{beforeData}{script to be called before any data is loaded (default: NULL)}
    \qualifier{afterData}{script to be called after the data have been loaded (default: NULL)}
    \qualifier{beforeFit}{array of scripts to be called right before a fit of faked data to
                 the model without [0] and with [1] the component (default: NULL)}
    \qualifier{id}{override the dataset ID(s) after fits_load_fit with the given one(s)}
    \qualifier{ignbinning}{do not apply the same binning after faking the data}
    \qualifier{ignnotice}{do not use the same energy ranges after faking the data}
    \qualifier{chatty}{a number >0 means more chatty (default: 0)}
    \qualifier{seed}{seed for random number generator}
    \qualifier{ }{additional qualifiers are passed to fits_load_fit}
}
\description
    This function calculates the significance of a spectral component
    found in real data. During each Monte Carlo loop, spectra data
    without the component are simulated (for each detector) and these
    data are then fitted with a model containing the component (that
    needs to be tested for) and separately fitted with a model without
    it.
    The resulting simulated differences in chi-square between these
    fits are returned and compared to the measured difference: the
    number of simulated chi squares below the measured one corresponds
    to the significance, that the spectral component is real (i.e. in
    80 cases out of 100 runs the simulated chi square difference is
    below the measured chi square difference, the significance is 80%).

    Two FITS-files created with fits_save_fit must be provided, the
    first includes the model "with" the component to be tested and
    the second one "without" it. An optional third filename may be given
    to store the results to. Otherwise, a default file will be created.

    This function is supposed to be used for parallel computation with
    MPI. The mcruns qualifier specifies the total number of MC runs which
    is split accordingly among the available nodes.

    The function will create a FITS file containing the following extensions:
      realdeltachisquare - the measured difference in chi square
      fakedeltachisquare - an array of simulated differences in chi square
      falsepositives     - the number of detected false positives
      significance       - the resulting significance as defined above
\example
    % FITS-files created by fitting a cutoffpl and iron line to
    % RXTE-PCA, -HEXTE and Swift-XRT data (Rmf_OGIP_Compliance = 0 to
    % load XRT data, see help of fits_load_fit)

     mpi_mc_sig("rxte_swift_cutoffpl_ironline.fits",
                "rxte_swift_cutoffpl.fits",
                "significance.fits";
                 mcruns = 100, ROC = [2,2,0],
                 beforeData = "defineMyModels.sl",
                 afterData = "setDataHooks.sl",
                 chatty = 2);
    % A typical call from the command line could look like this

    mpiexec -n 4 isis <script>

    to use 4 cores for the MC run. However, it is highly
    recommended to use a job scheduling system (e.g., SLURM)
    for these kind of simulations.
                              
\seealso{fits_save_fit, fits_load_fit, fakeit, mc_sig}
\done

\function{multibknpower (fit-function)}
\synopsis{implements a multiply-broken powerlaw}
\description
    Only break energies >0 and the corresponding photon indices are considered.
    \code{PhoIndx0} applies for energies below the first break.
    Each \code{PhoIndex}i applies for energies above \code{BreakE}i, but below the next break.
    As for \code{bknpower}, the normalization is the flux of the first power law at 1 keV
    (not necessarily the broken power law model)  in ph/s/cm^2/keV.
\done

\function{multiplot_flux_counts_res}
\synopsis{creates a plot of model flux, data counts and residuals in 3 panels}
\usage{multiplot_flux_counts_res(data_ind[, rel_size]);}
\description
    If data_ind is an array of data-indices, the data will be rebinned
    to the grid of the first index.
\done

\function{narrowline (fit-function)}
\synopsis{multiplicative model; implements unresolved absorption/emission line}
\description
   simple multiplicative line-function to efficiently model
   unresolved absorption and emission lines. Allows to direclty fit
   the equivalent width of the line.

   Caution: the function does not check whether the line is
   unresolved; user needs to take care of that before using the
   function. In particular, for absorption lines the absolute value
   of the equivalent width should be smaller than the resolution of
   the data.

   The fit parameters are

      center [A]   line position
      eqw    [A]   equivalent width of the line
\done

\function{new_plot_labels}
\synopsis{Create new plot labels for routines in isis_fancy_plots package.}
\usage{new_plot_labels(String_Type [, String_Type]);}
\qualifiers{
\qualifier{xlabel}{New X-axis label}
\qualifier{ylabel}{String array with new Y-axis labels for data}
\qualifier{rlabel}{String array with new Y-axis labels for residuals}
\qualifier{clabel}{String array with new Y-axis residual labels for Cash statistics}
\qualifier{mllabel}{String array with new Y-axis residual labels for Maximum Likelihood statistics}
\qualifier{vlabel}{String with new Doppler velocity X-axis label}
\qualifier{zlabel}{String with new redshift X-axis label}
\qualifier{pg_font}{PGPLOT font type (default is \\\\fr)}
}
\description

 new_plot_labels(x_unit [,y_unit];xlabel=string,ylabel=[string],...,
                                   pg_font=string);

 Changes the default labels for plot_counts, plot_data, plot_unfold
 for the different units set by fancy_plot_unit. pg_font string will
 be *prepended* to all inputs. Use any valid pair of units to set residual,
 Cash statistic, or velocity/redshift axis labels.

 Use: 

    set_plot_labels(;pg_font="\\\\fr") -or- set_plot_labels(;pg_font=``\\\\fr``)

 to restore defaults.

 Inputs:

   x_unit : angstrom,a,nm,um,micron,cm,mm,m, ev,kev,mev,gev,tev,
            hz,khz,mhz,ghz, psd
   y_unit : photons (=default), ergs, watts, mjy, psd_leahy, or psd_rms
   xlabel : String with new X-axis label
   ylabel : String *array* (up to 9 elements) with new Y-axis labels.
            Order of the array *must* be: 
             [plot_data, plot_unfold(power=0->3),plot_counts(power=0->3)]
   rlabel : String *array* (up to 3 elements) with new Y-axis labels for
            residuals.  Order of the the array *must* be: 
             [chi, chi^2, ratio]
   clabel : String Array with new Cash statistic labels [res=1 or 4, 2 or 5,
            which yield +/-sqrt(|Delta C|), Delta C, respectively]
   mllabel: String Array with new Maximum Likelihood statistic labels [res=1 or 4, 
            2 or 5, which yield +/-sqrt(|Delta ML|), Delta ML, respectively]
   vlabel : String with new Doppler velocity X-axis label
   zlabel : String with new Redshift X-axis label
   pg_font: "\\\\fn", "\\\\fr", "\\\\fi", "\\\\fs" = normal, roman, italic,
            or script fonts will be used on the labels (Default is \\\\fr.)
\seealso{set_plot_labels, fancy_plot_unit, add_plot_unit}
\done

\function{new_printplot}
\synopsis{returns a new printplot-structure}
\usage{Struct_Type new_printplot(Double_Type x0, x1, y0, y1);}
\qualifiers{
    \qualifier{W, H}{the width, i.e. the number of columns,
                 and the height, i.e. the number of rows,
                 of the plotting area without any axes or
                 ticmarks (default: 60 and 10)}
    \qualifier{[x,y]axis}{if set to zero do not draw the axis}
    \qualifier{[x,y]tics}{if set to zero do not draw the ticmark}
    \qualifier{[x,y]format}{sprintf format of the tics
                 (default: "%f")}
    \qualifier{[x,y]strlen}{maximum string length of the ticmarks
                 (default: 4)}
}
\description
    For printing a simple plot into the terminal a char
    matrix is returned by this function, which can be
    later printed by 'printplot_out'.
    Per default, the matrix is 20x4 in size. Changing its
    size can be done by the W- and H-qualifiers. The
    arguments specify the x- and y-ranges of the plotting
    area. The minimum and maximum values are the only
    ticmarks which are added. Note that the char-matrix
    is enlarged to fit the axes and ticmarks.
\seealso{printplot_out, printplot, printhplot}
\done

\function{new_table_model}
\synopsis{Initialize object and file to create table model}
\usage{Struct_Type model = new_table_model(String_Type filename)}
#c%{{{
\qualifiers{
  \qualifier{name}{[=<filename-no-ext>] set model name}
  \qualifier{double}{if set, stores all values as doubles instead of floats}
  \qualifier{overwrite}{if set, ignores existing file with same name}
}
\description
  Giving a file name the function creates a fits file with the given
  name and returns a handle that eases the process of creating a
  table model according to the XSPEC format
     OGIP Memo 92-009 (XSPEC Table Model File Format)

  To create a file one has to give the interpolation points of the
  model, the additional contributions and the model spectra.

\example
  variable table = new_table_model("test_model.fits");

  % create "parameter1" with given interpolation points
  table.add_parameter("parameter1", [0:2:0.1]);
  table.add_parameter("parameter2", [-1:1:0.001]); % create "parameter2"
  % create additional parameter "additional"
  table.add_additional("additional");

  table.set_grid(bin_lo, bin_hi); % define the valid energy grid for the model
  table.set_function(&my_fancy_model); % set the model function
  % set additional contributions
  % (only necessary if additional parameters are given)
  table.set_additional(&my_fancy_contributions);

  table.write(); % fill table
  table.close(); % finish table
\seealso{table_model.add_parameter, table_model.add_additional,
  table_model.set_grid, table_model.set_function, table_model.set_additional,
  table_model.write, table_model.close}
\done

\function{nice_width}
\synopsis{Sets reasonable defaults for plot line widths.}
\usage{nice_width;}
\description

  Equivalent to:
    isis> set_plot_widths(;d_width=2, de_width=2, r_width=2, re_width=2, m_width=2);
\seealso{set_plot_widths, apj_size, keynote_size, open_print, close_print}
\done

\function{normalized_in_0_1}
\synopsis{computes a normalized array with min=0, max=1}
\usage{Double_Type a_norm[] = normalized_in_0_1(Double_Type a[]);}
\description
    \code{a_norm = (a - min(a)) / (max(a)-min(a));}
\done

\function{normpsd}
\usage{psd_norm = normpsd(psd, normtype, avrate, avbackrate, timeseg, dimseg);}
\description
    \code{psd} is the unnormalized power spectrum.
    \code{avrate} is the average countrate of corresponding lightcurve.
    \code{avbackrate} is the average background countrate of corresponding lightcurve.
    \code{timeseg} is the real time of one corresponding lightcurve segment.
    \code{dimseg} is the number of bins in one lightcurve segment.

    String_Type \code{normtype} can be:\n
    - "Miaymoto" (default) [Miyamoto et al. (1991), ApJ 383, 784]\n
      \code{psd *= 2 * timeseg / (dimseg * [avrate-avbackrate]^2)}\n
    - "Leahy" [Leahy et al. (1983), ApJ 266, 160]\n
      \code{psd *= 2 * timeseg / (dimseg^2 * avrate)}\n
    - "Schlittgen" [Schlittgen, H.J., Streitberg, B. (1995), Zeitreihenanalyse, R. Oldenbourg]
      \code{psd /= dimseg}\n
\done

\function{notice_human2isis}
\synopsis{create isis notice_list from str = notice_isis2human}
\usage{Array_Type notice_list = notice_human2isis(String_Type str);}
\description
   This routine converts a notice_list string created with
   notice_isis2human back to the ISIS conform binning array
   (see e.g. get_data_info(idx).notice_list). It can be used to
   restore the otcing of data directly, by using it as argument
   in the function notice_list(...).
   
\seealso{fits_save_fit, get_data_info, notice_isis2human}
\done

\function{notice_isis2human}
\synopsis{converts isis notice_list in a human readable string}
\usage{String_Type str = notice_isis2human(Array_Type notice_list);}
\description
   This routine converts the notice_list as used by ISIS (see e.g. 
   get_data_info(idx).notice_list) to a string which is easy to read.
   
   The routine fits_save_fit() uses this string to save the noticed
   bins of the data. It can be converted back by using 
   notice_human2isis().
\seealso{fits_save_fit, get_data_info, notice_human2isis}
\done

\function{notice_listFromString}
\synopsis{creates a isis notice_list from a string containing the
noticed bins separated with ",".}
\usage{Array_Type notice = notice_listFromString(String_Type str);}                
\seealso{notice_list, notice_listToString, get_data_info, fits_save_fit}       
\done

\function{notice_listToString}
\synopsis{converts an isis notice_list to a string}
\usage{String_Type str = notice_listToString(Array_Type notice_list);}                
\seealso{notice_list, notice_listFromString, get_data_info, fits_save_fit}       
\done

\function{notice_xy}
\synopsis{notice points from xy-dataset}
#c%{{{
\usage{notice_xy (index [, low, high]);}
\description
    Wraper function of the notice function for datasets defined with
    define_xydata. Include data points of dataset \code{index}
    (in the range low to high) for fitting.
\seealso{ignore_xy, notice, notice_en, define_xydata}
\done

\function{ntree}
\synopsis{creates a new node of a tree with n children}
\usage{Struct_Type ntree(DataType_Type objectType[, Struct_Type ntreeType]);
 or Struct_Type ntree(DataType_Type objectType[, Integer_Type numChilds]);}
\description
  An n-tree is a special kind of a tree data structure.
  Each node has exactly n children, which are n-tree
  nodes themselves. The number of children is specified,
  e.g., by the 'numChilds' parameter, which is 2 by
  default. Initially, all children are null pointers.
  Most importantly, a node contains a list of objects,
  which is an array of the type specified by the
  'objectType' parameter. Using the functions provided
  in the n-tree structure, objects can be inserted or
  removed.
  
  The n-tree structure contains the following functions:
    insert    - appends an object to the list.
                Usage: ntree.insert(objectType object);
    remove    - removes an object from the list and
                returns 1 on success, 0 otherwise.
                Usage: Integer_Typer ntree.remove(objectType object);
    get       - returns all objects stored in the node.
                Usage: objectType[] ntree.get();
    getChilds - returns all objects stored in the children.
                Usage: objectType[] ntree.getChilds();
    getAll    - iterates over all nodes recursively to
                return all objects stored in the tree.
                Usage: objectType[] ntree.getAll();
    childN    - returns the N's child of the actual node.
                If the child does not exist, NULL will be
                returned. The function names can be
                overwritten by the type definition (see
                below).
                Usage: Struct_Type ntree.childN();
                Qualifiers:
                  new    - creates the child if it does
                           not exist
                  renew  - creates a new child no matter
                           if it exists already
                  delete - deletes the child

  In addition, the following fields exist:
    objects   - array of objects stored in the node
    childs    - array of length N containing the
                children of the node
    type      - structure describing the type of the
                n-tree (see below)

  The type of the n-tree can be specified by the
  optional 'ntreeType' parameter. This structure
  defines additional behaviours of the n-tree, and
  the number of children and their the return function
  names eventually. The default type sets these names
  to the ones described above (childN). Additional
  informations about the usage of n-tree types can
  be found in the help of the 'ntree_type' function.
\example
  % creates an 4-tree (called quad tree) with one
  % initial node and an object array of Integer_Type
  tree = ntree(Integer_Type, 4);

  % insert '5' into the initial node
  tree.insert(5);

  % create and insert '10' into the third child
  tree.child3(; new).insert(10);

  % checks if child2 exists
  if (tree.child2() == NULL)
    message("child2 does not exist");

  % insert '8' into child3, but it is not
  % created again since it exists already
  tree.child3(; new).insert(8);

  % get all objects of the children
  % (will return [10,8])
  print(tree.getChilds());
\seealso{ntree_type}
\done

\function{ntree_type}
\synopsis{returns a specific stucture defining an n-tree type}
\usage{Struct_Type ntree_type(String_Type typeName); }
\description
  This functions returns the structure defining
  the specific n-tree type named 'typeName'. A
  list of all available types is shown if no
  name is passed. The 'help' qualifier shows
  additional information about the given type.

  The type of an n-tree is specified and can be
  extended by a structure with the following
  fields:
  
  numChilds
    Defines the number of children of each node.
    node. It has the same effect as the
    'numChilds' parameter of the 'ntree' function.

  childNames (optional)
    Array of strings defining the return function
    names for each children. The default names are
    "child1" to "childN".

  new (optional)
    Reference to a callback function, which gets
    called right before a new node created by
    'ntree()' is returned. Parameters passed are
    (1) the new node as a structur, (2) as which
    child number it is created (starting at 1,
    NULL if it is the root) and (3) the parent
    node it belongs to (NULL if there is none).
    The function has to return the new node.
    
  insert (optional)
    Reference to a callback function, which gets
    called before an object is inserted into a
    node by 'node.insert()'. Parameters passed are
    (1) the node as a structure and (2) the object,
    which should be inserted. The function has to
    return the node. If NULL is returned, the
    object will not be inserted into the node.

  remove (optional)
    Reference to a callback function, which gets
    called before an object is removed from a node
    by 'node.remove()'. Parameters passed are (1)
    the node as a structure and (2) the object,
    which should be removed. The function has to
    return the node. If NULL is returned, the
    object will not be removed from the node.
\example
  % first define a callback function, which
  % prevents inserting negative numbers
  define ntree_myType_insert(node, object) {
    if (object < 0) return NULL;
    else return node;
  }

  % structure defining a 4-tree (called quadtree)
  % and the children return functions are renamed
  % to match directions. The insert callback
  % function defined above is assigned also.
  variable myType = struct {
    numChilds = 4,
    childNames = ["left","right","top","bottom"],
    insert = &ntree_myType_insert
  };

  % create a new n-tree of myType
  variable tree = ntree(Double_Type, myType);
\seealso{ntree}
\done

\function{ntree_type_mapQuad}
\synopsis{returns the n-tree definition of a map quadtree}
\usage{Struct_Type ntree(DataType_Type objectType, ntree_type("mapQuad"), Double_Type size[, Double_Type x0, y0]); }
\description
  A map quadtree is a 4-tree, which divides a 2d-space
  into squares with an edge length of 'size'. Therefore,
  each node of the tree is identified by a certain
  coordinate x,y. The coordinate of the tree's root can
  be specified by the x0 and y0 parameters and are (0|0)
  by default. The children are named "top", "right",
  "bottom" and "left". Due to that structure the tree
  will be an undirected graph, meaning that, e.g., the
  left node is connected to the right node and vise versa:

   -------            ----            -------
  |x0-size| -right-> | x0 | -right-> |x0+size|
  |  y0   | <--left- | y0 | <--left- |  y0   |
   -------            ----            -------
                       ^|
                   top || bottom
                       |v
                    -------
                   |   x0  |
                   |y0-size|
                    -------

  The map quadtree extends the n-tree structure by the
  following functions:
    lookup(x,y) - returns the node fitting the given
      x and y coordinates
    insert(object[,x,y]) - overwrites the usual insert
      function such, that the object is inserted into
      the node fitting the optional x and y coordinates

  Note: deleting a child by the 'delete' qualifier,
    e.g. tree.left(; delete) does NOT cascade! That is,
    the left node still points to the right one (here
    called tree). It might still be accessable via
    tree.top().left().bottom()
\example
\seealso{ntree, ntree_type}
\done

\function{nuFnu}

\synopsis{changes an Energy/Flux Spectrum to a Frequency/Flux*Frequency Spectrum}
\usage{Struct_Type = nuFnu (hist_index, E_min(keV), E_max(keV));}

\description
    Use this function on a dataset that has been modeled.
    This function uses get_data_flux/eval_fun_keV to load
    spectral data into isis and convert energy bins in a 
    given range to frequency (s^-1) bins as well as Flux 
    to flux*frequency.
    If no range specified: E_min=0.5 keV, E_max=10 keV.
    Values and Errors are given in erg s^-1 cm^-2.

    WARNING: Currently only frequency has been implemented 
             as x-unit.
             
    For the evaluation of a deabsorbed component use the 
    qualifier "deabs". deabs will try to calculate the 
    factor between absorbed and deabsorbed model 
    components and correct the data.

    WARNING: This does not work when a convolution 
             normalization (e.g., cflux, enflux, phflux)
             is part of the model!

    The "numbin" qualifier allows to specify the number
    of output bins. Currently only a logarithmic grid 
    has been implemented. If numbin is not specified 
    the flux for the current grid will be returned.

    The "group" qualifier allows to specify the S/N that
    will be used for the flux calculation. Flux calculations
    are only correct for large number of bins. The final 
    grid can be smaller (qualifier: numbin).


    WARNING: Currently tested only on low counts spectra.
             S/N might have to be increased for large 
             number of counts.

\qualifiers{
\qualifier{deabs}{   evaluate deabsorbed flux}
\qualifier{ff}{      define deabsorbed fit function}
\qualifier{numbin}{  define number of bins}
\qualifier{group}{  define S/N for grouping}
}

\example
    isis>xray = load_data("data.pha");
    isis>load_par("x.par");
    isis>() = fit_counts;
    isis>new=nuFnu(xray,2,10);	 %change spectrum (2-10 keV) into frequency spectrum

    %Plot data:
    isis>hplot(new.lo, new.hi, new.val);

    % Evaluate Function for deabsorbed model
    % (WARNING: DOES NOT WORK IF ENFLUX IS USED IN FIT FUNCTION)

    isis>xray = load_data("data.pha");
    isis>fit_fun("tbnew_simple_z(1)*powerlaw(1)");
    isis>() = fit_counts;
    isis>new = nuFnu(xray,2,10; deabs, ff="powerlaw(1)");
    isis>%Calculate a deabsorbed powerlaw

\seealso{get_data_flux, rebin_mean, eval_fun_keV, group}
\done

\function{num_bin}

\synopsis{Number of noticed bins}
\usage{Double_Type = num_bin (hist_index);}
\description
       Use this function to retrieve number of
       noticed bins.
\qualifiers{
    \qualifier{dof}{gives degrees of freedom instead of number of bins}
}
 
\example
       isis>xray = load_data("data.pha");
       isis>variable num = num_bin(1);

\seealso{dof}
\done

\function{nutation_angles}
\synopsis{calculate the nutation in longitude and obliquity for a given date}
\usage{ (dpsi,deps)=nutation_angles(JD;qualifiers)}
\qualifiers{
   \qualifier{JD_frac}{fractional part of the Julian Date (for highest precision)}
   \qualifier{theory}{nutation theory to use. Possible string values are
           IAU2000A - full IAU2000A theory, i.e., an implementation of the
                      theory of Matthews et al. (2002, J. Geophys. Res, 107, 2068)
           IAU2000B - truncated IAU2000A theory per McCarthy & Luzum (2003,
                      Cel. Mech. Dyn. Astron, 85, 37); accuracy is
                      1 mas in 1995-2050
           NU2000K  - truncated IAU2000A theory from Kaplan (2009, USNO
                      circular 181); accuracy is 0.1mas in 1700-2300
                      [the default]}
   \qualifier{mjd}{the time argument is in MJD, not in JD}
   \qualifier{JD_frac}{fractional part of the time argument. Use this if
                     highest precision is needed. The nutation angles are then
                     returned for the date JD+JD_frac.}
   \qualifier{deg}{return nutation angles in degrees}
   \qualifier{arcsec}{return nutation angles in arcseconds}
   \qualifier{mas}{return nutation angles in milliarcseconds}
   \qualifier{iaufile}{FITS file containing the theory coefficients
                   (usually found in ISISSCRIPTS_REFPATH)}
 
}
\description
 This routine calculates the nutation angles for the forced nutation
 of the non-rigid earth according to one of the nutation theories described
 above for date given by the JD-argument (which is interpreted as TT).
 For most applications, usage of the NU2000K or IAU2000B routines
 is fully sufficient.

 This routine is array safe (but JD_frac MUST be a scalar in this case!),
 the angles returned are in radians unless one of the deg, arcsec, or mas
 keywords is given.

 The routine is based on the NOVAS 3.1 C-code (Bangert et al., 2011, see
 http://aa.usno.navy.mil/software/novas/novas_info.php )

\seealso{precess}
\done

\function{nutation_matrix}
\synopsis{Calculate the nutation matrix for the mean equinox JD}
\usage{Matrix33_Type N=nutation_matrix(JD)}
\qualifiers{
   \qualifier{theory}{nutation theory to use, see help for
                      function nutation_angles for details.
                      Default: NU2000K}
}
\description
 This function returns the nutation matrix to convert coordinates
 for the mean equinox of JD to the true equinox of JD.
 
 Note: This is mainly an internal function, it therefore does NOT
 have the usual qualifiers such as mjd, however, for some applications
 it is useful to have direct access to the nutation matrix
\done

\function{N_body_simulation}
\synopsis{Compute orbits for N interacting particles}
\usage{Struct_Type ret = N_body_simulation(Struct_Type in, Double_Type t_end; qualifiers)}
\description
    By numerical integration and without any simplifying approximations, this function
    directly solves the equations of motion of a system of N particles under the
    influence of their mutual forces (The interaction is specified in a separate
    function, see qualifier 'kernel'.) from time t=0 to time t=t_end. Negative values
    for t_end imply backward integration. Cartesian coordinates are used throughout.

    The input structure 'in' has to contain the fields "x", "y", "z", "vx", "vy", "vz".
    Each of these fields has to be an array of length N. The corresponding index gives
    the particle id, i.e., particle 0 refers to x[0], y[0], z[0], vx[0], vy[0], vz[0].
    The return structure 'ret' contains for each particle a field, e.g., for particle 0
    a field named "o0". This field is again a structure with fields "t", "x", "y", "z",
    "vx", "vy", and "vz" giving the time-dependent coordinates of the respective particle.
\notes
    An adaptive Runge-Kutta-Fehlberg method of fourth/fifth order is applied to solve the
    coupled system of first-order differential equations. The stepsize is hereby controlled
    such that for each step an absolute accuracy in coordinates and velocity components is
    achieved that is smaller than given by the qualifier 'tolerance'.
\qualifiers{
\qualifier{kernel}{[\code{="N_body_simulation_std_kernel"}] Name of the function which describes the
      mutual interaction. Note that all qualifiers are passed to this function as well.}
\qualifier{threshold}{[\code{=0}] Lower limit on the time difference of two consecutive moments of time
      that will be saved.}
\qualifier{tolerance}{[\code{=1e-10}] Absolut error control tolerance; lower limit: 1e-15.}
\qualifier{verbose}{Show intermediate times t.}
}
\example
    % Four interacting particles:
    s = struct{x, y, z, vx, vy, vz};
    s.x = [-10,0,10,0];
    s.y = [0,10,0,-10];
    s.z = [0,0,0,0];
    s.vx = [0,-1,0,1];
    s.vy = [-1,0,1,0];
    s.vz = [0,0,0,0];
    r = N_body_simulation(s, 50; kernel="N_body_simulation_std_kernel", psa=10*[1,1,1,1], psb=0.1*[1,1,1,1]);
    xrange(min_max([r.o0.x,r.o1.x,r.o2.x,r.o3.x]));
    yrange(min_max([r.o0.y,r.o1.y,r.o2.y,r.o3.y]));
    plot(r.o0.x,r.o0.y); oplot(r.o1.x,r.o1.y); oplot(r.o2.x,r.o2.y); oplot(r.o3.x,r.o3.y);
\seealso{N_body_simulation_std_kernel, N_body_simulation_MW_kernel}
\done

\function{N_body_simulation_MW_kernel}
\synopsis{Alternative interaction kernel for the function 'N_body_simulation'}
\usage{Double_Type r[6,N] = N_body_simulation_MW_kernel(Double_Types t, m[6,N]; qualifiers)}
\description
    This function is an alternative interaction kernel for the function 'N_body_simulation'.
    It combines the mutual N-nody interactions of the standard interaction kernel
    'N_body_simulation_std_kernel' with the external forces stemming from an analytical
    model for the gravitational potential of the Milky Way (see qualifier 'model').
\notes
    Because of the unit convention used for the potentials of the Milky Way, 'psa', which
    is a qualifier of the function 'N_body_simulation_std_kernel' and which is assumed to
    be the mass of the Plummer spheres in solar masses, has to be converted to Galactic
    mass units and then multiplied with a constant accounting for the remaining unit
    conversions (see example below). The units of 'psb' have to be kpc^2.
\qualifiers{
\qualifier{model}{[\code{="AS"}]: Function ("AS", "MN_NFW", or "MN_TF"), which evaluates the equations of
      motion that result from a model for the gravitational potential of the Milky Way.}
\qualifier{All qualifiers from the model potential function except 'coords'.}{}
\qualifier{All qualifiers from the function 'N_body_simulation_std_kernel'.}{}
}
\example
    % Interacting satellite galaxies in the Milky Way:
    s = properties_satellite_galaxies();
    i = struct{ x, y, z, vx, vy, vz, psa, psb };
    model = "AS"; % Milky Way mass model
    SunGCDist = (@(__get_reference(model)))(; eomecd="sgcd"); % Sun-GC distance of chosen mass model
    temp = [SunGCDist,0,0,0,0,0]; reshape(temp, [6,1]);
    vlsr = (@(__get_reference(model)))(0, temp; eomecd="circ")[0]; % Local standard of rest velocity of chosen mass model
    (i.x, i.y, i.z, i.vx, i.vy, i.vz) = cel2gal(s.ah, s.am, s.as, s.dd, s.dm, s.ds, s.dist, s.vrad, s.pma_cos_d, s.pmd; SunGCDist=SunGCDist, vlsr=vlsr);
    kpcmyr_to_kms = 977.7736364875057; % = conversion factor from kpc/myr to km/s = 3.0856775975*10^16 / (10^6 * 3.15582 * 10^7);
    i.vx /= kpcmyr_to_kms;
    i.vy /= kpcmyr_to_kms;
    i.vz /= kpcmyr_to_kms;
    i.psa = s.Pl_mass/2.325131802556774e+07; % conversion from solar masses to Galactic mass units Mgal to have G=1
    % Mgal = 2.325131802556774e+07 = 1e8*3.0856775975*1e19/6.6742/1e-11/1.9884/1e30, see Irrgang et al., 2013, A&A, 549, A137
    const = 100./kpcmyr_to_kms^2; % factor 100 because potential is given in 100 km^2/s^2, see Irrgang et al. 2013
    i.psa *= const;
    i.psb = (s.Pl_radius)^2;
    r = N_body_simulation(i, -1000; kernel="N_body_simulation_MW_kernel", psa=i.psa, psb=i.psb, model=model);
    xrange(min_max([r.o0.x,r.o1.x])); yrange(min_max([r.o0.y,r.o1.y]));
    plot(r.o0.x,r.o0.y); oplot(r.o1.x,r.o1.y);
\seealso{N_body_simulation, N_body_simulation_std_kernel, AS, MN_NFW, MN_TF}
\done

\function{N_body_simulation_std_kernel}
\synopsis{Default interaction kernel for the function 'N_body_simulation'}
\usage{Double_Type r[6,N] = N_body_simulation_std_kernel(Double_Types t, m[6,N]; qualifiers)}
\description
   This function is the default interaction kernel of the function 'N_body_simulation'.
   It computes the equations of motion of N interacting particles at time t.
   The input parameter 'm' is a [6,N]-matrix with
      m[0,j] = x_j;
      m[1,j] = y_j;
      m[2,j] = z_j;
      m[3,j] = vx_j;
      m[4,j] = vy_j;
      m[5,j] = vz_j;

   For each particle, the potential of particle i (at position (x_i,y_i,z_i)) exerted
   on particle j (at position (x_j,y_j,z_j)) is a Plummer sphere of the form
      Phi(x_i,y_i,z_i,x_j,y_j,z_j) = -psa_i*(psb_i+dis^2)^(-1/2)
   with
      dis^2 = (x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2
   and psa_i and psb_i model constants (see qualifiers).
   The resulting acceleration of particle j is then
      d^2/dt^2 x_j = -d/dx_j Phi = sum( psa_i*(psb_i+dis^2)^(-3/2)*(x_i-x_j), i!=j)
      d^2/dt^2 y_j = -d/dy_j Phi = sum( psa_i*(psb_i+dis^2)^(-3/2)*(y_i-y_j), i!=j)
      d^2/dt^2 z_j = -d/dz_j Phi = sum( psa_i*(psb_i+dis^2)^(-3/2)*(z_i-z_j), i!=j)
   yielding the following system of first-order differential equations, which are the
   equations of motion of this system and which are returned as a [6,N]-matrix 'r'
      d/dt x_j  = r[0,j] = vx_j
      d/dt y_j  = r[1,j] = vy_j
      d/dt z_j  = r[2,j] = vz_j
      d/dt vx_j = r[3,j] = -d/dx_j Phi = sum( psa_i*(psb_i+dis^2)^(-3/2)*(x_i-x_j), i!=j)
      d/dt vy_j = r[4,j] = -d/dy_j Phi = sum( psa_i*(psb_i+dis^2)^(-3/2)*(y_i-y_j), i!=j)
      d/dt vz_j = r[5,j] = -d/dz_j Phi = sum( psa_i*(psb_i+dis^2)^(-3/2)*(z_i-z_j), i!=j)
\qualifiers{
\qualifier{psa}{[\code{=Double_Type[N]+1}] Parameter used to parametrize the interaction potential.}
\qualifier{psb}{[\code{=Double_Type[N]+0}] Parameter used to parametrize the interaction potential.}
}
\example
    N = 4;
    m = Double_Type[6,N];
    r = N_body_simulation_std_kernel(0,m; psa=Double_Type[N]+1, psb=Double_Type[N]+1);
\seealso{N_body_simulation}
\done

\function{object_visibility_satellite}
\synopsis{Calculate the time intervals when an object is observable with an astronomical satellite}
\usage{visibility=object_visibility_satellite(ra,dec,mjdstart,mjdstop);}
\qualifiers{
\qualifier{dt}{Time resolution of the visibility calculation (days, default: 1)}
\qualifier{numerical}{see below} 

    Qualifiers to select the satellite. Constraints are ANDed if multiple
    satellite qualifiers are given, i.e., the joint visibility is returned
\qualifier{swift}{46-180 deg from Sun; so far ignores lunar constraint (>23deg)}
\qualifier{suzaku}{70-110 deg from Sun}
\qualifier{xmm}{70-110 deg from Sun}
\qualifier{chandra}{>46deg from Sun; ignores lunar constraint (>6deg)}
\qualifier{hst}{>50deg from Sun}
\qualifier{nustar}{no solar avoidance constraints(!)}
}
\description
 This routine calculates the time intervals during which an astronomical
 source with position ra, dec (both given in degrees) is visible to a given
 astronomical satellite. Most popular facilities are supported (see above).
 For each time between mjdstart and mjdstop the routine calculates the
 angular separation between the source and the Sun. By default it returns
 an array of structures giving the time intervals during which a source is
 visible. The tags of the structure describing each interval are 
    MJDstart and MJDstop: start and stop MJD of the visibility interval
    MJDstring           : human readable string of the above
 An empty array is returned if the object is not visible during the time
 interval specified by mjdstart and mjdstop.

 If the qualifier "numerical" is set, the function returns a structure with
 the following tags
    mjd: MJD
    theta: separation betwee source and Sun (deg)
    visible: boolean visibility (0: no, 1: visible)
 instead of the array of time intervals

 Note: if the visibility interval starts before mjdstart or extends beyond
   mjdend, the start and stop times of the search interval are shown instead.
 Note 2: several satellites also have moon avoidance angles, or avoidance
   angles for the Earth. Both are NOT (yet) taken into account.

\example
 % convert source position into degrees
 variable ra=hms2deg(19,49,35.49);
 variable dec=dms2deg(+30,12,31.8);

 % visibility during the year 2013
 variable tstart=MJDofDate(2013,1,1);
 variable tstop=tstart+365;

 variable vis=object_visibility_satellite(ra,dec,tstart,tstop;suzaku,dt=0.1);

 variable i;
 for(i=0;i<length(vis);i++) {
     printf("%s\n",vis[i].MJDstring);
 }
\done

\function{obliquity}
\synopsis{calculate the obliquity of the ecliptic}
\usage{eps=obliquity(JD;mjd);}
\qualifiers{
  \qualifier{mjd}{if set, the argument is in MJD, not in JD}
  \qualifier{laskar}{use the polynomial expression by Laskar (1986,
            Astron. Astrophys. 157, 59), which is valid for a time
            span of around 10000 years around J2000, rather than
            the IAU 2006 expression.}
  \qualifier{true}{return the true obliquity for the date by applying
            nutation}
  \qualifier{theory}{nutation theory to use, see help for
            function nutation_angles; default: NU2000K}
  \qualifier{deg}{return the obliquity in degrees }
  \qualifier{arcsec}{return the obliquity in arcseconds}
}
\description
 This routine calculates the obliquity of the ecliptic for
 the Julian Date(s) JD using the IAU 2006 resolutions, which
 are based on Hilton et al., (2006, Cel. Mech. Dyn. Astron. 94, 351).
 JD can be an array, it is formally measured in TT.

 The IAU 2006 expression is good to better than 1 arcsecond
 for +/-500 years around J2000. Outside of this interval
 use the polynomial expression by Laskar (laskar keyword).

 The angles returned are in rad, use the deg or arcsec qualifiers
 if you want them in degrees or arc seconds.

\seealso{nutation_angles}
\done

\function{ohplot_filled}
\synopsis{over-plot a filled histogram defined by slang arrays}
\usage{ohplot_filled(Array_Type bin_lo, Array_Type bin_hi, Array_Type values)
}
\qualifiers{
\qualifier{fill_style}{[\code{=1}] set the fill style:\n
                       \code{FS = 1} \code{=>} solid (default)\n
                       \code{FS = 2} \code{=>} outline\n
                       \code{FS = 3} \code{=>} hatched (cannot be used with ylog;)\n
                       \code{FS = 4} \code{=>} cross-hatched (cannot be used with ylog;)\n}
\qualifier{ymin}{[\code{=min(values)}] set the lower y-value to which the areas are filled}
\qualifier{angle}{[\code{=degrees}] sets the angle of the hatched lines for FS=3 [default = 45] }
}
\description
    This function overplots a histogram described by three 1-D S-Lang arrays of size N.
    The area below the histogram is filled. The fill style can be selected with a qualifier.
\examples
    \code{ohplot_filled([1:5],[2:6],[1:5]);}\n
\seealso{hplot_filled, hplot}
\done

\function{ohplot_with_err}
\synopsis{overplots histogram data points with errorbars}
\description
    This function passes all its arguments and qualifiers to the
    \code{hplot_with_err} function, but adds the \code{overplot} qualifier.
\seealso{[o][h]plot_with_err, [o][h]plot}
\done

\function{open_plot_ps2eps}
\synopsis{opens a plot and saves .ps filename for latter use of ps2eps}
\usage{Integer_Type id = open_plot_ps2eps(device[, nxpanes[, nypanes]]);}
\description
    \code{open_plot_ps2eps} passes its arguments to \code{open_plot}.
    If used together with \code{close_plot_ps2eps}, an .eps file
    is finally produced from an usual PGPLOT .ps output
    through the external tool ps2eps. Therefore, \code{device} has to be
    a \code{.ps} file with \code{/}[\code{v}][\code{c}]\code{ps} specification.
\qualifiers{
\qualifier{ps2epsopt}{ [="-R=+ -B -f"]: option for ps2eps}
\qualifier{noremoveps}{the .ps file will not be removed after conversion}
\qualifier{pre_enlargeBB}{enlarge bounding box by specified value [=1, if none specified] before ps2eps}
\qualifier{pre_run_cmd=cmd}{command to run before ps2eps.
                       The ps-file is passed to \code{cmd} as an argument.
                       \code{cmd} is expected to write the modified ps file to \code{stdout}.}
\qualifier{enlargeBB}{enlarge bounding box by specified value [=1, if none specified] after ps2eps}
}
\seealso{open_plot, close_plot_ps2eps}
\done

\function{open_print}
\synopsis{Wrapper around open_plot (and pg_color) to allow a system function to call the output file (isis_fancy_plots package)}
\usage{id = open_print(String_Type);}
\description

   Use as:
   isis> id = open_print("fig1.ps/vcps"); keynote_size; nice_width;
   isis> plot(x,y);
   isis> close_print(id,"gv");
\seealso{close_print, sov, open_plot, close_plot, apj_size, keynote_size, nice_width, pg_color, pg_info}
\done

\function{oplotGTIs}
\synopsis{visualizes Good Time Intervals}
\usage{oplotGTIs(Struct_Type gti[, Double_Type offset]);}
\done

\function{oplot_contour_lines}
\synopsis{overplots contour lines of a 2d array (image)}
\usage{oplot_contour_lines(f, f0[, X, Y]);}
\qualifiers{
\qualifier{save}{=filename: saves/restores the contour lines in a FITS file}
\qualifier{pgplot}{uses \code{_pgline} instead of \code{oplot}}
}
\description
    \code{f} has to be a two-dimensional array (an image).
    \code{f0} is the value of the contour lines \code{f[y,x] = f0}.
    The arrays \code{X} and \code{Y}, if present, transform the array-indices \code{x} and \code{y}
    to the coordinate system used for plotting.
\seealso{get_contour_lines}
\done

\function{oplot_rect}
\synopsis{overplots a rectangle}
\usage{oplot_rect(Double_Type x1, y1, x2, y2);
\altusage{oplot_rect(Double_Type [x1, x2], [y1, y2]);}
}
\done

\function{oplot_struct_arrays}
\synopsis{overplots the fields of a structure agains each other}
\usage{oplot_struct_arrays(Struct_Type s, String_Type fieldnameX, String_Type fieldnameY);}
\qualifiers{
\qualifier{xoffset}{}
\qualifier{yoffset}{}
}
\description
    \code{oplot(s.fieldnameX - xoffset,  s.fieldnameY - yoffset);}
\done

\function{oplot_with_err}
\synopsis{overplots data points with errorbars}
\description
    This function passes all its arguments and qualifiers to the
    \code{plot_with_err} function, but adds the \code{overplot} qualifier.
\seealso{[o][h]plot_with_err, [o][h]plot}
\done

\function{oplot_xline}
\synopsis{overplots one or more vertical line(s)}
\usage{oplot_xline(Double_Type x1[, x2, ...]);}
\qualifiers{
\qualifier{ymin}{[=<minimum of current \code{yrange}>]}
\qualifier{ymax}{[=<maximum of current \code{yrange}>]}
\qualifier{color}{number of the color to use}
}
\description
    For each \code{x} of \code{x1}[, \code{x2}, ...] (which may be arrays, too),
    a line is overplotted from \code{ymin} to \code{ymax} with the same \code{color}.
\done

\function{oplot_yline}
\synopsis{overplots one or more horizontal line(s)}
\usage{oplot_yline(Double_Type y1[, y2, ...]);}
\qualifiers{
\qualifier{xmin}{[=<minimum of current \code{xrange}>]}
\qualifier{xmax}{[=<maximum of current \code{xrange}>]}
\qualifier{color}{number of the color to use}
}
\description
    For each \code{y} of \code{y1}[, \code{y2}, ...] (which may be arrays, too),
    a line is overplotted from \code{xmin} to \code{xmax} with the same \code{color}.
\done

\function{orbitalphase}
\synopsis{calculates an orbital phase at a given MJD time from an ephemeris}
\usage{Double_Type phi = orbitalphase(Double_Type MJD, T0, P0, [Pdot, [Pddot ]]);
\altusage{Double_Type phi = orbitalphase(Double_Type MJD, String_Type ephemeris);}
}
\description
    \code{phi = (MJD-T0)/P - floor( (MJD-T0)/P );}
    where P = P0 + ( Pdot/2. + Pddot*MJD/6. )*MJD ;
    The string \code{ephemeris} can be used to refer to an internally
    stored (T0, P) pair or, if available, (T0, P, Pdot, Pddot) quadruple.

    If \code{orbitalphase} is called with an unknown \code{ephemeris},
    the allowed values are shown by \code{get_ephemeris}.
\qualifiers{
\qualifier{numpulse}{return pulse number instead of phase}
}
\seealso{get_ephemeris}
\done

\function{orbit_calculator}
\synopsis{Calculate orbits of test particles in a Galactic gravitational potential}
\usage{orbit_calculator(Double_Types ah[], am[], as[], dd[], dm[], ds[], dist[], vrad[], pma_cos_d[], pmd[], t_end; qualifiers)
    % or (for error propagation)
    orbit_calculator(Double_Types ah, am, as, dd, dm, ds, dist, dist_sigma[], vrad, vrad_sigma[], pma_cos_d, pma_cos_d_sigma[], pmd, pmd_sigma[], t_end; qualifiers)}
\altusage{orbit_calculator(Double_Types x[], y[], z[], vx[], vy[], vz[], t_end; qualifiers)}
\qualifiers{
\qualifier{coords}{[\code{="cyl"}]: Use cylindrical ("cyl") or cartesian ("cart") coordinates for the internal
      computations. Note that circular orbits like that of the Sun are computed faster in cylindrical
      coordinates whereas cartesian coordinates are much more efficient for straight-line trajectories
      or those that come very close to the z-axis where angular momentum terms slow down cylindrical
      calculations.}
\qualifier{dt}{Deactivate adaptive mode and use this fixed stepsize instead.}
\qualifier{model}{[\code{="AS"}]: Function ("AS", "MN_NFW", "MN_TF", or "plummer_MW") evaluating the equations
      of motion, circular velocity, and energy of the model potential.}
\qualifier{MC_runs}{[\code{=nint(10^5)}]: Number of Monte Carlo realizations in the case that 1-sigma errors are given.}
\qualifier{ODE_solver}{[\code{="RKCK"}]: Choose among three different Runge-Kutta (RK) integration methods:
      "RKF": RK-Fehlberg, "RKCK": RK-Cash-Karp, "RKDP": RK-Dormand-Prince.}
\qualifier{parallax_pma_corr}{[\code{=0}]: Correlation between parallax and proper motion in right ascension.}
\qualifier{parallax_pmd_corr}{[\code{=0}]: Correlation between parallax and proper motion in declination.}
\qualifier{pma_pmd_corr}{[\code{=0}]: Correlation between proper motion in right ascension and in declination.}
\qualifier{disk}{[\code{=struct{radius = 0, height = 0.1, x = 0, y = 0, z = 0, crossings = 1}}]: If disk.radius > 0,
      orbit integration will be stopped at the moment a trajectory has crossed a horizontal plane
      located at z = disk.z + disk.height * Gaussian-random-number inside a circle of radius
      disk.radius centered at (disk.x, disk.y) for the disk.crossings-th time.}
\qualifier{seed}{[\code{=_time()}]: Seed the random number generator via the function 'seed_random'.}
\qualifier{set}{[\code{=0}]: If present, trajectories will be saved ("Save entire trajectories"). As
      this can be very memory-consuming for a large number of orbits, one can additionally
      use the value of this qualifier to specify a lower limit on the time difference of
      two consecutive moments of time that will be saved.}
\qualifier{stff}{"Save to fits files": Prefix of fits files to which initial and final structures are written.}
\qualifier{SunGCDist}{By default, the Sun-Galactic center distance is taken from the current model.
      Use this qualifier to explicitly set a distance in kpc.}
\qualifier{tolerance}{[\code{=1e-8}]: Absolute error control tolerance; lower limit: 1e-15.}
\qualifier{verbose}{Show intermediate times t.}
\qualifier{Any qualifiers from 'cel2gal' and the model potential function except 'vlsr' and 'eomecd'.}{
      Important note: for consistency, the local standard of rest velocity vlsr is calculated from the
      circular velocity of the current model potential evaluated at (r=SunGCDist, z=0).}
}
\description
    Calculate orbits of test particles in a Galactic gravitational potential from given
    initial conditions. The latter can be given either in celestial or Galactic cartesian
    coordinates (see the help on the function 'cel2gal' for format and unit conventions).
    Integration starts at time t=0 and ends at t=t_end [Myr], which is the last input
    parameter. A negative t_end implies backward integration, which is e.g. useful to
    trace back orbits in time. The potential and its equations of motion are outsourced
    to a function specified by the qualifier 'model'. In this way, switching from one model
    to another one is simply done by changing the before mentioned qualifier. Make always
    use of cylindrical coordinates (r [kpc], phi [rad], z [kpc]) and their canonical momenta
    (vr [kpc/Myr], Lz [kpc^2/Myr], vz [kpc/Myr]) as well as of cartesian coordinates (x [kpc],
    y [kpc], z [kpc], vx [kpc/Myr], vy [kpc/Myr], vz [kpc/Myr]) to set up the equations of
    motion (see the qualifier 'coords'). To numerically integrate the coupled differential
    equations, an adaptive Runge-Kutta method of fourth/fifth order is used (see qualifier
    'ODE_solver'). The stepsize is hereby controlled such that for each step an absolute
    accuracy in coordinates (in units of kpc) and velocity components (in units of km/s) is
    achieved that is smaller than given by the qualifier 'tolerance'. The function is optimized
    for multi-orbit calculations, i.e., when the input parameters are either arrays or 1-sigma
    uncertainties are given in addition. The latter are used to create Gaussian distributed
    initial conditions to perform error propagation based on a number of Monte Carlo runs as
    specified by the qualifier 'MC_runs'. To assign asymmetric uncertainties, use an array
    of length two, i.e., [sigma_plus, sigma_minus], instead of a single number. The Gaussian
    distribution is then split up into two Gaussian distributions, one with standard deviation
    sigma_plus (for values larger than the respective input parameter) and one with sigma_minus
    (else). To account for correlations between the distance (or alternatively the parallax,
    see the qualifier 'parallax' in the function 'cel2gal'), proper motions in right ascension,
    and proper motion in declination, make use of the qualifiers 'parallax_pma_corr',
    'parallax_pmd_corr', and 'pma_pmd_corr'. Note that asymmetric uncertainties and
    parameter correlations are mutually exclusive for individual parameters. All orbits are
    computed simultaneously and on the same time grid. Stepsize control is hereby based on
    the worst-offender principle. The function returns one structure with the two fields
    "i" (initial) and "f" (final) (both again structures) containing the initial and final
    Galactic cartesian coordinates (in kpc) and velocities (in km/s) as well as the z-component
    of angular momentum Lz (in kpc^2/Myr) and total energy E_total = E_kin + E_pot (in kpc^2/Myr^2).
    To see whether conservation of energy or conservation of angular momentum is implemented
    in the equations of motion or not, have a look at the help of the outsourced potential
    function. The final structure contains also the minimum and maximum Galactocentric distances
    Rmin = min(R(t)) and Rmax = max(R(t)) with R(t) = sqrt(x(t)^2+y(t)^2+z(t)^2) and - if the
    qualifier 'coords' is set to "cyl" - the number of revolutions about the Galactic z-axis
    (negative values imply a motion in direction of Galactic rotation, i.e. prograde orbits,
    while positive ones imply retrograde orbits). To save the initial and final structures to
    fits files, set the qualifier 'stff' to the desired prefix of the filename. If not only
    the initial and final situation is of interest, but rather the entire trajectories, use
    the qualifier 'set'. In this case, the additional field "tr" (trajectory) (again a structure,
    one field per orbit ("o1", "o2", ...)) is added to the returned structure containing all
    orbits. For tracing back orbits to the Galactic disk, use the 'disk' qualifier to stop
    individual calculations at the moment a trajectory has crossed the disk for a certain
    number of times.
\example
    s = orbit_calculator(12,22,29.6,40,49,36,3.078,262,-13.52,16.34,-1000; disk = struct{radius=50, height=0.2, x=0, y=0, z=0, crossings=1});
    s = orbit_calculator(12,22,29.6,40,49,36,3.078,[0.6,0.3],262,5,-13.52,1.31,16.34,1.37,-1000; disk = struct{radius=50, height=0.2, x=0, y=0, z=0, crossings=1}, stff="HIP60350", MC_runs=100);
    s = orbit_calculator(-8.4,0,0,0,242,0,250; set, model="MN_TF");
    plot(s.tr.o0.x, s.tr.o0.y);
    s = orbit_calculator([8:10:#500], [0:1:#500], [4:5:#500], [0:0:#500], [210:230:#500], [-10:-50:#500], 1000);

    % Example using parallax, proper motions, and correlation parameters from Gaia:
    s = orbit_calculator(12,22,29.6,40,49,36,0.325,0.3,262,5,-13.52,1.31,16.34,1.37,-1; parallax, parallax_pma_corr=0.1, parallax_pmd_corr=0.2, pma_pmd_corr=0.75);

    % Example using the spectroscopic distance and only proper motions from Gaia:
    s = orbit_calculator(12,22,29.6,40,49,36,3.077,[0.6,0.3],262,5,-13.52,1.31,16.34,1.37,-1; pma_pmd_corr=0.75);
\seealso{cel2gal, AS, MN_NFW, MN_TF, plummer_MW, xfig_3d_orbit_on_cube}
\done

\function{outer_product}
\synopsis{computes the outer product of a number of vectors}
\usage{Double_Type P[] = outer_product(Double_Type f1[], f2[], ..., fn[]);}
\description
    The \code{n} arguments \code{f1}, \code{f2}, ..., \code{fn} have to be one-dimensional arrays.
    The return value \code{P} is an \code{n}-dimensional array with
    \code{P[i1, i2, ..., in] = f1[i1] * f2[i2] * ... fn[in]}.
\done

\function{outer_products}
\synopsis{computes multi-dimensional arrays from one-dimensional ones}
\usage{(Double_Type P1[], P2[], ...) = outer_products(Double_Type f1[], f2[], ..., fn[]);}
\description
    \code{P1[i1, i2, ..., in] = f1[i1];}\n
    \code{P2[i1, i2, ..., in] = f2[i2];}\n
    ...\n
    \code{Pn[i1, i2, ..., in] = fn[i1];}\n

    The return value \code{Pj} is calculated as outer product of \code{x1, x2, ..., xn},
    where \code{xj} = \code{fj} and
          \code{xi = [1, 1, ..., 1]} (same length as \code{fi}) for \code{i!=j}.
\seealso{outer_product}
\done

\function{overplot_clean_model}
\synopsis{creates an overlay of an clean VLBI image and the modelfit components}
\usage{overplot_clean_model(String_Type \code{cleanfile}, String_Type \code{modfile}, String_Type \code{outputfile});}
\qualifiers{
\qualifier{ra_mas}{[={20,-20}] {left,right} limits of image in mas}
\qualifier{dec_mas}{[={-20,20}] {top,bottom} limits of image in mas}
\qualifier{plot_size}{[=15] size of plot}
\qualifier{n_sigma}{[=3.0] lowest contour of clean image}
\qualifier{sourcename}{[=default] name of the source, by default the name is read from the .fits file,\n
 				set to NULL for not plotting a source name}
\qualifier{obs_date}{[=default] the observation date, by default the observation date is read from the .fits file, \n
				set to NULL for not plotting a observation date, set to "mjd" when MJD format required}
\qualifier{cont_color}{[="gray"] color of contours of clean image}
\qualifier{cont_scl}{[="2"] set factor to change the separation between contour levels}
\qualifier{cont_lvl}{[=[c1,c2,..] set contour levels [Jy] manually, overwrites other contour parameters}
\qualifier{model_color}{[="black"] color of Gaussian ellipses}
\qualifier{center_symbol}{[="+"] symbol stating the ellipse center, set to NULL for no symbol}
\qualifier{comp_label}{[=0] set to 1 when labelling of components depending on distance from [0,0]
 				is requested}
\qualifier{ind_inverse}{[=0] if the labels should be in inverse order set to 1}
\qualifier{label_alt}{[=NULL] give list of alternative labels}
\qualifier{label_color}{[="red"] color of component labels}
\qualifier{ex_counterjet}{[=0]	set to 1 if counterjet components (and core) should be excluded and define
 				the corresponding quadrant of the plot via the CJ-coordinates}
\qualifier{counterjet_ra}{[=NULL] define counterjet coordinates in mas}
\qualifier{counterjet_dec}{[=NULL] define counterjet coordinates in mas}
}
\description
    This function creates an overlay of a VLBI-clean image and the corresponding\n
    model of Gaussian components which are over-plotted as ellipses.\n
    The required input format are fits-file for both the clean and the modelfit images.
    The format of the output file depends on the suffix of the given\n
    \code{filename}. Possible formats of the output file are PDF, EPS,\n
    PNG, GIF, etc.
    If labelling = 1 is set, the components are labeled with J0,J1,J2,... depending on 
    their distance to [0,0].
    The counterjet components (and core) can be excluded by defining the corresponding 
    quadrant of the the plot via counterjet_ra and _dec.
\done

\function{pack_obj}
\synopsis{converts an SLang object into a binary string}
\usage{BString_Type[] pack_obj(Any_Type obj);}
\description
    Uses the `pack' function to convert a single SLang object into
    an array of binary strings (BString_Type). The format specifier
    is included as first character of the strings.
    
    If the object is a single number or string then the returned
    array consists of a single item only. In case the given object
    is an array or a structure, the returned array starts with the
    type of the input array and its length or the field structure
    definition, respectively. The remaining items are the input
    array items or the structure items. For structures, the
    contained objects are converted recursively.

    The following data-types are supported:
    Int16_Type, Int32_Type, Int64_Type, Float_Type, Double_Type,
    Char_Type, String_Type, Array_Type, Struct_Type
    and its aliases.
\example
#v+
#p+
    s = pack_obj(PI);        % s[0] = "d\030-DT\373!\011@"

    s = pack_obj([1:3] + 4); % s[0] = "ak\003\000\000\000"
                             % s[1] = "\005\000\000\000"
                             % s[2] = "\006\000\000\000"
                             % s[3] = "\007\000\000\000"

    variable obj = struct {
      number = 598105,
      float  = 341.12e8,
      more   = struct {
        msg    = "hello"
      }
    };
    s = pack_obj(obj);
    print(s);
#p-
#v-
\seealso{unpack_obj, pack, unpack}
\done

\function{palette--blend}
\synopsis{Blend one or more colors linearly (in RGB space).}
\usage{UInt_Type get_color_palette("blend", UInt_Type n);}
\qualifiers{
  \qualifier{colors[=[red,gray,blue]]}{The colors to interpolate, must be given as HEX}
  \qualifier{set[=[0:1:#length(colors)]]}{The setpoints of the colors. Can be unsorted.}
  \qualifier{space[="hsluv"]}{The color space in which interpolation is done.}
}
\description
  Blend colors to a palette from given colors. The setpoints can be
  used to shift the color at the same position to higher or lower
  interpolation values.

  The qualifier \code{space} allows to control in what space the interpolation
  is done. This either must be one of the known color spaces "hsl", "hsluv", 
  "hpluv", or "rgb", or a tuple (list with 2 elements) with references
  to two functions. The first must take three arguments (R,G,B) as 8 bit
  encoded colors and convert it to the color space variables (x,y,z). The
  second converts back (x,y,z) -> (R,G,B).
\done

\function{palette--cubehelix}
\synopsis{Create palette using 'cubehelix' description}
\usage{UInt_Type[] pattern = get_color_palette("cubehelix", UInt_Type n);}
\qualifiers{
  \qualifier{start[=0.0]}{Start color of the palette}
  \qualifier{s[=0.8]}{Saturation of the colors}
  \qualifier{rot[=0.4]}{How far the scale rotates in color (hue)}
  \qualifier{dark[=0.15]}{Dark end of the palette}
  \qualifier{light[=0.85]}{Light end of the palette}
}
\description
  Calculates a color palette according to the description
  in D. A. Green's, 'A colour scheme for the display of
  astronomical intensity images', (Bull. Astr. Soc. India (2011)).
\done

\function{palette--dark}
\synopsis{Get dark blend palette with given color}
\usage{UInt_Type[] palette = get_color_palette("dark", UInt_Type n);}
\qualifiers{
  \qualifier{color[=red]}{Color to blend with dark pattern (as HEX value).}
  \qualifier{reverse}{If given, blend from color to dark}
}
\description
  Get a color palette from the chosen color fading in from
  a dark tone.
\done

\function{palette--diverging}
\synopsis{Create a diverging palette between two colors}
\usage{UInt_Type palette = get_color_palette("diverging", UInt_Type n);}
\qualifiers{
  \qualifier{dark}{If given, midpoint is a dark tone}
  \qualifier{h_pos[=0.7]}{Hue of positive color}
  \qualifier{h_neg[=0.2]}{Hue of negative color}
  \qualifier{sep[=0.01]}{Size of midpoint region}
  \qualifier{s[=0.75]}{Saturation of both colors}
  \qualifier{l[=0.5]}{Luminosity of both colors}
}
\description
  Create a color palette useful to represent diverging data.

\seealso{hsluv2rgb}
\done

\function{palette--hpluv}
\synopsis{Generate palette from HSLuv space}
\usage{UInt_Type[] palette = get_color_palette ("hpluv", UInt_Type n);}
\qualifiers{
\qualifier{h[=0.01]}{Starting hue}
\qualifier{s[=0.90]}{Saturation}
\qualifier{l[=0.60]}{Lightness}
}
\description
  Create a color palette with \code{n} entries. Control the
  appearence with the HSL qualifiers.
\seealso{hpluv2rgb}
\done

\function{palette--hsl}
\synopsis{Generate palette from HSL space}
\usage{UInt_Type[] palette = get_color_palette ("hsl", UInt_Type n);}
\qualifiers{
\qualifier{h[=0.01]}{Starting hue}
\qualifier{s[=0.65]}{Saturation}
\qualifier{l[=0.60]}{Lightness}
}
\description
  Create a color palette with \code{n} entries. Control the
  appearence with the HSL qualifiers.
\seealso{hsl2rgb}
\done

\function{palette--hsluv}
\synopsis{Generate palette from HSLuv space}
\usage{UInt_Type[] palette = get_color_palette ("hsluv", UInt_Type n);}
\qualifiers{
\qualifier{h[=0.01]}{Starting hue}
\qualifier{s[=0.90]}{Saturation}
\qualifier{l[=0.60]}{Lightness}
}
\description
  Create a color palette with \code{n} entries. Control the
  appearence with the HSL qualifiers.
\seealso{hsluv2rgb}
\done

\function{palette--light}
\synopsis{Get light blend palette with given color}
\usage{UInt_Type[] palette = get_color_palette("light", UInt_Type n);}
\qualifiers{
  \qualifier{color[=red]}{Color to blend with dark pattern (as HEX value).}
  \qualifier{reverse}{If given, blend from color to light}
}
\description
  Get a color palette from the chosen color fading in from
  a light tone.
\done

\function{palette--map}
\synopsis{Retrieve a set of colors from a colormap}
\usage{UInt_Type[] palette = get_color_palette (String_Type name, UInt_Type len);}
\qualifiers{
  \qualifier{start[=0.0]}{Starting "color" (lowest value)}
  \qualifier{range[=1.0]}{Range of "color", larger than one causes repetitions}
}
\description
  This function is useful if a range of colors is required.
  It returns \code{len} values from a linear mapping of the
  given colormap. The colors are constructed from palette--blend
  and forwards all qualifiers to this constructor.

  Note: This function is used only if the given \code{name} is
  a colormap (and does not match a palette name, see \code{png_get_colormap_names}),
  to construct a %  set of colors from a color array see
  palette--blend.

\seealso{png_get_colormap_names, palette--name}
\done

\function{palette--sb}
\synopsis{Get Seaborn color paletts}
\usage{UInt_Type[] get_color_paletts("sb:"+name, UInt_Type n);}
\description
  The seaborn paletts are a very good set of colors for qualitative
  representation of data (like lines, points, etc.).
\done

\function{palette--sron}
\synopsis{Get SRON palettes}
\usage{UInt_Type[] get_color_palette("sron:"+name, UInt_Type n);}
\description
  The SRON colors are defined in Paul Tol's technical note (\code{https://personal.sron.nl/~pault/data/colourschemes.pdf})
  and provides a set of colors for qualitative, diverging, and sequential
  representations.

  Note: To get the original set of colors (was provided by \code{get_sron_colors})
  use \code{get_color_palette("sron-rainbow", n);} for diverging and sequential
  there are no direct matches. See the other color palette options.

\seealso{get_color_palette_names, palette--sron-rainbow}
\done

\function{palette--sron-rainbow}
\synopsis{Get SRON rainbow palette from Paul Tol}
\usage{UInt_Type[] palette = get_color_palette("sron-rainbow", UInt_Type n);}
\description
  Returns a set of colors more or less like rainbow colors with good contrast.
  \code{n} makes a smart subset up to 23.
\done

\function{palette--tab}
\synopsis{Get Tableau10 color palettes}
\usage{UInt_Type[] palette = get_color_palette("tab:"+name, UInt_Type n);}
\description
  Color palettes used by the Tableau10 software for visualization. Usuall only
  good for qualitative data.
\done

\function{panstarrsImageDownload}
\synopsis{Download PanSTARRS image cutouts}
\usage{Assoc_Type fileNames = panstarrsImageDownload(String_Type position, Double_Type size, String_Type outdir)
    Assoc_Type fileNames = panstarrsImageDownload(Double_Type ra, Double_Type dec, Double_Type size, String_Type outdir)}
\qualifiers{
\qualifier{g}{Download images taken with the g filter}
\qualifier{r}{Download images taken with the r filter}
\qualifier{i}{Download images taken with the i filter}
\qualifier{z}{Download images taken with the z filter}
\qualifier{y}{Download images taken with the y filter}
\qualifier{stack}{Download final stacks}
\qualifier{warp}{Download individual warps (single epoch images)}
\qualifier{data}{Download main data products}
\qualifier{mask}{Download masks}
\qualifier{wt}{Download weight images}
\qualifier{exp}{Download exposure maps}
\qualifier{expwt}{Download weighted exposure maps (undocumented)}
\qualifier{num}{Download num images (undocumented)}
}
\notes
    Please note that PanSTARRS only covers the sky north of DEC=-30 deg.
    At least one filter is required.
    At least one of stack or warp is required.
    At least one of data, mask, wt, exp, expwt or num is required.
    The function only throws an exception on real runtime errors. If some data
    just aren't available no exceptions is thrown, the images just don't exist
    and are therefore not downloaded and not entered into the returned assoc.
\description
    Access the PanSTARRS DR1 image cutout server located at
    http://plpsipp1v.stsci.edu/cgi-bin/ps1cutouts to
    download different combinations of data products and filters.
    The usage of this function is analogous to the usage of this website.
    position can be a generic string which is resolved by the server itself
    using Simbad or other means. Alternatively RA and DEC can be supplied
    directly to the function in degrees.
    outdir must supply a directory where the output files shall be written.
    The function returns an Assoc_Type which contains the information about
    the downloaded files. If all options are set the assoc looks like this:
#v+
#p+
    assoc
     stack
      aux
       exp
        g=path
        ...
       expwt
        g=path
        ...
       mask
        g=path
        ...
       num
        g=path
        ...
       wt
           g=path
           ...
      colors
          g=path
          ...
     warp
         aux
          mask
           g
            55860.59225=path
            ...
           i
            55200.51443=path
            ...
           r
            55911.49788=path
            ...
           y
            55518.62770=path
            ...
           z
               55302.23917=path
               ...
          wt
              g
               55860.59225=path
               ...
              i
               55200.51443=path
               ...
              r
               55911.49788=path
               ...
              y
               55518.62770=path
               ...
              z
                  55302.23917=path
                  ...
         colors
             g
              55860.59225=path
              ...
             i
              55200.51443=path
              ...
             r
              55911.49788=path
              ...
             y
              55518.62770=path
              ...
             z
                 55302.23917=path
                 ...
#p-
#v-
    The structure should be self explanatory, except the set of numbers at
    the lowest level of the warps. These are the MJDs when the exposure
    was completed.
\done

\function{__panstarrsRetrieveSite}
\synopsis{Download a html page of the PanSTARRS cutout server}
\usage{String_Type __panstarrsRetrieveSite(String_Type URL)}
\description
    Simply takes a url and downloads the html content of the site using curl,
    returning it as a string. Throws an exception upon error.
\done

\function{parseDateString}                                                                                         
\synopsis{parses a string including year, month, day [, hour [, minute [, second]]]}
\usage{Struct_Type parseDateString(String_Type date_string);}
\qualifiers{
\qualifier{no_2digit_correction}{prevents the correction of two digit year specification,
                              e.g., 83/12/13 instead of 1983/12/13. If this qualifier
                              is not set, 2000 is added to years from 00-19 and
                              1900 to years 20-99.}
}
\description
    This function tries to interprete the given string as a data and returns
    a structure consistent with the return value of `localtime'. It is
    assumed that the first number occuring in the string specifies the year,
    the second the month, and the third the day. If further numbers are found,
    they are interpreted as hour, minute, second and fraction of seconds. Note
    that the seconds contained in the structure as defined in `localtime' has
    to be an integer, while here a double is used in case of fraction of
    seconds. There can be any separators between the numbers (except numbers).
\example
    parseDateString("69/07/21");
    parseDateString("1969-07-21 02:56:30.44");
    parseDateString("sdf1969--X07..21QQ_02/56((30,,44xyz");
\seealso{localtime}
\done

\function{__parsePanstarrsPage}
\synopsis{Parse the html of a PanSTARRS cutout server page}
\usage{Assoc_Type __parsePanstarrsPage(String_Type html)}
\description
    Takes the result of __panstarrsRetrieveSite() and parses it in order to
    determine which images are available and what their specific download
    URLs are
\done

\function{partial_correlation}
\synopsis{Tests two luminosities for partial correlation due to redshift}
\usage{partial_correlations(String_Type filename);}
\description
       This function tests if a correlation of two parameters is due to
       the redshift. Description of method and code adapted from
       Akritas & Siebert, 1996, MNRAS, 278, 919.
       Works with FITS and ASCII files.

       The input file needs 6 columns with luminosity1, UL, 
       luminosity2, UL, redshift, UL. The upper limit columns should 
       have a 1 for a detection, and a 0 for an upper limit.

\examples
    partial_correlation("data.txt")
  
    partial_correlation("data.fits")

\seealso{}
\done

\function{par_bounds}
\synopsis{Return parameter index of parameters hitting bounds}
\usage{Int_Type[] par_bounds();}
\qualifiers{
  \qualifier{tolerance}{[=0.01] Boundary hit tolerance}
}
\description
  This function returns the parameter index for every parameter with satisfies
  (par.max-par.value)/(par.max-par.min) <= tolerance or
  (par.value-par.min)/(par.max-par.min) <= tolerance.
\seealso{list_par_bounds, list_free_bounds}
\done

\function{par_list}
\synopsis{Print a parameter file listing}
\usage{Int_Type par_list([String_Type filename | Ref_Type buffer | File_Type fd]);}
\qualifiers{
    \qualifier{tex}{[=0]: Format parameter listing in form of a LATEX tabular
                       environment. This does currently not work for a Ref_Type
                       argument.}
    \qualifier{pdf}{[=0]: Compile a pdf file containing the parameter listing
                       as a table.}
    \qualifier{all}{[=0]: Add all parameters to tex and pdf file. By default
                       parameters with the following name - value combinations
                       are excluded:
                           norm    1}
    \qualifier{exclude}{[=0]: Array or list of parameters to be excluded in
                       tex and pdf listings. These are even excluded if the
                       "all" qualifier above is set.}
    \qualifier{include}{[=0]: Array or list of additional parameters to be
                       included in tex and pdf listings.}
}
\description
    Prints all internally stores parameter file lines to screen. This includes
    out-commented fit functions and parameter lines, which are only loaded if
    par_load is used instead of load_par. Writes parameter listing to file if
    a String_Type parameter is given, which will be interpreted as filename.
    If a File_Type parameter is given the listing will be written to this
    file. If the argument is of Ref_Type, the listing will be returned in this
    buffer. This argument functionality works analog to the default list_par
    function.
\seealso{par_load, par_save, save_par, load_par, list_par}
\done

\function{par_load}
\synopsis{Loads fit function and parameters for given component from file}
\usage{par_load(String_Type filename [, String_Type component]);}
\qualifiers{
    \qualifier{clean}{[=1]:   Remove temporary parameter files for individual
                       components after calling load_par.}
    \qualifier{dryrun}{[=0]:  Generate parameter file for component but
                       do not load it, i.e., do not call load_par.
                       Implies clean=0.}
    \qualifier{iterate}{[=0]: Iterate through components. Returns the name of
                       the current component or NULL if no more components
                       are available.}
}
\description
    Creates a copy of the given parameter file and modifies it such that only
    the fit function associated with the given component is active. This file
    is then loaded using the standart load_par function.
    This way individual model components, or alternative fit functions, can
    be defined in the same parameter file using the following format:
    	#[name1] fitFunction1
    	#[name2] fitFunction2
    	defaultFitFunction
    The rest of the file contains the parameters in the usual format used by
    list_par. The name "DEFAULT" is reserved for the (uncommented) default
    fit function. The function returns the name of the loaded component. It
    returns NULL if the desired component does not exist or if no more
    components are available during iteration.
\seealso{par_save, load_par, list_par}
\done

\function{par_save}
\synopsis{Save fit function and parameters to labeled component in file}
\usage{Int_Type par_save(String_Type filename [, String_Type component]);}
\qualifiers{
    \qualifier{dryrun}{[=0]: generate parameter file for component but
                      do not load it, i.e., do not call load_par.
                      Implies clean=0.}
}
\description
    Saves the current fit function and parameter list
    to the given parameter file. If a component name is given, the current fit
    function will be saved as a named and commented out component, which can
    loaded using par_load. Otherwise the main difference to save_par is that
    comment lines, which are only loaded if par_load is used, are saved to the
    parameter file as well. Calling this function is equivalent to calling
    save_par if no comment lines are present or if the parameter file has been
    loaded using load_par.
    The function returns the number of components written to the file, or -1
    upon error.
\seealso{par_load, save_par, load_par, list_par}
\done

\function{path_realpath}
\synopsis{Get full path of specified path}
\usage{String_Type realpath = path_realpath(String_Type path);}
\description
  Turn the given path into an asbolute path (replacing '..' and '.').
\seealso{path_concat,path_is_absolut}
\done

\function{period_search_via_string_length}
\synopsis{A period-finding method for sparse randomly spaced observations}
\usage{(Double_Type p[], SL[]) = period_search_via_string_length(Double_Type t[], s[], pmin, pmax; qualifiers)}
\description
    This function may be useful to determine the period of a variable
    signal 's' observed at a relatively small number 'N' of randomly
    spaced observations at times 't' possibly taken many periods apart.

    To identify candidate periods in the given range between 'pmin' and
    'pmax', string-lengths 'SL' for trial periods 'p' (sampled in equal
    frequency steps, see qualifier "delta_phi_max") are computed and
    tabulated. The trial period giving the lowest string-length results
    in the most smooth-looking curve and is thus the most likely period.
\notes
    The underlying idea is to minimize the so-called "string-length" in
    a phase diagram. The phase diagram for a trial period 'p' is created
    by computing the phase 'phi' according to
      phi = (t-min(t))/p mod 1.
    The string-length 'SL' is then simply the sum of the lengths of line
    segments joining successive points (phi_i, s_i) in a phase diagram:
      SL = sum ( ((s_i-s_i-1)^2+(phi_i-phi_i-1)^2)^(1/2) )
         + ((s_1-s_N)^2+(phi_1-phi_N+1)^2)^(1/2).
    To get rid of different scales caused by different units in ordinate
    (e.g., km/s or magnitude) and abscissa (unit = trial period), the
    observations are scaled so that
      s' = 0.5*(s-min(s))/(max(s)-min(s))-0.25
    in order to give equal weight to measures and phases (for details,
    see Dworetsky 1983, MNRAS, 203, 917).

    For reference: the string-length of a sinusoidal string is 1.4637.
\qualifiers{
\qualifier{delta_phi_max [=0.01]}{: Trial periods 'p' are sampled in equal
      frequency steps of
        delta_freq = delta_phi_max/(max(t)-min(t)).
      This ensures that phase errors are always less than 'delta_phi_max'
      since
        phi = (t-min(t))/p mod 1 = (t-min(t))*freq mod 1
        -> delta_phi = (t-min(t))*delta_freq
        -> delta_phi_max = (max(t)-min(t))*delta_freq.}
}
\example
    t = 1000*urand(50); % time of observations
    s = sin(t)+0.05*grand(length(t)); % signal of observations; true period is 2*PI
    connect_points(0); point_style(2); plot(t, s);
    (p, SL) = period_search_via_string_length(t, s, 1, 10; delta_phi_max=0.01);
    connect_points(-1); point_style(-1); plot(p, SL);
    ind = where(SL==min(SL)); SL[ind[0]]; p[ind[0]]; % index, value, and period of minimum string-length
    connect_points(0); point_style(2); plot((t-min(t))/p[ind[0]] mod 1, s); % phase diagram
\seealso{}
\done

\function{pfold}
\synopsis{folds a lightcurve or event list on a given period}
\usage{Struct_Type pp = pfold(Double_Type t, r, p);
\altusage{Struct_Type pp = pfold(Double_Type t, r, p, e);}
\altusage{Struct_Type pp = pfold(Double_Type t, p);}}
\qualifiers{
\qualifier{nbins}{[=30] number of bins for the pulse profile}
\qualifier{exact}{take finite lightcurve bins into account (LC case).
             take differantial phase grid into account (Events case).}
\qualifier{dt}{lightcurve bin size.}
\qualifier{fracexp}{fractional exposure per lightcurve bin.}
\qualifier{t0}{set reference time.}
\qualifier{pdot}{[=0] first derivative of pulse period p.}
\qualifier{pddot}{[=0] second derivative of pulse period p.}
\qualifier{gti}{GTIs for event data, given as struct{start=Double_Type, stop=Double_Type}}
}
\description
   Calculates the pulse profile of a given lightcurve or event list
   for times \code{t} and rate \code{r}. In case of an event list
   normally the use of GTIs is necessary to ensure correct results.

   If the qualifiers \code{pdot} or \code{pddot} are given and not
   zero a taylor expansion of the period evolution is used to
   calculate the mapping to phase (from \code{t0}). If \code{pddot}
   is non-zero the result is only approximately correct. See
   \code{phaseOfTime}.

   The input arrays \code{t}, \code{r}, and \code{e} can also be
   given as qualifiers.

   If the \code{exact} qualifier is given the finite size of lightcurve
   bins is taken into account by linear interpolation. However, the
   computation time is increased. For event lists, the code will take the
   differentail phase grid change into account. Otherwise the exposure
   time is approximated assuming a constant period over one GTI frame.

   The returned structure contains the fields bin_lo, bin_hi, value
   and err, ready for plotting the pulse profile (as rate). For further
   diagnostics the \code{counts} (counts) and \code{ttot} (per-bin exposure)
   are given. The field \code{npts} contains the number of lightcurve bins
   starting in each bin.
\seealso{phaseOfTime, timeOfPhase}
\done

\function{pfold_map}
\synopsis{folds a lightcurve or event list on a given period map}
\usage{Struct_Type ppmap = pfold_map(Double_Type t, r, p);
\altusage{Struct_Type ppmap = pfold_map(Double_Type t, r, p, e);}
\altusage{Struct_Type ppmap = pfold_map(Double_Type t, p);}}
\qualifiers{
\qualifier{nbins}{[=30] number of bins for the pulse profile}
\qualifier{exact}{take finite lightcurve bins into account (LC case).
             take differantial phase grid into account (Events case).}
\qualifier{dt}{lightcurve bin size.}
\qualifier{fracexp}{fractional exposure per lightcurve bin.}
\qualifier{t0}{set reference time.}
\qualifier{pdot}{[=0] first derivative of pulse period p.}
\qualifier{pddot}{[=0] second derivative of pulse period p.}
\qualifier{gti}{GTIs for event data, given as struct{start=Double_Type, stop=Double_Type}}
}
\description
   This function acts identical to \code{pfold} except that a two
   dimensional array is calculated, where the first dimension runs
   over the cycles and the second over the period.
   
   The returned structure has the following fields
     bin_lo, bin_hi  Phase grid of pulses (from 0 to 1)
     phase_lo, phase_hi  Cycle grid, continuously
     counts  2D map of counts
     exposure  2D map of exposure time
     rate  2D map of rate
     p  struct returned by pfold

   For illustration: The returned map can be collapsed to the pulse
   profile via \code{sum(ppmap.counts, 0)} or \code{mean(ppmap.rate, 0)}.
\seealso{pfold, phaseOfTime, timeOfPhase}
\done

\function{_pgaxis}
\synopsis{draw a labelled graph axis}
\usage{_pgaxis(opt, x1, y1, x2, y2, v1, v2, step, nsub, dmajl dmajr, fmin, disp, orient);}
\description
    Besides \code{String_Type opt} and \code{Integer_Type nsub},
    all parameters are of \code{Double_Type}.\n
    \code{_pgaxis} draws an axis from world-coordinate position (\code{x1},\code{y1}) to (\code{x2},\code{y2}).

    Normally, this routine draws a standard linear axis with equal subdivisions.
    The quantity described by the axis runs from \code{v1} to \code{v2}.
    If the '\code{L}' option is specified, the routine draws a logarithmic axis.
    In this case, the quantity described by the axis runs from \code{10^v1} to \code{10^v2}.
    A log. axis always has major, labeled, tick marks spaced by one or more decades.
    If the major tick marks are spaced by one decade (as specified by the \code{step} argument),
    then minor tick marks are placed at 2, 3, ..., 9 times each power of 10;
    otherwise minor tick marks are spaced by one decade.  If the axis spans
    less than two decades, numeric labels are placed at 1, 2, and 5 times each
    power of ten.  If the axis spans less than one decade, or if it spans many decades,
    it is preferable to use a linear axis labeled with log(quantity of interest).

    Arguments:\n
    \code{opt}    : a string containing single-letter codes for various options.\n
             The options currently recognized are:\n
             \code{L} : draw a logarithmic axis\n
             \code{N} : write numeric labels\n
             \code{1} : force decimal labelling, instead of automatic choice\n
             \code{2} : force exponential labelling, instead of automatic.\n
    \code{x1}, \code{y1} : world coordinates of one endpoint of the axis.\n
    \code{x2}, \code{y2} : world coordinates of the other endpoint of the axis.\n
    \code{v1}     : axis value at first endpoint.\n
    \code{v2}     : axis value at second endpoint.\n
    \code{step}   : major tick marks are drawn at axis value \code{0.0} plus or minus
             integer multiples of step.  If \code{step==0.0}, a value is chosen automatically.\n
    \code{nsub}   : minor tick marks are drawn to divide the major divisions into \code{nsub} equal
             subdivisions (ignored if \code{step==0.0}).   If \code{nsub <= 1},
             no minor tick marks are drawn. \code{nsub} is ignored for a logarithmic axis.\n
    \code{dmajl}  : length of major tick marks drawn to left of axis
             (as seen looking from first endpoint to second),
             in units of the character height.\n
    \code{dmajr}  : length of major tick marks drawn to right of axis,
             in units of the character height.\n
    \code{fmin}   : length of minor tick marks, as fraction of major.\n
    \code{disp}   : displacement of baseline of tick labels to right of axis,
             in units of the character height.\n
    \code{orient} : orientation of label text, in degrees;
             angle between baseline of text and direction of axis (0-360 deg).
\seealso{http://www.astro.caltech.edu/~tjp/pgplot/subroutines.html#PGAXIS}
\done

\function{_pgbox}
\synopsis{annotate the viewport with frame, axes, numeric labels, etc.}
\usage{_pgbox(xopt, xtick, nxsub,  yopt, ytick, nysub);}
\description
    \code{x}/\code{yopt}: string of options for X (horizontal) / Y (vertical) axis of plot.
            Options are single letters, and may be in any order:\n
            "A": draw Axis (X axis is horizontal line Y=0, Y axis is vertical line X=0).
            "B": draw bottom (X) or left (Y) edge of frame.
            "C": draw top (X) or right (Y) edge of frame.
            "G": draw Grid of vertical (X) or horizontal (Y) lines.
            "I": Invert the tick marks; ie draw them outside the viewport instead of inside.
            "L": label axis Logarithmically (see below).
            "N": write Numeric labels in the conventional location
                 -- below the viewport (X) or to the left of the viewport (Y).
            "P": extend ("Project") major tick marks outside the box
                 (ignored if option I is specified).
            "M": write numeric labels in the unconventional location
                 -- above the viewport (X) or to the right of the viewport (Y).
            "T": draw major Tick marks at the major coordinate interval.
            "S": draw minor tick marks (Subticks).
            "V": orient numeric labels Vertically. This is only applicable to Y.
                 The default is to write Y-labels parallel to the axis.
            "1": force decimal labelling, instead of automatic choice (see PGNUMB).
            "2": force exponential labelling, instead of automatic.

    \code{x}/\code{ytick}: world coordinate interval between major tick marks on X/Y axis.
             If \code{x}/\code{ytick==0}, the interval is chosen by _pgbox,
             so that there will be at least 3 major tick marks along the axis.

    \code{n}\{\code{x}/\code{y}\}\code{sub}: number of subintervals to divide the major coordinate interval into.
               If \code{x}/\code{ytick==0} or \code{nx/ysub==0},the number is chosen by _pgbox.

    To get a complete frame, specify BC in both \code{xopt} and \code{yopt}.
    Tick marks, if requested, are drawn on the axes or frame or both,
    depending which are requested. If none of ABC is specified,
    tick marks will not be drawn.

    For a logarithmic axis, the major tick interval is always 1.0.
    The numeric label is 10^x where x is the world coordinate at the tick mark.
    If subticks are requested, 8 subticks are drawn between each major tick at equal logarithmic intervals.
\seealso{http://www.astro.caltech.edu/~tjp/pgplot/subroutines.html#PGBOX}
\done

\function{pgcolor}
\synopsis{Set pgplot colors 17, 18, 19, 20 to a green, brown, pink, dark yellow (isis_fancy_plots package)}
\usage{pg_color; -or- pgcolor;}
\description
\seealso{pg_info, pginfo}
\done

\function{pginfo}
\synopsis{Print a core dump of some useful pgplot and isis_fancy_plots information.}
\usage{pg_info; -or- pginfo;}
\description
\seealso{Nearly all isis_fancy_plot functions return a use message if invoked without arguments}
\done

\function{_pgsls (set line style)}
\synopsis{set the line style attribute for subsequent plotting}
\usage{_pgsls(Integer_Type linestyle);}
\description
    This attribute affects line primitives only;
    it does not affect graph  markers, text, or area fill.
    Five different line styles are available, with the following codes:
    1 (full line), 2 (dashed), 3 (dot-dash-dot-dash), 4 (dotted),
    5 (dash-dot-dot-dot). The default is 1 (normal full line).
\done

\function{pg_color}
\synopsis{Set pgplot colors 17, 18, 19, 20 to a green, brown, pink, dark yellow (isis_fancy_plots package)}
\usage{pg_color; -or- pgcolor;}
\description
\seealso{pg_info, pginfo}
\done

\function{PG_function}
\synopsis{sets fitting statistic for poisson source counts and already subtracted gaussian background}
\usage{set_fit_statistic("PG");}
\description
    	This fitting statistic can be used when dealing with
	 	low count spectra that have already been subtracted
		by the instrumental background. Cash statistics are
		not suited for this task since purely poisson distri-
		buted counts are assumed in that case. 
		This version of the fitting statistic has been derived
		from the profile likelihood statistic for zero back-
		ground counts from:

		https://heasarc.gsfc.nasa.gov/xanadu/xspec/manual/XSappendixStatistics.html

		The statistic should be used when fitting low count 
		spectra of the Swift/BAT instrument.
		IMPORTANT: the statistic can only be used when fitting
		count rate spectra since the exposure times for each
		channel have dummy values of 1s.
\seealso{set_fit_statistic}
\done

\function{pg_info}
\synopsis{Print a core dump of some useful pgplot and isis_fancy_plots information.}
\usage{pg_info; -or- pginfo;}
\description
\seealso{Nearly all isis_fancy_plot functions return a use message if invoked without arguments}
\done

\function{phasebin_GTIs}
\synopsis{computes the GTI time in phase bins}
\usage{Double_Type dt[] = phasebin_GTIs(tstart, tstop, T0, P, n);}
\done

\function{phaseOfTime}
\synopsis{Compute the phase corresponding to given time}
\usage{Double_Type[] phaseOfTime(Double_Type[] time, Double_Type, p, dp, ddp);}
\description
  Given a time and a period evolution by the period P \code{p},
  first, and second derivative of P, \code{dp} and \code{ddp},
  respectively, calculate the phase.

  Note: This function does a simple taylor expansion of the phase
  dphi = 1./P(t)dt. Time zero correpsonds to phase zero. For
  long time arrays this function suffers from precision errors.

  Since it is expected that the period changes monotonically, this
  function throws an error if the given time is outside of the extrema
  (if any). This only applies to non-zero \code{ddp}.
\seealso{timeOfPhase}
\done

\function{phflux (fit-function)}
\synopsis{fits the photon flux in a given energy range}
\description
    This function can be used as a convolution model to determine
    the photon flux [ph/s/cm^2] of the model in the energy range
    given by \code{E_min} and \code{E_max}. Only bins within(!) this energy range
    are considered for the calculation of the flux, thus the model
    should be evaluated on a grid including the values defining the
    energy range, i.e. \code{E_min} and \code{E_max} are elements
    of \code{[bin_lo, bin_hi]} of the grid. A user grid can be used
    for this purpose.
    As this function fits the normalization of the total convolved
    model, the normalizations of its components are not defined
    absolutely, but only relativ to each others. For that reason it
    is meaningful to freeze the normalization of one component, at
    best the one of the continuum to avoid ambiguities during
    fitting.

\examples
    % data definition:
    \code{variable lo = _A([1:10]);}
    \code{variable hi = make_hi_grid(lo);}
    \code{variable my_data = define_counts(lo,hi,lo,sqrt(lo));}\n
    \code{variable my_emin = 2.5;}
    \code{variable my_emax = 6.5;}
    % defining a grid containing the energy limits for the flux:
    % (in this way only valid if energy range is covered by the grid
    % and if these values are not already element of bin_lo, bin_hi)
    \code{define my_grid(id, s)}
    \code{\{}
    \code{   variable mygdc = get_data_counts(id);}
    \code{   mygdc.bin_lo = [mygdc.bin_lo,_A(my_emax),_A(my_emin)];}
    \code{   mygdc.bin_hi = [mygdc.bin_hi,_A(my_emax),_A(my_emin)];}
    \code{   s.bin_lo = mygdc.bin_lo[array_sort(mygdc.bin_lo)];}
    \code{   s.bin_hi = mygdc.bin_hi[array_sort(mygdc.bin_hi)];}
    \code{   return s;}
    \code{\}}
    
    \code{set_eval_grid_method (USER_GRID, my_data, &my_grid);}\n
    \code{fit_fun("phflux(1,powerlaw(1))");}

    \code{set_par("phflux(1).E_min",  my_emin,  1); % keV}
    \code{set_par("phflux(1).E_max",  my_emax,  1); % keV}
    \code{freeze("powerlaw(1).norm");}
    \code{()=fit_counts();}
    \code{list_par;}


    % It is also possible to determine only the flux of certain
    % model components, e.g., the unabsorbed flux:
    \code{fit_fun(phabs(1)*phflux(1, powerlaw(1)));}

\seealso{enflux, set_eval_grid_method}
\done

\function{photon_flux}

\synopsis{}
\usage{String_Type = photon_flux (hist_index, E_min, E_max);}
\description
	This function returns the integrated photon flux of
	a defined Energy range. 
	If no energy range specified E_min=0.5, E_max=10.

\seealso{get_data_flux;}
\done

\function{physical_quantity}
\synopsis{initializes a physical quantity with number and units}
\usage{PhysicalQuantity_Type physical_quantity(Double_Type number)}
\qualifiers{
\qualifier{leng}{length unit and dimension}
\qualifier{time}{time unit and dimension}
\qualifier{mass}{mass unit and dimension}
\qualifier{curr}{electrical current unit and dimension}
\qualifier{temp}{temperature unit and dimension}
\qualifier{unit}{array of compound units (see \code{physical_quantity_unit})}
}
\description
    The units are specified as strings with their dimensionality
    indicated by "^" as powers, i.e., "[unit]^[dim]".
    [dim] can be a negative number as well.
    For dim==1, "^1" can be omitted.
\example
    variable c = physical_quantity(299792.458; length="km", time="s^-1");
\seealso{physical_quantity_unit}
\done

\function{physical_quantity_current_in_A}
\synopsis{defines a new current unit or gets the conversion for an existing one}
\usage{physical_quantity_current_in_A(String_Type name, Double_Type value);
\altusage{Double_Type value = physical_quantity_current_in_A(String_Type name);}
}
\description
    The unit  \code{name} = \code{value} A  can be used in
       \code{physical_quantity(x; curr=name);  % = x * value} A
\seealso{physical_quantity}
\done

\function{physical_quantity_length_in_m}
\synopsis{defines a new length unit or gets the conversion for an existing one}
\usage{physical_quantity_length_in_m(String_Type name, Double_Type value);
\altusage{Double_Type value = physical_quantity_length_in_m(String_Type name);}
}
\description
    The unit  \code{name} = \code{value} m  can be used in
       \code{physical_quantity(x; leng=name);  % = x * value} m
\seealso{physical_quantity}
\done

\function{physical_quantity_mass_in_kg}
\synopsis{defines a new mass unit or gets the conversion for an existing one}
\usage{physical_quantity_mass_in_kg(String_Type name, Double_Type value);
\altusage{Double_Type value = physical_quantity_mass_in_kg(String_Type name);}
}
\description
    The unit  \code{name} = \code{value} kg  can be used in
       \code{physical_quantity(x; mass=name);  % = x * value} kg
\seealso{physical_quantity}
\done

\function{physical_quantity_temperature_in_K}
\synopsis{defines a new temperature unit or gets the conversion for an existing one}
\usage{physical_quantity_temperature_in_K(String_Type name, Double_Type value);
\altusage{Double_Type value = physical_quantity_temperature_in_K(String_Type name);}
}
\description
    The unit  \code{name} = \code{value} K  can be used in
       \code{physical_quantity(x; curr=name);  % = x * value} K
\seealso{physical_quantity}
\done

\function{physical_quantity_time_in_s}
\synopsis{defines a new time unit or gets the conversion for an existing one}
\usage{physical_quantity_time_in_s(String_Type name, Double_Type value);
\altusage{Double_Type value = physical_quantity_time_in_s(String_Type name);}
}
\description
    The unit  \code{name} = \code{value} s  can be used in
       \code{physical_quantity(x; time=name);  % = x * value} s
\seealso{physical_quantity}
\done

\function{physical_quantity_unit}
\synopsis{defines a new compound unit or gets the value of an existing one}
\usage{physical_quantity_unit(String_Type name, Double_Type value;; qualifiers);
\altusage{physical_quantity_unit(String_Type name, PhysicalQuantity_Type value);}
\altusage{PhysicalQuantity_Type value = physical_quantity_unit(String_Type name);}
}
\description
    If the second argument of the \code{physical_quantity_unit} function
    is a double value, the value of the unit is
    constructed with the \code{physical_quantity} function,
    i.e., all its qualifiers can be applied.

    The unit  \code{name} = \code{value}  can be used in
       \code{physical_quantity(x; unit=name);  % = x * value}
\seealso{physical_quantity}
\done

\function{planetpos}
\synopsis{Calculate the apparent position of the Sun, Moon, and the planets
 (Mercury,..., Neptune, and Pluto).}
\usage{(r,alp,del) = planetpos(JD;qualifiers);}
\altusage{(r,alp,del) = planetpos(JD,object;qualifiers);}
\description

 Returns the true geocentric distance (in AU), and apparent right ascension
 and declination of a planet in the geocentric celestial reference system for the
 date JD (in TT or TDB [the default]).
 
 Use the center-qualifier to choose another center object.

 The default coordinate system is the J2000.0 (ICRS) system, to include the
 effects of precession and nutation to get "apparent" positions, use the
 of_date qualifier, to get the mean equinox position (w/o nutation), use
 of_date together with the mean_equinox qualifier.

 The true geocentric distance is the distance the planet has at the time of the
 calculation, while the apparent position is corrected for light travel time
 effects. This is the default (and identical to the Astronomical Almanac).
 If the lighttravel qualifier is set, the function returns the
 distance the light traveled. In most cases, this is what you want.
 Use the "no_apparent" to switch off the correction for light travel time
 (e.g., useful for the calculation of heliocentric coordinates, where
 traditionally light travel time effects are are not taken into account)

 The default is the JPL ephemeris DE430 (as used in the astronomical almanac
 and as distributed with HEASOFT). Use the ephemeris qualifier to switch to
 another ephemeris.

 Alternatively, the less precise VSOP87 can be used (but not for Pluto and
 the Moon). There is no computing time advantage to doing so, however, but
 vsop87 is valid for several 1000 years around AD2000, while most JPL
 ephemerides are limited to less than that.

 This function has been checked against the values tabulated in the
 Astronomical Almanac for 2014 and 2015, which also uses DE430.
 The results match exactly.

 JD and object can be arrays. In this case the return value is
 an array of Vectors or three arrays of distance, RA and DEC,
 sorted by object first and by date next.

\qualifiers{
 \qualifier{object}{a string (valid values: Sun, Mercury, Venus,
                      Mars, Jupiter, Saturn, Uranus, Neptune, Pluto, Moon,
                      default: Sun).}
 \qualifier{center}{center object for the position. Default: Earth.
                      Either a string (same as "object"), or a
                      3D or 6D long double array. If 3D: position
                      in AU in a barycentric coordinate system in
                      ICRS coordinates (xyz), if 6D: position and
                      velocity (AU/D) in a barycentric coordinate system.}
 \qualifier{topocentric}{return topocentric positions; if this qualifier
                      is of type Vector_Type, then the xyz coordinates are
                      interpreted as the position of the observer in the
                      IERS 2010 coordinate system, in units of meters (see
                      function geographic2vector). Otherwise, a structure with
                      geographic coordinates is used, use tags lat, lon for
                      the geographic position (in degrees, east is positive),
                      and tag height as height (in meters) over the WGS 84
                      geoid (as appropriate for GPS coordinates).}
 \qualifier{of_date}{if set, return the apparent position in the geocentric
                       celestial reference system for the ecliptic and
                       equinox of date (this is what is listed in the
                       astronomical almanac).}
 \qualifier{mean_equinox}{if set together with of_date, return coordinates
                       for the mean equinox of date (i.e., ignore nutation).}
 \qualifier{no_apparent}{ignore light travel time effects}
 \qualifier{ecliptical}{return coordinates in ecliptical coordinates,
                        i.e., return r,lon,lat (or the xyz vector in ecliptical coords)
                       (also takes of_date qualifier into account)}
 \qualifier{vector}{return a geocentric/object centric direction (!) vector
                      for the position.}
 \qualifier{deg}{return the angles in degrees (default: radian).}
 \qualifier{mks}{return distance in m (default: AU)}
 \qualifier{cgs}{return distance in cm (default: AU)}
 \qualifier{lighttravel}{return distance the light traveled (default: true geocentric dist)}
 \qualifier{mjd}{the given date is the MJD}
 \qualifier{tt}{the given date is TT (default: TDB, difference only matters for pulsar work)}
 \qualifier{vsop87}{use the VSOP87 ephemeris rather than the JPL ephemeris, does not work
                      for Pluto and the Moon.}
}
\seealso{jpl_initeph,vsop87,geographic2vector}
\done

\function{plotGTI}
\synopsis{visualizes Good Time Intervals}
\usage{plotGTI(Struct_Type gti[, Double_Type offset]);}
\qualifiers{
\qualifier{ylevel}{plot GTIs at constant y-level}
\qualifier{overplot}{}
}
\done

\function{plotxy}
\synopsis{Generate a simple x/y plot with error bars (isis_fancy_plots package)}
\description

 plotxy(x,dxm,dxp,y,dym,dyp, pstruct); % pstruct = struct{dcol, decol, xrng, ...}
 plotxy(x,dxm,dxp,y,dym,dyp; dcol=#, decol=#, ...);

 Also accepts:

 plotxy(x,y [,pstruct;qualifiers]);
 plotxy(x,dxm,dxp,y [,pstruct;qualifiers]);
 plotxy(x,,,y,dym,dyp [,pstruct;qualifiers]);

   Plot simple x,y plots with error bars: x-dxm, x+dxp, etc.

   Options below refer to structure variables/associative keys/qualifiers:

   dcol  = (pgplot) color value for data
   decol = (pgplot) color value for data
   dsym  = (pgplot) symbol value for data
           Note: dsym=0 will *not* produce a histogram plot
   xrange= List of X-limits for the data. Ranges previously input
           via xrange(); will be respected if this option is not set.
   yrange= List of Y-limits for the data. Ranges previously input
           via yrange(); will be respected if this is not set.
   xlabel= String for the X-axis label
   ylabel= String for the Y-axis label
           Note: xlabel(); ylabel(); commands will also work.
   oplt    = 0 (default) for new plot, !=0 for overplotting

   Further note: plotxy(...); will apply the choices from connect_points(#);
\seealso{plot_counts, plot_data, plot_unfold, plot_residuals, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_atime}
\synopsis{plots the residuals of a pulse arrival times fit}
\usage{plot_atime([Integer_Type[] index]);}
\qualifiers{
    \qualifier{t0}{reference time of xticks}
    \qualifier{col}{array of colors for datasets}
    \qualifier{noerrbars}{do not plot errorbars}
    \qualifier{connect_points}{see 'connect_points'}
    \qualifier{style}{see 'point_style'}
    \qualifier{xunit}{used time unit (d or s)}
    \qualifier{yunit}{used residual unit (phi or s)}
    \qualifier{xlunit}{label of xunit}
    \qualifier{ylunit}{label of yunit}
    \qualifier{xrng}{time range}
    \qualifier{yrng}{residual range (default -0.5 to 0.5)}
    \qualifier{ploteph}{overplot the pulse ephemeris}
    \qualifier{plotorb}{overplot the orbital motion}
    \qualifier{plotpnum}{overplot the modelled pulse numbers
                     ('modnum' reference must be set,
                     see 'define_atime')}
    \qualifier{plotdif}{overplot the meassured pulse period
                     ('modnum' reference must be set,
                     see 'define_atime')}
    \qualifier{y2rng}{yrange of overplot}
    \qualifier{y2rel}{if pulse period tics are displayed
                     wrong, you should switch to relative
                     values and milliseconds}
    \qualifier{extcol}{color of overplot}
    \qualifier{extcolY}{yaxis color of overplot}
    \qualifier{oplot}{do not erase plot window}
    \qualifier{lshift}{if 'peph' and 'porb' set, shift the
                     labels printed at the ephemeris by
                     the given value in y-direction. May
                     be a two value array [eph,orb].
                     Values are interpreted as relative
                     coordinates.}
}
\description
    By default plots the residuals of all datasets
    containing arrival times. The residuals are
    calculated by
      residuals = data - model
    Using the optional
    first parameter the indices of the datasets to
    be plotted can be specified. Also works if
    some datasets are excluded from the fit (see
    'atime_xinclude').
    The residuals in units of pulse phase or seconds
    are plotted against the meassured pulse arrival
    times in days or seconds. The units can be set
    by using the accordant qualifiers. Further
    informations can be overplotted, like the
    pulse period in the binary barycentre.
\seealso{arrtimes, atime_dataind, atime_xinclude, plot_data}
\done

\function{plot_combined_data_model_residuals}
\synopsis{plots data and model counts, and residuals for a combination of data sets}
\usage{plot_combined_data_model_residuals([Integer_Type id[]]);}
\qualifiers{
\qualifier{dcol}{color of data [default=1]}
\qualifier{mcol}{color of model [default=2]}
}
\description
    If no indices \code{id} of data sets are specified, \code{all_data} are used.\n
    The combined data, model and residuals are plotted in a two panel multiplot.
\seealso{get_combined_data_model_residuals}
\done

\function{plot_component}
\synopsis{to plot modelfit parameters of different components}
\usage{plot_component([array of \code{struct comp}]);}
\qualifiers{
\qualifier{outputfile}{outputfile name and directory by hand}
\qualifier{mjd_min}{[=53736.0] lower limit of time axis in mjd (default: 1/1/2006)}
\qualifier{mjd_max}{[=55927.0] upper limit of time axis in mjd (default: 1/1/2012)}
\qualifier{date_min}{[=2006.0] lower limit of time axis (default: 1/1/2006)}
\qualifier{date_max}{[=2012.0] upper limit of time axis (default: 1/1/2012)}
\qualifier{distance_min}{[=-2] lower limit of distance axis in mas}
\qualifier{distance_max}{[=20] upper limit of distance axis in mas}
\qualifier{flux_min}{[=1e-5] lower limit of flux axis in Jy}
\qualifier{flux_max}{[=5] upper limit of flux axis in Jy}
\qualifier{TB_min}{[=10^5] lower limit of brightness temperature axis in K}
\qualifier{TB_max}{[=10^15] upper limit of brightness temperature axis in K}
\qualifier{size_min}{[=1e-3] lower limit of size axis in mas^2}
\qualifier{size_max}{[=100] upper limit of size axis in mas^2}
\qualifier{pos_min}{[=-180] lower limit of pos angle in degrees}
\qualifier{pos_max}{[=180] upper limit of pos angle in degrees}
\qualifier{labels}{[=default] provide array of custom labels for the components}
\qualifier{sym}{[=default] provide array of custom symbols for the components}
\qualifier{symcolor}{[=default] provide array of custom colors for the components}
\qualifier{symsize}{[=1] provide array of custom symbol sizes or one value to apply size to all}
\qualifier{legend}{create a legend}
\qualifier{lpos_x}{set the x position of the legend in world0 system (0 = left, 1=right)}
\qualifier{lpos_y}{set the y position of the legend in world0 system (0 = bottom, 1=top)}
\qualifier{linestyle}{[=default] set to 0 if the flux values should not be conntected (or to another value for 
 					another line style}
\qualifier{nocomp}{provide a comp structure (or array of structures) to plot not identified components in black}
\qualifier{plots}{[=[9,10,3,4,11,12]]: provide an array of numbers to generated specified plots:
					1:	distance vs mjd
					2:	flux vs mjd
					3:	T_B vs distance
					4:	flux vs distance
					5:	size vs distance 
					6:	pos angle vs distance	
					7:	size vs mjd
					8:	pos angle vs mjd
					9:	distance vs time
					10:	flux vs time
					11:	size vs time
					12:	pos_angle vs time }
}
\description
This functions creates overview plots for modelfit components. The required input format is the comp structure which can be obtained with get_component or an array of the comp structure.
\done

\function{plot_comps}
\synopsis{Create a data plot with model components explicitly shown (isis_fancy_plots package)}
\usage{plot_comps({data},&plot_func;dcol={val},mcol={val},ccol={val},cstyle=val,...);}
\altusage{plot_comps({data},pstrut,&plot_func); where pstruct=struct{dcol, mcol, ...}}
\description
  Use a fancy plotting function, e.g., plot_counts or plot_data or
  plot unfold, passed as a reference, and cycle through all the
  components with a norm parameter.  Plot each of these as a separate
  model component.  The plot functions now take two additional
  optional qualifiers (which alternatively can be passed via the
  pstruct structure variable): ccol and cstyle.  The ccol parameter
  gives the color of the model components for each dataset, which can
  be different from the color of the model for the complete model.
  The cstyle allows a global change of the line_style for *all* of the
  model components.  (I.e., only one alternate line_style can be
  chosen.)  data is the usual combination of integers (=individual
  data sets), arrays (=data sets to be combined), and lists (=id of
  combined datasets).

\examples
    plot_comps({1,[2,3]},popt,&plot_counts;xrange={1,10});
    plot_comps({5,8},&plot_unfold);
    plot_comps({{1}},popt,&plot_data);
\seealso{plot_counts, plot_data, plot_unfold, plot_residuals, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_contour_trq}
\synopsis{creates a contour plot out of the results from function
    contour_trq.}
\usage{plot_contour_trq(String_Type inputDir);}
\qualifiers{
\qualifier{out}{   post script outputname}
\qualifier{image}{ fits image outputname}
\qualifier{x}{     x label}
\qualifier{y}{     y label}
}
\description
       - \code{inputDir} directory where all torque job files are,
                  created by function contour_trq.

       Be aware of the issues with the torque (look at the description
       of contour_trq function), as it may not have all bins^2
       values, and hence give wrong contour plot.
        
       The default post script file name is cont_default.ps.
       Output fits image file is cont_default.fits.

 EXAMPLE


       plot_contour_trq("fileDirectory";out="my_out",x="x_lab",y="y_lab");

       creates a plot "my_out.ps" with x-axis label named "x_lab" and y_axis
       label named "y_lab".                   
\seealso{contour_trq, missing_contour_trq}
\done

\function{plot_counts}
\synopsis{Plot counts per bin (isis_fancy_plots package)}
\description

plot_counts({indx,[arry],{cid}},pstruct);  % pstruct = struct{ dcol, mcol, rcol, ...}
plot_counts({indx,[arry],{cid}};dcol={val},mcol={val},rcol={val,[arry],val},...);

  Plot background subtracted data, model, and residuals as counts/bin
  Residuals are units of chi, chi2, or ratio, and will be based upon whether
  one chooses sigma=model, data, or gehrels in set_fit_statistic();
  (data error bars are only affected by the latter two).
  set_fit_statistic("cash"); will alter the residuals to the Cash statistic.
  set_fit_statistic("ml"); will alter the residuals to the Maximum Likelihood statistic.

  Options below refer to structure variables/qualifiers

   indx    = list of data set indices to be plotted. Any indices grouped in
             an array within that list will be *combined* in the data plot.
             Single number in list is combo id, {#} = [combination_members(#)].
   dcol    = (pgplot) color value for data (or list of color values)
   decol   = (pgplot) color value for data error bars (or list of color values)
   mcol    = (pgplot) color value for model (or list of color values)
             0 => No model plotted
   rcol    = (pgplot) color value for residuals (or list of color values; arrays
             within the list allow for individual color values if portions of
             the data are combined, but their associated residuals are not)

   recol   = color for residual error bars (or list of color values; arrays
             within the list act as for residual color inputs)
   dsym    = (pgplot) symbol value for data (or list of symbol values)
             0 => histogram plot
   rsym    = (pgplot) symbol value for residuals (or list of symbol values;
             arrays within the list act as for residual color inputs)
             0 => histogram plot
   xrange  = List of X-limits for the data & model & residuals
             Note: Any X- or Y-range set to NULL is autoscaled
   yrange  = List of Y-limits for the data & model and (optionally) residuals
   oplt    = 0 (default) for new plot, !=0 for overplotting
   no_reset= 0 (default)- plots *will* be reset, i.e., next plot moves to new pane
             (multiplot), next plot redraws window (single plot). no_reset=1 necessary
             for overplotting multiplots (oplt=1 sufficient for single plots).
   res     = 0 (default), no residuals; 1, 2, or 3 = chi, chi2, or ratio residuals
             4, 5, or 6 = chi, chi2, or ratio, but combine residuals for combined data
             set_fit_method("cash"); or set_fit_method("ml") will cause res=(2 or 5) 
             or res=(1 or 4) to plot the residual for the Cash or Maximum Likelihood 
             statistic or its square root, respectively
   power  = 0, 1 (default), 2, or 3 for Counts/bin X
            (1/Unit, 1, Unit, Unit^2), respectively
   bkg     = List of 0's (subtract background-default), 1's (include backgrounds),
             or -1's (plot *only* the background [no model plotted in this case]).
             Ratio residuals will include background in data/model, other residuals
             are unaffected. Indices within a combination are treated the same.)
   xlabel  = String that will overwrite default X-axis label (default=NULL)
   ylabel  = String or string array that will overwrite the default Y-axis labels
             (second element of array applies to residuals; default=NULL)
   zshift  = List of redshifts to be applied to the data (default zshift={0,0,...})
   vzero   = If set, the reference X-unit value to be defined as zero velocity.
             The X-axis then becomes a velocity axis (km/s) referenced to this
             point (default vzero=NULL; setting vzero/zaxis supersedes zshift)
   zaxis   = If not 0, use a redshift axis instead of a velocity axis *if* vzero
             is defined (default zaxis=0)
   scale   = Multiplicatively scale the Y-axis by the values in a list.
             Any arrays in the list should hold the individual scalings for
             data set arrays in the input index list.  **Note:**  these values
             only scale the plots, not the fits.  (Default values are 1.)
   gap     =  1 (default), models are histograms with gaps where data has gaps,
              0          , model are bin-centered lines, without gaps.
\seealso{plot_data, plot_unfold, plot_residuals, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_data}
\synopsis{Plot counts per unit per second (isis_fancy_plots package)}
\description

plot_data({indx,[arry],{cid}},pstruct);  % pstruct = struct{ dcol, mcol, rcol, ...}
plot_data({indx,[arry],{cid}};dcol={val},mcol={val},rcol={val,[arry],val},...);

  Plot background subtracted data, model, and residuals as counts/xunit/sec.
  Residuals are units of chi, chi2, or ratio, and will be based upon whether
  one chooses sigma=model, data, or gehrels in set_fit_statistic();
  (data error bars are only affected by the latter two).
  set_fit_statistic("cash"); will alter the residuals to the Cash statistic.
  set_fit_statistic("ml"); will alter the residuals to the Maximum Likelihood statistic.

  Options below refer to structure variables/qualifiers

   indx    = list of data set indices to be plotted. Any indices grouped in
             an array within that list will be *combined* in the data plot.
             Single number in list is combo id, {#} = [combination_members(#)].
   dcol    = (pgplot) color value for data (or list of color values)
   decol   = (pgplot) color value for data error bars (or list of color values)
   mcol    = (pgplot) color value for model (or list of color values)
             0 => No model plotted
   rcol    = (pgplot) color value for residuals (or list of color values; arrays
             within the list allow for individual color values if portions of
             the data are combined, but their associated residuals are not)
   recol   = color for residual error bars (or list of color values; arrays
             within the list act as for residual color inputs)
   dsym    = (pgplot) symbol value for data (or list of symbol values)
             0 => histogram plot
   rsym    = (pgplot) symbol value for residuals (or list of symbol values;
             arrays within the list act as for residual color inputs)
             0 => histogram plot
   xrange  = List of X-limits for the data & model & residuals
             Note: Any X- or Y-range set to NULL is autoscaled
   yrange  = List of Y-limits for the data & model and (optionally) residuals
   oplt    = 0 (default) for new plot, !=0 for overplotting
   no_reset= 0 (default)- plots *will* be reset, i.e., next plot moves to new pane
             (multiplot), next plot redraws window (single plot). no_reset=1 necessary
             for overplotting multiplots (oplt=1 sufficient for single plots).
   res     = 0 (default), no residuals; 1, 2, or 3 = chi, chi2, or ratio residuals
             4, 5, or 6 = chi, chi2, or ratio, but combine residuals for combined data
             set_fit_method("cash"); or set_fit_method("ml") will cause res=(2 or 5) 
             or res=(1 or 4) to plot the residual for the Cash or Maximum Likelihood 
             statistic or its square root, respectively
   bkg     = List of 0's (subtract background-default), 1's (include backgrounds),
             or -1's (plot *only* the background [no model plotted in this case]).
             Ratio residuals will include background in data/model, other residuals
             are unaffected. Indices within a combination are treated the same.)
   xlabel  = String that will overwrite default X-axis label (default=NULL)
   ylabel  = String or string array that will overwrite the default Y-axis labels
             (second element of array applies to residuals; default=NULL)
   zshift  = List of redshifts to be applied to the data (default zshift={0,0,...})
   vzero   = If set, the reference X-unit value to be defined as zero velocity.
             The X-axis then becomes a velocity axis (km/s) referenced to this
             point (default vzero=NULL; setting vzero/zaxis supersedes zshift)
   zaxis   = If not 0, use a redshift axis instead of a velocity axis *if* vzero
             is defined (default zaxis=0)
   scale   = Multiplicatively scale the Y-axis by the values in a list.
             Any arrays in the list should hold the individual scalings for
             data set arrays in the input index list.  **Note:**  these values
             only scale the plots, not the fits.  (Default values are 1.)
   sum_exp = If==1, then when combining data sets, sum the exposure times (as
             opposed to using the mean exposure time; default sum_exp=1).
   gap     =  1 (default), models are histograms with gaps where data has gaps,
              0          , model are bin-centered lines, without gaps.
\seealso{plot_data, plot_unfold, plot_residuals, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_double}
\synopsis{Use two different plot functions in the same figure (isis_fancy_plots package)}
\description

plot_double({data},pstruct,&plot_funcI,&plot_funcII);  % pstruct=struct{dcol, mcol, ...}
plot_double({data},&plot_funcI,&plot_funcII;dcol={val},mcol={val},ccol={val},...);

  Using fancy plotting functions, e.g., plot_counts or plot_data or
  plot unfold, passed as references, apply plot_funcI in the upper
  panel and plot_funcII in the lower panel.  (If residuals are chosen
  to be displayed, include a third panel for them beneath the first
  two plots.)  In each plot, cycle through all the components with a
  norm parameter.  Plot each of these as a separate model component.
  The plot functions now take two additional optional qualifiers
  (which alternatively can be passed via the pstruct structure
  variable): ccol and cstyle.  The ccol parameter gives the color of
  the model components for each dataset, which can be different from
  the color of the model for the complete model.  The cstyle allows a
  global change of the line_style for *all* of the model components.
  (I.e., only one alternate line_style can be chosen.)  data is the
  usual combination of integers (=individual data sets), arrays (=data
  sets to be combined), and lists (=id of combined datasets). Plot
  parameters retain their usual meaning, with the exception of yrange
  and power.  yrange is now a list of up to 6 elements, with the first
  two applying to plot_funcI, the next two applying to plot_funcII,
  and the final two applying to the residuals. If power is a list of
  two elements, then the first element applies to plot_funcI and the
  second element applies to plot_funcII.

\examples
    plot_double({1,[2,3]},popt,&plot_unfold,&plot_counts;xrange={1,10});
    plot_double({5,8},&plot_unfold,&plot_data);
    plot_double({{1}},popt,&plot_data,&plot_counts);
\seealso{plot_counts, plot_data, plot_unfold, plot_residuals, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_fit_model}
\synopsis{Plot background subtracted model as counts/xunit/sec (isis_fancy_plots package).}
\description

plot_fit_model({indx,[arry],{cid}},pstruct);  % pstruct = struct{ mcol, ...}
plot_fit_model({indx,[arry],{cid}};mcol={val},...);

  Plot background subtracted model as counts/xunit/sec.

  Options below refer to structure variables/qualifiers

   indx    = list of data set indices to be plotted. Any indices grouped in
             an array within that list will be *combined* in the data plot.
             Single number in list is combo id, {#} = [combination_members(#)].
   mcol    = (pgplot) color value for model (or list of color values)
             0 => No model plotted
   xrange  = List of X-limits for the data & model & residuals
             Note: Any X- or Y-range set to NULL is autoscaled
   yrange  = List of Y-limits for the data & model and (optionally) residuals
   oplt    = 0 (default) for new plot, !=0 for overplotting
   no_reset= 0 (default)- plots *will* be reset, i.e., next plot moves to new pane
             (multiplot), next plot redraws window (single plot). no_reset=1 necessary
             for overplotting multiplots (oplt=1 sufficient for single plots).
   bkg     = List of 0's (subtract background-default), 1's (include backgrounds),
             or -1's (plot *only* the background [no model plotted in this case]).
             Ratio residuals will include background in data/model, other residuals
             are unaffected. Indices within a combination are treated the same.)
   xlabel  = String that will overwrite default X-axis label (default=NULL)
   ylabel  = String or string array that will overwrite the default Y-axis labels
             (second element of array applies to residuals; default=NULL)
   zshift  = List of redshifts to be applied to the data (default zshift={0,0,...})
   vzero   = If set, the reference X-unit value to be defined as zero velocity.
             The X-axis then becomes a velocity axis (km/s) referenced to this
             point (default vzero=NULL; setting vzero/zaxis supersedes zshift)
   zaxis   = If not 0, use a redshift axis instead of a velocity axis *if* vzero
             is defined (default zaxis=0)
   scale   = Multiplicatively scale the Y-axis by the values in a list.
             Any arrays in the list should hold the individual scalings for
             data set arrays in the input index list.  **Note:**  these values
             only scale the plots, not the fits.  (Default values are 1.)
   sum_exp = If==1, then when combining data sets, sum the exposure times (as
             opposed to using the mean exposure time; default sum_exp=1).
   gap     =  1 (default), models are histograms with gaps where data has gaps,
              0          , model are bin-centered lines, without gaps.
\seealso{plot_counts; plot_data, plot_unfold, plot_residuals, plotxy, plot_comps, plot_double}
\done

\function{plot_jet_speed}
\synopsis{visualizes the jet_speed_struct used by init_jet_fit}
\usage{plot_jet_speed (Struct_Type jet_speed_struct);}
\qualifiers{
\qualifier{xrange}{set the xrange (provide a list or array)}
\qualifier{yrange}{set the yrange (provide a list or array)}
}
\seealso{init_jet_fit}
\done

\function{plot_residuals}
\synopsis{Plot the data residuals (isis_fancy_plots package)}
\description

plot_residuals({indx,[arry],{cid}},pstruct);  % pstruct = struct{ rcol, rsym, ...}
plot_residuals({indx,[arry],{cid}};dcol={val},rcol={val,[arry],val},rsym=...);

  Plot data residuals, without plotting the data.  Residuals related to data
  indices in [arry] will appear in a single ascii file if write_plot(); is
  used, and will be combined in the plot if res>3 is chosen.

  Options below refer to structure variables/qualifiers

   indx    = list of data set indices to be plotted. Any indices grouped in
             an array within that list will be *combined* in the data plot.
             Single number in list is combo id, {#} = [combination_members(#)].
   rcol    = (pgplot) color value for residuals (or list of color values; arrays
             within the list allow for individual color values if portions of
             the data are combined, but their associated residuals are not)
   recol   = color for residual error bars (or list of color values; arrays
             within the list act as for residual color inputs)
   rsym    = (pgplot) symbol value for residuals (or list of symbol values;
             arrays within the list act as for residual color inputs)
             0 => histogram plot
   xrange  = List of X-limits for the data & model & residuals
             Note: Any X- or Y-range set to NULL is autoscaled
   yrange  = List of Y-limits for the data & model and (optionally) residuals
   oplt    = 0 (default) for new plot, !=0 for overplotting
   no_reset= 0 (default)- plots *will* be reset, i.e., next plot moves to new pane
             (multiplot), next plot redraws window (single plot). no_reset=1 necessary
             for overplotting multiplots (oplt=1 sufficient for single plots).
   res     = 0 (default), no residuals; 1, 2, or 3 = chi, chi2, or ratio residuals
             4, 5, or 6 = chi, chi2, or ratio, but combine residuals for combined data
             set_fit_method("cash"); or set_fit_method("ml") will cause res=(2 or 5) 
             or res=(1 or 4) to plot the residual for the Cash or Maximum Likelihood 
             statistic or its square root, respectively
   xlabel  = String that will overwrite default X-axis label (default=NULL)
   ylabel  = String or string array that will overwrite the default Y-axis labels
             (second element of array applies to residuals; default=NULL)
   zshift  = List of redshifts to be applied to the data (default zshift={0,0,...})
   vzero   = If set, the reference X-unit value to be defined as zero velocity.
             The X-axis then becomes a velocity axis (km/s) referenced to this
             point (default vzero=NULL; setting vzero/zaxis supersedes zshift)
   zaxis   = If not 0, use a redshift axis instead of a velocity axis *if* vzero
             is defined (default zaxis=0)
\seealso{plot_counts, plot_data, plot_unfold, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_spix}
\synopsis{plots a spectral index map using the struct provided by make_spix}
\usage{plot_spix(Struct_Type values, String_Type outputname);}
\qualifiers{
\qualifier{inv_color}{[=0] inverts color scale (i.e., roles of red and blue switch)}
\qualifier{min_spix}{[=min(values.spec_map)] minimum spectral index to be plotted}
\qualifier{max_spix}{[=max(values.spec_map)] maximum spectral index to be plotted}
\qualifier{lum_scale}{[=1] use the sum of the intensity of both images to scale
                      the brightness of the spectral index map, set to 0 in order to
                      show the significant regions at uniform brightness}
\qualifier{nsigma [=0]}{spectral index values are only shown for pixels where
                      the emission (in both bands) is above the specified
                      significance (e.g., nsigma=3 means that the emission
                      has to be 3*sigma above the noise level)}
\qualifier{nsigma_lo}{[=0] emission limit for low frequency  band separately}
\qualifier{nsigma_hi}{[=0] emission limit for high frequency band separately}
\qualifier{low_lum_limit}{[=0] number of stdevs above the median that luminosity data
                                        begins to be displayed}
\qualifier{hi_lum_limit}{[=0] number of (e^n) stdevs above the median at which color
                                        is fully saturated (0 -> no limit)}
\qualifier{ra_frac}{[={0.0,1.0}] fraction of RA to display with {left,right} limits,
                                        will keep scaling}
\qualifier{dec_frac}{[={0.0,1.0}] fraction of DEC to display with {left,right} limits,
                                        will keep scaling}
\qualifier{ra_mas}{[={NULL,NULL}] {left,right} limits of image in mas}
\qualifier{dec_mas}{[={NULL,NULL}] {top,bottom} limits of image in mas}
\qualifier{size}{[=14] size of plot (maximum dimension)}
\qualifier{print_mode}{[=0] set =1 for white background}
\qualifier{blue_bkgr}{[=0] for a dark blue background set to 1}
\qualifier{quadratic}{create a quadratic image, by default the scaling is such that
                                        scaling in RA and DEC is equal}
\qualifier{no_labels}{plot no labels}
\qualifier{pmodecolor}{[=1] color saturation decreases wth this root of luminosity
                                        (i.e., 2=sqrt) in print mode only}
\qualifier{beam_color}{[="gray"] beam color}
\qualifier{src_name}{[=values.source] manually set source name in plot}
\qualifier{obs_date}{[=values.date] manually set date in plot}
\qualifier{xyunit}{[="mas"] unit of the x/y labels, can be changed between "mas", "arcsec", "arcmin" and "deg";
                                               assumption: FITS header sets unit "mas"}                                       
\qualifier{xfig_tmp_dir}{[=xfig_get_tmp_dir] set the path for the tmp directory used by xfig}
\qualifier{xfig_autoeps_dir}{[=xfig_get_autoeps_dir] set the path for the autoeps directory}
\qualifier{color_scheme}{[="hot"] setting another colorscheme (lum_scale=0 is automatically set)}
\qualifier{h_scalebar}{if given, a horizontal scalebar is plotted above the spixmap}
\qualifier{no_scalebar}{if given, no scalebar is plotted}
}
\description
    This function creates a spectral image plot with xfig from the
    structure generated by make_spix. The log of the average brightness
    is displayed as color saturation and the spectral index as hue so
    that both spectral index and brightness are apparent in the image.
    It will ouput a .pdf file of the plot with a color scale bar on the
    side.  For the luminosity scaling, the data below the median value
    is cut out and the data above the median is fit to a gaussian. The
    hi_lum_limit and low_lum_limit are based on the stdev from this fit.
\seealso{make_spix, read_spix, write_spix}
\done

\function{plot_struct_arrays}
\synopsis{plots the fields of a structure agains each other}
\usage{plot_struct_arrays(Struct_Type s, String_Type fieldnameX, String_Type fieldnameY);}
\qualifiers{
\qualifier{xoffset}{}
\qualifier{yoffset}{}
}
\description
    \code{plot(s.fieldnameX - xoffset,  s.fieldnameY - yoffset);}
\done

\function{plot_table_columns}
\synopsis{plots columns of a table against each other}
\usage{plot_table_columns(Struct_Type table);}
\qualifiers{
\qualifier{x}{array of columns to be used for the x-axis [default: all columns]}
\qualifier{y}{array of columns to be used for the y-axis [default: all columns]}
\qualifier{path}{path to save the postscript plots}
\qualifier{multiplot}{produce one single multiplot (does not call open_plot)}
}
\done

\function{plot_unfold}
\synopsis{Plot flux-corrected spectra (isis_fancy_plots package).}
\description

plot_unfold({indx,[arry],{cid}},pstruct);  % pstruct = struct{ dcol, mcol, rcol, ...}
plot_unfold({indx,[arry],{cid}};dcol={val},mcol={val},rcol={val,[arry],val},...);

  Plot background subtracted unfolded data, model, and residuals using a
  variety of X- and Y-units set by fancy_plot_unit(xunit, [yunit]);
  Residuals are units of chi, chi2, or ratio, and will be based upon whether
  one chooses sigma=model, data, or gehrels in set_fit_statistic();
  (data error bars are only affected by the latter two).
  set_fit_statistic("cash"); will alter the residuals to the Cash statistic.
  set_fit_statistic("ml"); will alter the residuals to the Maximum Likelihood statistic.

  Options below refer to structure variables/qualifiers

   indx    = list of data set indices to be plotted. Any indices grouped in
             an array within that list will be *combined* in the data plot.
             Single number in list is combo id, {#} = [combination_members(#)].
   dcol    = (pgplot) color value for data (or list of color values)
   decol   = (pgplot) color value for data error bars (or list of color values)
   mcol    = (pgplot) color value for model (or list of color values)
             0 => No model plotted
   rcol    = (pgplot) color value for residuals (or list of color values; arrays
             within the list allow for individual color values if portions of
             the data are combined, but their associated residuals are not)
   recol   = color for residual error bars (or list of color values; arrays
             within the list act as for residual color inputs)
   dsym    = (pgplot) symbol value for data (or list of symbol values)
             0 => histogram plot
   rsym    = (pgplot) symbol value for residuals (or list of symbol values;
             arrays within the list act as for residual color inputs)
             0 => histogram plot
   xrange  = List of X-limits for the data & model & residuals
             Note: Any X- or Y-range set to NULL is autoscaled
   yrange  = List of Y-limits for the data & model and (optionally) residuals
   oplt    = 0 (default) for new plot, !=0 for overplotting
   no_reset= 0 (default)- plots *will* be reset, i.e., next plot moves to new pane
             (multiplot), next plot redraws window (single plot). no_reset=1 necessary
             for overplotting multiplots (oplt=1 sufficient for single plots).
   res     = 0 (default), no residuals; 1, 2, or 3 = chi, chi2, or ratio residuals
             4, 5, or 6 = chi, chi2, or ratio, but combine residuals for combined data
             set_fit_method("cash"); or set_fit_method("ml") will cause res=(2 or 5) 
             or res=(1 or 4) to plot the residual for the Cash or Maximum Likelihood 
             statistic or its square root, respectively
   power  = 0, 1 (usual default), 2 (default for mJy) or 3 (default for ergs/Watts
            vs. energy units)=> photons/cm^2/s/xunit *(1/xunit,1,xunit,xunit^2)
   bkg     = List of 0's (subtract background-default), 1's (include backgrounds),
             or -1's (plot *only* the background [no model plotted in this case]).
             Ratio residuals will include background in data/model, other residuals
             are unaffected. Indices within a combination are treated the same.)
   xlabel  = String that will overwrite default X-axis label (default=NULL)
   ylabel  = String or string array that will overwrite the default Y-axis labels
             (second element of array applies to residuals; default=NULL)
   zshift  = List of redshifts to be applied to the data (default zshift={0,0,...})
   vzero   = If set, the reference X-unit value to be defined as zero velocity.
             The X-axis then becomes a velocity axis (km/s) referenced to this
             point (default vzero=NULL; setting vzero/zaxis supersedes zshift)
   zaxis   = If not 0, use a redshift axis instead of a velocity axis *if* vzero
             is defined (default zaxis=0)
   scale   = Multiplicatively scale the Y-axis by the values in a list.
             Any arrays in the list should hold the individual scalings for
             data set arrays in the input index list.  **Note:**  these values
             only scale the plots, not the fits.  (Default values are 1.)
   gap     =  1 (default), models are histograms with gaps where data has gaps,
              0          , model are bin-centered lines, without gaps.
   con_mod= 1 (default), the smear the model by the detector response, otherwise 
            plot the unsmeared model at the internal resolution of the arf

  Note: Model flux is: ( \\\\int dE S(E) )/dE, while data is 
  (C(h) - B(h))/(\\\\int R(h,E) A(E) dE)/dh/t, where A(E) is effective area, 
  R(h,E) is RMF, C(h)/B(h) are total/background counts. Thus, the data 
  and model will match best only in the limit of a delta function RMF, 
  and in fact might look different than the residuals (which is the 
  only proper comparison between data and model, anyhow).
\seealso{set_power_scale, plot_counts, plot_data, plot_residuals, plot_fit_model, plotxy, plot_comps, plot_double}
\done

\function{plot_vlbi_map}
\synopsis{creates an image of the VLBI map with transparent background using a .fits file (provided by DIFMAP)}
\usage{plot_vlbi_map(String_Type \code{fitsfile}, [String_Type \code{fitsfile (polflux)}], String_Type \code{filename})}
\altusage{plot_vlbi_map(Struct_Type \code{img_struct}, String_Type \code{filename})}
\qualifiers{
\qualifier{color_scheme [="ds9b"]}{  select a color scheme for the image}
\qualifier{dec_frac [={0.0,1.0}]}{   declination range of the image by
                                selecting a fraction of the file's range}
\qualifier{ra_frac [={0.0,1.0}]}{    right ascension range of the image by
                                selecting a fraction of the file's range}
\qualifier{dec_mas [={min,max}]}{    set declination range of the image directly in mas.
                                overwrites \code{dec_frac}}
\qualifier{ra_mas [={min,max}]}{     set right ascension range of the image directly in mas
                                (analog to \code{dec_mas})}
\qualifier{fit_noise [=1]}{          fit the noise of a map}                                
\qualifier{n_sigma [=3.0]}{          start color scaling at \code{n_sigma*sigma}}
\qualifier{cont_scl [=2.0]}{         set factor to change the separation between\n
                                contour levels, the levels are placed at:\n
                                \code{cont_scl^[0,1,...]*n_sigma*sigma}}
\qualifier{cont_lvl [=[c1,c2,..]}{   set contour levels [Jy] manually, overwrites\n
                                other contour parameters}
\qualifier{cont_width [=2]}{         set width of contour lines}
\qualifier{plot_cont [=1]}{          set to 0 in order to plot without contour lines}
\qualifier{cont_depth [=2]}{         set contour-line depth}
\qualifier{plot_vec [=0]}{           set to 1 in order to plot with vectors
                                (requires second fits file with polarized flux density)}
\qualifier{vec_density [=5]}{        density of EVPA vectors; possible values: [1..10]}
\qualifier{vec_width [=1]}{          width of EVPA vectors}
\qualifier{vec_color [="black"]}{    color of EVPA vectors}
\qualifier{plot_clr_img [=1]}{       set to 0 in order to plot without colored image}
\qualifier{plot_clr_key [=1]}{       set to 0 in order to plot without key for color scale}
\qualifier{plot_scale_arrows [=1]}{  set to 1 in order to plot with arrows denoting the size scalesin the map}
\qualifier{clrcut [=0]}{             set to 1 in order to start the color scale at the
                                significant emission (i.e., no color scaling below)}
\qualifier{clrmin [=min(image)]}{    set minimum value [Jy] for color scale (see png_gray_to_rgb)}
\qualifier{clrmax [=max(image)]}{    set maximum value [Jy] for color scale (see png_gray_to_rgb)}
\qualifier{clrmu [from fit_gauss_to_img_noise]}{set noise level of the image [Jy] for color scale
                                (mean value of regions without significant emission)}
\qualifier{clrsig [from fit_gauss_to_img_noise]}{set width of noise (1-sigma) [Jy] for color scale,
                                for identical color scale (e.g., in order to compare
                                different images) the following qualifiers have to be set:
                                clrmin, clrmax, clrmu, clrsig}
\qualifier{colmap_depth}{            set depth of png map}
\qualifier{bkg_white}{               set white map below clrmin, if clrcut is set, clrmin equals
                                n_sigma above the mean background.}
\qualifier{quadratic}{               create a quadratic image, by default the scaling is such that
                                scaling in RA and DEC is equal}
\qualifier{plotsize [=14]}{          size of the plot (scaling of image relative to font size)}
\qualifier{src_name [=default]}{     set the name of the source, by default the name\n
                                is read from the .fits file
                                set to NULL for not plotting a source name}
\qualifier{obs_date [=default]}{     set the observation date, by default the observation\n
                                date is read from the .fits file
                                set to NULL for not plotting a observation date}
\qualifier{axis_color [="gray"]}{    set axis color}
\qualifier{source_color [="gray"]}{  set color of source name}
\qualifier{date_color [="gray"]}{    set color of observation date}
\qualifier{date_depth [=1]}{         set depth of observation date}
\qualifier{source_depth [=1]}{       set depth of source name}
\qualifier{cont_color [="gray"]}{    set color of contour lines}
\qualifier{neg_cont_color [="gray"]}{set color of contour lines below noise level}
\qualifier{date_xy [=[0.95,0.95]]}{  set world0 coordinates of date string}
\qualifier{plot_beam [=1]}{          set to 1 in order to plot the beam}
\qualifier{plot_components}{         set to plot model components on top of image}
\qualifier{model_circs}{             set to plot model component circles on top of image}
\qualifier{pos_comp_color [="seagreen"]}{set for color of model components with positive flux}
\qualifier{neg_comp_color [="seagreen"]}{set for color of model components with negative flux}
\qualifier{beam_color [="gray"]}{    set color of the beam}
\qualifier{no_labels}{:	        plot no labels}
\qualifier{colmap_label}{:	        set label of colormap}
\qualifier{neglog [=0]}{             use log scale for negative flux values}
\qualifier{frac [=0.1]}{:	        fraction to which the linear region below n_sigma*sigma
                                is scaled in the color scale (if clrcut != 0)}
\qualifier{funit [="mJy"]}{          unit of the color scale bar, can be changed from "mJy" to "Jy"}
\qualifier{xyunit [="mas"]}{         unit of the x/y labels, can be changed between "mas", "arcsec", "arcmin" and "deg";
                                       assumption: FITS header sets unit "mas"}
\qualifier{return_xfig}{             set qualifier to return the xfig object(s) instead than
                                rendering directly}
}
\description
    This function creates an image of a jet using the .fits file provided
    by DIFMAP.
    Alternatively a structure with the required fields (as obtained
    with the function \code{read_difmap_img}) can be given directly to
    \code{plot_vlbi_map} instead of the name of a fits file.
    The color scheme can be selected and contour lines are calculated.
    The format of the output file depends on the suffix of the given
    \code{filename}. Possible formats of the output file are PDF, EPS,
    PNG, GIF, etc.

    If plot_vlbi_map is called with three arguments, the first two of them beiing maps,
    the code will determine the noise level of the second map and cut the first
    map at the given sigma level. A typical application would be to plot the
    distribution of electric vectors or the degree of polarization only where
    the polarized flux is significant.
\seealso{read_difmap_fits,fit_gauss_to_img_noise}
\done

\function{plot_vlbi_pol_map}
\synopsis{creates a polarimetric image of a VLBI map using according .fits files (provided by DIFMAP)}
\usage{(Struct_Type xfig-map, Struct_Type xfig-colormap) = plot_vlbi_pol_map(String_Type \code{fitsfile flux-map-1},
                  String_Type \code{filename flux-map-2}, String_Type \code{filename plot-product})}
\altusage{(Struct_Type xfig-map, Struct_Type xfig-colormap, Struct_Type xfig-vectormap) =
                  plot_vlbi_pol_map(String_Type \code{fitsfile flux-map-1}, String_Type \code{fitsfile EVPA-map},
                  String_Type \code{filename flux-map-2}, String_Type \code{filename plot-product})}
\qualifiers{
\qualifier{n_sigma_pol}{[=5] start color scaling of polarized flux at \code{n_sigma*sigma}}
\qualifier{n_sigma_vec}{[=5] start plotting of EVPA vectors at \code{n_sigma*sigma} in reference to the polarized flux}
\qualifier{colmap_pol}{if set, plot most significant polarized flux (depending on n_sigma_pol),
                                      recommended: color map "iceandfire"}
\qualifier{colmap_back}{if set, plot polarized flux or total intensity flux, recommended: color map "ds9b"}
\qualifier{pol_cont}{set to 1 in order to plot contours of flux-map-1}
\qualifier{pol_cont_col}{[="white"] color of contours of flux-map-1}
\qualifier{pol_cont_depth}{[=0] depth of contours of flux-map-1}
\qualifier{pol_translate}{[=0.4] if plot_pol_colmap=0, the contours of polarized flux will be translated
                                      by \code{pol_translate*range_in_declination}}
\qualifier{render_object}{[=1] set to 1 in order to render the plot object}
}
\description
   This function uses the existing maps flux-map-1 (polarized flux),
   (the EVPA_map for the electric vector position angle) and
   flux_map_2 (polarized flux or total intensity flux) to plot the polarized flux
   as color-coded map (and the EVPA as vectors) - cutted at the most significant contours
   depending on n_sigma_pol - on top of the total intensity contours or as contours
   vertically translated to them.
   One has the flexibility to plot the polarized flux distribution instead of the total
   intensity flux color-coded depending on the map given for flux-map-2.
   All qualifiers of \code{plot_vlbi_map} are forwarded to this function.
\seealso{plot_vlbi_map}
\done

\function{plot_with_err}
\synopsis{plots data points with their errorbars}
\usage{plot_with_err(x, [xErr,] y, yErr);
\altusage{plot_with_err(Struct_Type s);}
}
\qualifiers{
\qualifier{xerr}{change 3-argument-syntax to \code{plot_with_err(x, xErr, y);}}
\qualifier{xminmax}{changes the meaning of \code{xErr} -- and \code{x}, if \code{xErr} is not a list}
\qualifier{yminmax}{changes the meaning of \code{yErr} -- and \code{y}, if \code{yErr} is not a list}
\qualifier{minmax}{equivalent to both \code{x}- and \code{yminmax}}
\qualifier{i}{index-array of subset of data points to be plotted}
\qualifier{set_xrange=frac}{ (default: 0.05): set the \code{xrange} from the lowest to highest x-value
                  with additional padding (given as a fraction fo the x-range) on both sides}
\qualifier{set_yrange=frac}{ (default: 0.05): set the \code{xrange} from the lowest to highest y-value
                  with additional padding (given as a fraction of the y-range) on both sides}
\qualifier{set_ranges}{equivalent to both \code{set_xrange} and \code{set_yrange}}
\qualifier{overplot}{The data will be overplotted.}
\qualifier{connect_points}{data points are also connected}
\qualifier{histogram}{draw histogram lines, too}
\qualifier{error_color}{draw error bars in a different color}
}
\description
    In order to use asymetric errors for x and/or y,
    the correspondig \code{Err} argument has to be a list \code{{ Err1, Err2 }}.
    If one of the \code{minmax} qualifiers is used,
    the corresponding \code{Err} list contains directly minimum and maximum values.

    If one of the \code{minmax} qualifiers is used, but \code{Err} is not a list,
    the value and \code{Err} arguments actually mean minimum and maximum values.
    The actual value is infered to be the mean of minimum and maximum.
\examples
    % examples with symmetrical errorbars:\n
     plot_with_err(x,       y, yErr);\n
     plot_with_err(x, xErr, y, yErr);\n
     plot_with_err(x, xErr, y,     ; xerr);\n
    \n
     % examples with unspecified x and/or y value:\n
      plot_with_err(xMin, xMax, y   , yErr; xminmax);  % => x = (xMin+xMax)/2\n
      plot_with_err(x[, xErr],  yMin, yMax; yminmax);  % => y = (yMin+yMax)/2\n
      plot_with_err(xMin, xMax, yMin, yMax;  minmax);  % => [both inferences]\n
      s = struct { bin_lo=x-xErr, bin_hi=x+xErr, value=y, err=yErr };\n
      plot_with_err(s);
    \n
    % examples with asymmetrical errorbars:
     plot_with_err(x, {xErr1, xErr2}, y,  {yErr1, yErr2});\n
     plot_with_err(x, {xMin,  xMax }, y,  {yMin,  yMax}; minmax);\n
    \n
    % example combining the above features:\n
     plot_with_err(xMin, xMax, y, {yMin, yMax}; minmax);  % => x = (xMin+xMax)/2\n
\seealso{[o][h]plot_with_err, [o][h]plot}
\done

\function{plot_with_y2axis}
\synopsis{plot two functions with different y-axes}
\usage{plot_with_y2axis(x1, y1, [ x2,] y2);}
\qualifiers{
\qualifier{color}{set the color of the 2nd y-axis [default: red]}
\qualifier{yspace}{space above and below the last data point in units of the total y-range.}
}
\description
    This function plots two different parameters y1 and y2, which depend
    on the same paramter x. By default, the same x is taken for
    both parameters.\n
    \n
    The y2-axis and the ranges are set automatically.\n
    \n
    Additionally the function returns the rescaled y2 values, which
    now lie in the range of the y1 values.
\done

\function{plot_xyfit}
\synopsis{plot xy-data and its current xy-model}
#c%{{{
\usage{plot_xyfit(Integer_Type data_id);}
\description
    A simple plot function of the xy-data (given by \code{define_xydata})
    and -model (last evaluation of the xy-fit-function, see \code{xyfit_fun}).

    This functions basically calls:\n
        \code{plot_with_err( get_xydata(data_id) );}\n
        \code{oplot( get_xymodel(data_id) );}
\seealso{get_xydata, get_xymodel, define_xydata, xyfit_fun, plot_with_err}
\done

\function{plummer_interaction_kernel}
\synopsis{Evaluate equations of motion or potential energy from a number of Plummer spheres}
\usage{Double_Type eom[3,n] = plummer_interaction_kernel(Double_Types t, m[6,n], Struct_Type ps; qualifiers)}
\altusage{Double_Type energy[n] = plummer_interaction_kernel(Double_Types t, m[6,n], Struct_Type ps; qualifiers)}
\qualifiers{
\qualifier{coords}{[\code{="cyl"}] Use cylindrical ("cyl") or cartesian ("cart") coordinates.}
\qualifier{eomecd}{[\code{="eom"}] Return equations of motion ("eom") or potential energy ("energy").}
}
\description
    This function computes the equations of motion or potential energy of n test particles
    at time 't' caused by the interaction with a number of moving Plummer spheres. Depending
    on whether cylindrical coordinates (r,phi,z) and their canonical momenta (vr,Lz,vz) or
    cartesian coordinates (x,y,z,vx,vy,vz; see qualifier 'coords') are used, the second
    input parameter 'm' is a [6,n]-matrix with
    (qualifier("coords")=="cyl")                       or (qualifier("coords")=="cart")
       m[0,*] = r;                                        m[0,*] = x;
       m[1,*] = phi;                                      m[1,*] = y;
       m[2,*] = z;                                        m[2,*] = z;
       m[3,*] = vr;                                       m[3,*] = vx;
       m[4,*] = Lz;                                       m[4,*] = vy;
       m[5,*] = vz;                                       m[5,*] = vz;
    If the qualifier 'eomecd' is set to "eom", the function returns a [3,n]-matrix 'delta' with
       delta[0,*] = -d/dr Phi(r,phi,z);                   delta[0,*] = -d/dx Phi(x,y,z);
       delta[1,*] = -d/dphi Phi(r,phi,z);                 delta[1,*] = -d/dy Phi(x,y,z);
       delta[2,*] = -d/dz Phi(r,phi,z);                   delta[2,*] = -d/dz Phi(x,y,z);
    whereby Phi is the sum over all Plummer potentials.
    If the qualifier 'eomecd' is set to "energy", the function returns an array of length n
    storing the potential energy for each orbit:
       E(r,phi,z) = Double_Type[n] = Phi(r,phi,z)
    or
       E(x,y,z) = Double_Type[n] = Phi(x,y,z)

    For each Plummer sphere, the third input parameter 'ps', which is a structure, contains
    a field which is again a structure with fields 't', 'x', 'y', and 'z' (all are arrays of
    the same length) that list the time-dependent positions of the sphere. Additionally, the
    shape of the respective sphere (see notes below) is given by the two fields 'psa' and
    'psb' (both scalars).
\notes
    The potential of a Plummer sphere at distance r is
       Phi(r) = -psa*(psb+r^2)^(-1/2)
    The resulting acceleration in radial direction is
       d^2/dt^2 r = -d/dr Phi = -psa*(psb+r^2)^(-3/2)*r

    Cubic spline interpolation is used to determine the positions of the Plummer spheres
    at time 't' if the GSL-module is available. Otherwise, linear interpolation is applied.
    Extrapolation is not allowed. Always make sure that the tabulated times '.t[*]' are in
    monotonic increasing order if '.t[-1]' is positive or in monotonic decreasing order if
    '.t[-1]' is negative.
\example
    ps = struct{ o0, o1 };
    ps.o0 = struct{ t=[0,1,2], x=[-1,0,1], y=[0,1,2], z=[0,1,2], psa=1, psb=4 };
    ps.o1 = struct{ t=[0,1,2], x=[1,0,-1], y=[0,1,2], z=[0,1,2], psa=1, psb=4 };
    m = Double_Type[6,1];
    m[0,0] = 0; m[1,0] = 2; m[2,0] = 0; m[3,0] = 0; m[4,0] = 0; m[5,0] = 0;
    r = plummer_interaction_kernel(0,m,ps; coords="cart");
    r = plummer_interaction_kernel(0,m,ps; eomecd="energy");
\seealso{N_body_simulation_std_kernel, plummer_MW}
\done

\function{plummer_MW}
\synopsis{Alternative model potential for the function 'orbit_calculator'}
\usage{plummer_MW(Double_Types t, m[6,n]; qualifiers)}
\qualifiers{
\qualifier{plummer_spheres}{Structure whose fields are again structures with fields 't' [Myr],
      'x' [kpc], 'y' [kpc], 'z' [kpc], 'psa' [Msun/Mgal*constant] (see notes), and 'psb'
      [kpc^2] describing the orbits and shapes of the Plummer spheres.
      Always make sure that the tabulated times 't[*]' are in monotonic increasing order
      if 't[-1]' is positive or in monotonic decreasing order if 't[-1]' is negative.}
\qualifier{MW_potential}{[\code{="AS"}]: Function ("AS", "MN_NFW", or "MN_TF"), which evaluates the
      equations of motion that result from a model for the gravitational potential of
      the Milky Way.}
\qualifier{All qualifiers from the Milky Way model potential (see qualifier 'MW_potential').}{}
\qualifier{All qualifiers from the function 'plummer_interaction_kernel'.}{}
}
\description
    This function provides an alternative model for the gravitational potential of the
    Milky Way which can be used by the function 'orbit_calculator'. The gravitational
    forces of a standard Milky Way potential (see qualifier 'model') are combined with
    those arising from the interaction with a number of moving Plummer spheres (see
    function 'plummer_interaction_kernel' and qualifier 'plummer_spheres') to determine
    the acceleration of n independent test particles at time 't'.
\notes
    Because of the unit convention used for the potentials of the Milky Way, the field
    'psa' of the qualifier 'plummer_spheres', which is assumed to be the mass of the
    respective Plummer sphere in solar masses, has to be converted to Galactic mass units
    and then multiplied with a constant accounting for the remaining unit conversions
    (see example below). The units of 'psb' have to be kpc^2.
\example
    % Test particle affected by the Milky Way and satellite galaxies:
    t_end = -100; % integration time in Myr
    model = "AS"; % Milky Way mass model
    s = properties_satellite_galaxies();
    i = struct{ x, y, z, vx, vy, vz, psa, psb };
    SunGCDist = (@(__get_reference(model)))(; eomecd="sgcd"); % Sun-GC distance of chosen mass model
    temp = [SunGCDist,0,0,0,0,0]; reshape(temp, [6,1]);
    vlsr = (@(__get_reference(model)))(0, temp; eomecd="circ")[0]; % Local standard of rest velocity of chosen mass model
    (i.x, i.y, i.z, i.vx, i.vy, i.vz) = cel2gal(s.ah, s.am, s.as, s.dd, s.dm, s.ds, s.dist, s.vrad, s.pma_cos_d, s.pmd; SunGCDist=SunGCDist, vlsr=vlsr);
    kpcmyr_to_kms = 977.7736364875057; % = conversion factor from kpc/myr to km/s = 3.0856775975*10^16 / (10^6 * 3.15582 * 10^7);
    i.vx /= kpcmyr_to_kms;
    i.vy /= kpcmyr_to_kms;
    i.vz /= kpcmyr_to_kms;
    i.psa = s.Pl_mass/2.325131802556774e+07; % conversion from solar masses to Galactic mass units Mgal to have G=1
    % Mgal = 2.325131802556774e+07 = 1e8*3.0856775975*1e19/6.6742/1e-11/1.9884/1e30, see Irrgang et al., 2013, A&A, 549, A137
    const = 100./kpcmyr_to_kms^2; % factor 100 because potential is given in 100 km^2/s^2, see Irrgang et al. 2013
    i.psa *= const;
    i.psb = (s.Pl_radius)^2;
    ps = N_body_simulation(i, t_end; kernel="N_body_simulation_MW_kernel", psa=i.psa, psb=i.psb, model=model);
    % ps contains time-dependent coordinates of Plummer spheres
    % add information about shape of the Plummer spheres:
    j = 0;
    foreach field (get_struct_field_names(ps))
    {
      temp = struct{ psa=i.psa[j], psb=i.psb[j] };
      set_struct_field(ps, field, struct_combine(get_struct_field(ps, field), temp));
      j++;
    };
    % compute trajectories of test particles:
    s = orbit_calculator(4,38,12.8,-54,33,12,61,723,0.86,0.57,t_end; set, model="plummer_MW", MW_potential=model, plummer_spheres=ps);
    plot(s.tr.o0.x,s.tr.o0.y);
    % without satellite galaxies:
    s = orbit_calculator(4,38,12.8,-54,33,12,61,723,0.86,0.57,t_end; set, model=model);
    oplot(s.tr.o0.x,s.tr.o0.y);
\seealso{orbit_calculator, plummer_interaction_kernel, AS, MN_NFW, MN_TF}
\done

\function{png_read_curve}
\synopsis{reads a curve from a x-y plot in an image}
\usage{(X, Y) = png_read_curve(filename, xpix1, xval1, xpix2, xval2, ypix1, yval1, ypix2, yval2);}
\description
   \code{filename} is a png file containing a color-defined curve.
   Its \code{x} and \code{y} coordinates are calibrated by \code{pix}/\code{val} pairs
   of pixel coordinate and corresponding value assuming a linear scale.
   The pixel coordinates start with (0, 0) in the upper left corner.
   The return values are calibrated x-values and y-values
   averaged over all pixels of the curve in the corresponding column.
\qualifiers{
\qualifier{color}{[\code{=0x000000} (black)]: considered color of the curve}
\qualifier{wherenot}{consider any color except the one specified above}
}
\done

\function{point_distance2_from_line}
\synopsis{Calculate the squared distance of a point from a line}
\usage{d2=point_distance2_from_line(xp,yp,x1,y1,x2,y2)}
\description
 Calculate the  distance of point (xp,yp) from the line
 defined by the points P1=(x1,y1) and P2=(x2,y2), where
 P1!=P2. This condition is not tested for speed
 reasons and will result in a division by zero.
\done

\function{point_in_polygon}
\usage{ret=point_in_polygon(p0,V);}
\altusage{ret=point_in_polygon(x,y,Vx,Vy);}
\synopsis{determine whether a point is in a polygon}
\qualifiers{
\qualifier{evenodd}{use the crossing number method}
\qualifier{crossing}{use the crossing number method}
\qualifier{winding}{use the winding number method (the default)}
}
\description
 The function returns 1 if the point p0 is located inside the
 polygon defined by the vertices V, and 0 if it is located
 outside of the polygon.

 The polygon has to be closed, i.e. V.x[n]==V.x[0] and
 V.y[n]==V.y[0] where n is the number of polygon points.

 Either the winding number (default) or the even-odd-rule
 define what is meant by inside.

 The point is either defined by a struct{x,y} and the vertices
 by a struct{x[],y[]}, or the coordinates can be directly
 given in the respective arrays.

 See the URL below for more explanations.

 Based on code by Dan Sunday,
 http://geomalgorithms.com/a03-_inclusion.html

\seealso{crossing_number_polygon,point_in_polygon,simplify_polygon}
\done

\function{point_is_left_of_line}
\usage{ret=point_is_left_of_line(p0,p1,p2);}
\altusage{ret=point_is_left_of_line(p0x,p0y,p1x,p1y,p2x,p2y);}
\synopsis{determines whether point p2 is left of a line through p0 and p1}
\description
 This function tests if point p2 is to the left of an infinite line defined
 by points p0 and p1. The points are defined by structs p=struct{x,y},
 where x is the x-coordinate and y is the y-coordinate.

 The function returns
   +1 if P2 is left of the line
    0 if P2 is on the line
   -1 -1 if P2 is right of the line

 Based on code by Dan Sunday, http://geomalgorithms.com/a03-_inclusion.html

\seealso{crossing_number_polygon,winding_number_polygon,point_in_polygon,simplify_polygon}
\done

\function{polint}
\synopsis{Polynomial Interpolation and Extrapolation with error estimation}
\usage{ (Double_Type y, dy ) = polint (Double_Type[] xa, ya, Double_Type x);}
\description
    Returns an interpolated value \code{y} at the given point \code{x} and an
    error estimate \code{dy} for the given arrays \code{xa} and \code{ya}.
    If P(x) is the polynomial of degree n-1 such that P(xa_i) = ya_i, i=0,n-1,
    then the retruned value y=P(x).
    Adapted from algorithm in Numerical Recipes,
    by Press et al. (1992, 2nd edition), Section 3.1. 

\seealso{qromb}
\done

\function{polytropic_standard_model}
\synopsis{Compute the structure of a polytropic standard model}
\usage{Struct_Type s = polytropic_standard_model(Double_Type M, R, X, Y)}
\description
    Based on the stellar mass 'M' (in solar masses), stellar
    radius 'R' (in solar radii), hydrogen mass fraction 'X',
    and helium mass fraction 'Y', this function computes the
    structure of a polytropic standard model. The output
    structure 's' contains the fields "r" (radial coordinate
    in solar radii), "rho" (density in g per cm^3), "P"
    (pressure in dynes per cm^2), and "T" (temperature in K),
    i.e., the structure of the polytrope.
\notes
    Polytropes are gaseous spheres in hydrostatic equilibrium
    which obey a polytropic equation of state
      P = K rho^((n+1)/n)
    where 'P' is the pressure, 'rho' the density, 'K' the
    polytropic constant, and 'n' the polytropic index. The
    so-called polytropic standard model is a polytropic model
    of index n=3. Its structure has a surprisingly good
    resemblence to those of main sequence stars and is, thus,
    used as a starting point to study the internal structure
    of stars. For details, see, e.g., Chapter 2.4 in the book
    "Principles of Stellar Evolution and Nucleosynthesis" by
    D.D. Clayton.
\example
    s = polytropic_standard_model(1, 1, 0.73, 0.26);
\seealso{solve_Lane_Emden_equation, solve_Eddington_quartic_equation}
\done

\function{position_angle}
\usage{ sep=position_angle(ra1,dec1,ra2,dec2);}
\synopsis{calculates the position angle of object1 with respect to object2}
\qualifiers{
\qualifier{deg}{if set, the input coordinates and output are in degrees (default: radian)}
\qualifier{radian}{if set, the input coordinates and output are in radian (the default))}
}
\description
 This routine calculates the position angle on the sky of the point 
 with coordinates ra1/dec1 with respect to the object with coordinates
 ra2/dec2.

 Following the IAU conventions, the position angle is 0 if object1 is
 directly north of 2 and it is counted in the N-E-S-W-N direction, i.e.,
 it is pi/2 / 90deg if object 1 is East of object 2.

 Note that per IAU convention, "position angle" always refers to the
 relative position with respect to the equatorial system. This
 routine is agnostic to the type of coordinate system, i.e., the
 position angle can be computed in all coordinate systems, but the
 term "position angle" should be avoided for other coordinate systems.

 The position angle is not defined if object 2 is at one of the poles,
 it is 0 if object1 and object2 are at the same position.

 The position angle is returned in the units of the input coordinate system.

 This function is array safe for either ra1/dec1 or for ra2/dec2.

\seealso{hms2deg,dms2deg,angle2string,greatcircle_distance}
\done

\function{pos_modulo}
\synopsis{computes the modulo and ensures that it is positive}
\usage{m = pos_modulo(a, b);}
\description
    \code{a} and \code{b} can either be arrays or scalars of a numerical type.

    \code{m =  a mod b;  %} if m is positive, otherwise:\n
    \code{m = (a mod b) + b;}
\done

\function{powerlaw_noise}
\synopsis{simulates a light curve with a power law distributed power spectrum}
\usage{Double_Type rate[] = powerlaw_noise(Integer_Type n);}
\qualifiers{
\qualifier{beta}{[= 1.5]: power law index of the power spectrum}
\qualifier{mean}{[= 0]: mean rate of the simulated light curve}
\qualifier{sigma}{[= 1]: standard deviation of the simulated light curve}
}
\description
    \code{n} is the number of bins of the simulated lightcurve.
    It should be a power of two best performance of the fast Fourier transform.
\seealso{Timmer & Koenig (1995): "On generating power law noise", A&A 300, 707-710}
\done

\function{powerlaw_xyfit}
\synopsis{linear xy fit function to be used with xyfit_fun}
\usage{xyfit_fun ("powerlaw");}
\description
    This function is not meant to be called directly!
    
    Calling \code{xyfit_fun ("powerlaw");} sets up a powerlaw fit
    function for xy-data. It has the form \code{y = norm*x^{-index}}
\seealso{xyfit_fun, define_xydata, plot_xyfit, linear_regression}
\done

\function{precess}
\synopsis{precess equatorial coordinates between two equinoxes}
\usage{(alpha,delta)=precess(alpha,delta;qualifiers)}
\altusage{AstroVector3_Type ecl=precess(eqp;qualifiers)}
\qualifiers{
\qualifier{fromequinox}{starting equinox of the transformation.
                      Float: JD, string: epoch designation (e.g.,
                      "J2000.0" or "B1950.0";
                      default: J2000.0)}
\qualifier{toequinox}{end equinox of the transformation (same interpretation)
                    as fromequinox; required!}
\qualifier{deg}{interpret angular arguments in degrees (default is radians!)
                applies also to the return value.}
\qualifier{mjd}{interpret equinoxes as a MJD (default: JD)}
}
\description

 Precess a coordinate from one equinox to another using
 the IAU 2000A model.

 The default equinox is J2000.0, formally the equinoxes are in TDB.

\seealso{Vector_Type,JDofEpoch,dms2deg,hms2deg,precession_matrix,gcrs2j2000_matrix}
\done

\function{precession_matrix}
\synopsis{return the precession matrix for the transform from J2000.0 to a given JD}
\usage{Matrix33_Type mat=precession_matrix(JD)}

\description
 This function calculates the precession matrix for the calculation
 of the precession FROM J2000.0 TO JD using the IAU 2000.0A theory
 (Capitaine et al., 2003, A&A 412, 567, Sect. 7)

 To get the matrix for the conversion from JD to J2000.0, transpose the
 matrix returned by this function.

 In most cases users will want to use the precess-function to
 precess astronomical coordinates.

 Note: This is mainly an internal function, it therefore does NOT
 have the usual qualifiers such as mjd or any safety checks, however,
 for some applications it will be useful to have direct access to the
 precession matrix.

\seealso{precess,precession_matrix}
\done

\function{precession_matrix2}
\synopsis{return the precession matrix for the transform between two equinoxes}
\usage{Matrix33_Type mat=precession_matrix2(fromJD,toJD)}

\description
 This function calculates the precession matrix for the calculation
 of the precession from fromJD TO toJD using the IAU 2000.0A theory
 (Capitaine et al., 2003, A&A 412, 567, Sect. 7)

 In most cases users will want to use the precess-function to
 precess astronomical coordinates.

 Note: This is mainly an internal function, it therefore does NOT
 have the usual qualifiers such as mjd or any safety checks, however,
 for some applications it will be useful to have direct access to the
 precession matrix.

\seealso{precess,precession_matrix}
\done

\function{primefactors}
\synopsis{factorizes an integer number into primes}
\usage{Integer_Type factors[] = primefactors(Integer_Type x);}
\description
    The array of prime factors will be ordered:
    \code{factor[i] <= factor[j]  %} for \code{i<j}.
\done

\function{principal_components}
\synopsis{performs a principal components analysis}
\usage{Struct_Type PCA = principal_components(Struct_Type s);}
\description
    The normalized components (which are stored in \code{PCA.components.c}#i)
    are calculated from the fields of the structure \code{s}
    such that they have a mean of 0 and a variance of 1.
    From them, the covariance matrix \code{PCA.cov_matrix} is calculated,
    which is diagonalized (see \code{PCA.eigenvalues} and \code{PCA.eigenvectors}).
    The principal components are stored in \code{PCA.components.pc}#i
    in ascending order of their contribution to the total variance.
\qualifiers{
\qualifier{table}{[="tab"]: name of the structure \code{s}}
}
\seealso{cov_matrix}
\done

\function{printhplot}
\synopsis{print a plot of a histogram into the terminal}
\usage{printplot(Double_Type[] lo, hi, values[, Struct_Type printplot]);}
\qualifiers{
    \qualifier{sym}{the char used for the bar (default: 'o')}
    \qualifier{get}{return the structure instead of printing}
}
\description
    Prints a very simple ASCII-version of plotting the
    given histogram into the terminal. The x- and y-ranges
    are defined automatically by the input, but can be
    specified via the optional printplot-structure (see
    'new_printplot'). The only drawn ticmarks represent
    these ranges.

    All qualifiers are also passed to new_printplot
\example
    % plot gaussian distributed random numbers
    (lo,hi) = linear_grid(-3, 3, 40);
    printhplot(lo, hi, histogram(grand(10000), lo, hi); W=40);
\seealso{new_printplot, printplot}
\done

\function{printplot}
\synopsis{print a plot of xy-values into the terminal}
\usage{printplot(Double_Type[] x, y[, Struct_Type printplot]);}
\qualifiers{
    \qualifier{sym}{the char used for a data point (default: 'x')}
    \qualifier{get}{return the structure instead of printing}
}
\description
    Prints a very simpel ASCII-version of plotting the
    given xy-values into the terminal. The x- and y-ranges
    are defined automatically by the input, but can be
    specified via the optional printplot-structure (see
    'new_printplot'). The only drawn ticmarks represent
    these ranges.

    All qualifiers are also passed to new_printplot.
\example
    % plot a sin
    x = [0:2*PI:#100];
    printplot(x, sin(x));

    % overplot a cosin
    printplot(x, .5*cos(x), printplot(x, sin(x); get); sym = '*');
\seealso{new_printplot, printhplot}
\done

\function{printplot_out}
\synopsis{print a char-matrix into the terminal}
\usage{printplot_out(Char_Type[] matrix);}
\seealso{new_printplot}
\done

\function{print_array}
\synopsis{Prints the entries of an array in one line.}
\usage{print_array(Array_Type);}
\description
     Prints the entries of an array in one line instead of different lines.
     Helpful for displaying dependent values of different arrays like a table.
\example
     UVOT-filter names and their wavelengths.

     isis> filters=["UVW2","UVM2","UVW1","U","B","V"];
     isis> wave=[2078.546871,2257.186625,2659.009185,3475.482080,4359.034280,5429.548507];
     isis> print_array(filters); print_array(wave);
     UVW2    UVM2    UVW1    U       B       V
     2078.546871     2257.186625     2659.009185     3475.482080     4359.034280     5429.548507
\done

\function{print_statistics}
\synopsis{shows statistical information on an array of numbers}
\usage{print_statistics(Double_Type a[]);
\altusage{print_statistics(Struct_Type s);}
}
\description
    If the argument of \code{print_statistics} is an array \code{a},
    its minimum, average, standard deviation and maximum 
    are shown. If \code{a} contains irregular numbers (\code{nan} or \code{+/-inf}),
    the same quantities are also shown for the regular numbers only.

    If \code{print_statistics} is called with a structure argument,
    the above mentioned task is performed on all array fields.
\seealso{moment}
\done

\function{print_struct}
\synopsis{prints the fields of a structure as columns of a table}
\usage{print_struct([File_Type F,] Struct_Type s);}
\description
    If \code{s} is a structure of arrays or lists,
    these are displayed as columns of a table.
    The output is written to \code{stdout} unless another \code{F}
    is specified, which may either be a file pointer
    or a string containing the filename.
\qualifiers{
\qualifier{i}{array of rows which are to be shown at all}
\qualifier{mark}{array of rows which are to be marked}
\qualifier{fields}{array of fieldnames (columns) which are to be shown}
\qualifier{fmt}{format string(s) for the columns, see \code{help("sprintf");}}
\qualifier{sep}{string separator between the columns (default = \code{"   "})}
\qualifier{initial}{initial separator on a line (default = \code{""})}
\qualifier{final}{final separator on a line (default = \code{""})}
\qualifier{html}{set \code{sep}, \code{initial} and \code{final} such that an HTML table is produced}
\qualifier{tex}{set \code{sep}, \code{initial} and \code{final} such that a TeX table is produced}
\qualifier{nohead}{don't show head line with field names}
}
\examples
    \code{print_struct(   s);                       %} The fields of \code{s} are printed.\n
    \code{print_struct(F, s);                       %} The fields of \code{s} are printed to the file \code{F}.\n
    \code{print_struct( , s; fmt="%.2f");           %} The fields are floats which are printed with 2 decimals.\n
    \code{print_struct( , s; fmt="%.2f", sep=" ");  %} As before, but with smaller separation between the columns.
\seealso{writecol, sprintf}
\done

\function{probmap}
\synopsis{histogram-like function for 2d-data including uncertainties}
\usage{Struct_Type probmap([Double_Type x0, x1, y0, y1]);}
\qualifiers{
    \qualifier{xbins}{number of bins in x-direction (preferred over 'bins')}
    \qualifier{ybins}{number of bins in y-direction (preferred over 'bins')}
    \qualifier{bins}{number of bins in both directions (default: 201)}
}
\description
    Using this object, a histogram of multiple xy-datasets
    including uncertainties for both, x- and y-direction,
    can be created. The uncertainties are handled as 2d-
    gaussians with sigma_x and sigma_y equal to the given
    errors.

    Calling this function creates a structure with the
    following functions among other fields:
      add      - adds xy-data to the map and returns the dataset
                 number of the added data
                 Usage: probmap.add(Double_Type[] x, y[, dy]);
                     or probmap.add(Double_Type[] x, y[, dx, dy]);
      delete   - deletes data identified by its dataset number
                 Usage: probmap.delete(Integer_Type number);
      xrange   - sets the x-range
                 Usage: probmap.xrange(Double_Type xMin, xMax);
      yrange   - sets the y-range
                 Usage: probmap.yrange(Double_Type yMin, yMax);
      npoints  - count the number of xy-data added in each point
                 of the map. If set the density map is available
                 at .data.npoints, and it is passed as qualifier
                 'npoints' to the norm_fun (see next qualifier).
      norm_fun - reference to a function used for normalization.
                 The parameters are an array of all datasets,
                 given as a structure (x,y,dx,dy), and the
                 histogram map resulting by adding all data.
                 The function has to return either a single
                 normalization factor or a map of factors.
                 Default factor: 1./max(histogram)
      getmap   - calculates the histogram map from the added data
                 and returns it as 2d-array
                 Usage: probmap.getmap();
      getpoint - returns the value of the histogram map at a
                 specific point
                 Usage: probmap.getpoint(x,y);
      calcpath - calculates the path with the highest averaged
                 probability between to points using the
                 A*-algorithm returning a structure with the
                 best path (x,y)
\example
    % creata a new map and set x- and y-range
    variable pm = probmap(0.1, 100, 0, 20);
    % add two xy-datasets including uncertainties
    pm.add(x1, y1, dx1, dy1);
    pm.add(x2, y2, dx2, dy2);
    % plot the resulting histogram
    plot_image(pm.getmap());
\seealso{histogram2d, histogram_gaussian_probability, aStar}
\done

\function{properties_satellite_galaxies}
\synopsis{Retrieve the properties of several satellite galaxies of the Milky Way}
\usage{Struct_Type s = properties_satellite_galaxies()}
\description
    Retrieve the properties (current kinematics, spatial extent, mass) of several satellite
    galaxies of the Milky Way.

    The output structure contains for each object the celestial coordinates in right ascension
    [h, m, s] and declination [deg, arcmin, arcsec], distance [kpc], radial velocity [km/s],
    proper motion in right ascension times cosine of declination [mas/yr], proper motion in
    declination [mas/yr]), half-light radius [kpc], dynamcial mass inside half-light radius
    [solar masses], Plummer softening radius [kpc], and the Plummer mass [solar masses].
\notes
    Positions, distances, radial velocities, half-light radii, and dynamcial masses are from
    McConnachie 2012 when available. Proper motions (PMs) are available only for a subset of
    satellites and are set to zero otherwise.

    The parameters of the Plummer potentials (softening radius, mass) are determined under
    the assumption that the dynamcial mass inside the half-light radius is 0.85 times the
    total mass, which gives a softening radius of about one third of the half-light radius.

    Main reference: McConnachie 2012, AJ, 144, 4.
    Additional references or comments:
    - LMC, SMC:         PMs and Plummer parameters from Kallivayalil et al. 2013, ApJ, 764, 161.
                        Half-light radius and mass derived from Plummer parameters.
    - Canis Major:      PMs from Dinescu et al. 2005, AJ, 631, L49.
                        Dynamical mass assumed to be ten times stellar mass, half-light radius set to 1 kpc.
    - Sagittarius dSph: PMs from Pryor et al. 2010, AJ, 139, 839.
    - Fornax:           PMs from Piatek et al. 2007, AJ, 133, 818.
    - Sextans:          PMs from Walker et al. 2008, ApJ, 688, L75.
    - Sculptor:         PMs from Piatek et al. 2006, AJ, 131, 1445.
    - Draco:            PMs from Pryor et al. 2014, AJ, submitted [arXiv:1407.3509].
    - Ursa Minor:       PMs from Piatek et al. 2005, AJ, 130, 95.
    - Carina:           PMs from Piatek et al. 2003, AJ, 126, 2346.
    - Leo I:            PMs from Sohn et al. 2013, ApJ, 768, 139.
    - Leo II:           PMs from Lepine et al. 2011, ApJ, 741, 100.
    - Bootes III:       Dynamical mass assumed to be ten times stellar mass, half-light radius set to 10 pc.
\example
    s = properties_satellite_galaxies();
    print_struct(s);
\seealso{N_body_simulation_MW_kernel}
\done

\function{psdcorr_zhang}
\synopsis{Timing Tools: Poisson Noise and Deadtime Correction (Zhang)}
\usage{(freq, psd) = psdcorr_zhang(totrate, tseg, dimseg);}
\qualifiers{
\qualifier{nonparalyzable}{Set this qualifier if the deadtime is non-paralyzable}
}
\description
    The deadtime is assumed to be paralyzable by default.
    
    Inputs:
      totrate - total countrate of all instruments
      tseg    - realtime length of the lc segments used for psd calculation
      dimseg  - bincount of the lc segments used for psd calculation

    Outputs:
      freq    - fourier frequency array
      noipsd  - array of observational noise of the psd
\seealso{Zhang et al. (1995), ApJ, 449, 930}
\done

\function{psd_lc}
\synopsis{simulate a random light curve that follows a given PSD using the algorithm of Timmer and Koenig}
\usage{Double_Type rate[] = psd_lc(Integer_Type n, Double_Type dt, Ref_Type PSD);
\altusage{(rate1, rate2) = psd_lc(Integer_Type n, Double_Type dt, Ref_Type PSD; time_lag_spectrum=...);}
}
\qualifiers{
\qualifier{mean}{[= 100]: mean count rate of the simulated lightcurve}
\qualifier{sigma}{[= 20]: standard deviation of the simulated lightcurve}
\qualifier{poisson}{if Poisson noise should be applied on the lightcrurve}
\qualifier{nr_PCUs}{[= 1]: number of PCUs}
\qualifier{time_lag_spectrum}{spectrum of time lags (two lightcurves will be returned)}
\qualifier{mean_2}{[= 100]: mean count rate of the second lightcurve}
\qualifier{sigma_2}{[= 20]: standard deviation of the second lightcurve}
}
\description
    \code{n} is the number of bins of the simulated lightcurve.
    It should be a power of two for best performance of the FFT.
    \code{dt} is the time resolution.
    \code{PSD} is a reference to a function which takes one argument
    -- the frequency -- and calculates the corresponding PSD value.
    The number of PCUs is needed for the calculation of Poisson noise
    if a mean RXTE count rate is given in counts/PCU.

    If \code{time_lag_spectrum} is reference to a function of one argument
    -- the frequency -- that calculates the time lag spectrum,
    an additional second lightcurve is returned that has
    the corresponding time lag with respect to the first one.

    see Timmer & Koenig (1995): "On generating power law noise",
    A&A 300, 707-710
\done

\function{pulsar}
\synopsis{fit-function modelling a neutron star pulse period evolution}
\usage{fit_fun("pulsar");}
\description
    Deprecated, use 
      fit_fun("pulsartorque");
    from now on.
\done

\function{pulsarorbit}
\synopsis{fit-function modelling a neutron star pulse period evolution}
\usage{fit_fun("pulsarorbit");}
\description
    DEPRECATED, use 
      fit_fun("dopplerorbit*pulsartorque");
    from now on.
\done

\function{pulsarorbit_fluxerror_mc}
\synopsis{estimates additional uncertainties caused by the flux measurements}
\usage{Double_Type[] pulsarorbit_fluxerror_mc(Integer_Type dataset);}
\qualifiers{
    \qualifier{runs}{number of MC loops (default: 100)}
    \qualifier{save}{save the uncertainty estimation to the
              assigned FITS-filename}
    \qualifier{collect}{file-pattern used by 'glob' to read and
              merge all FITS-files from previous runs}
    \qualifier{modify}{add the estimated uncertainties to the
              dataset(s) assigned to this qualifier
              (see below for a detailed description)}
    \qualifier{chatty}{be chatty if > 0 (default: 0)}
}
\description
    The flux measurements used to calculate the evolution
    of the spin-period in the fit-function 'pulsarorbit'
    usually have uncertainties. These uncertainties are
    not taken into account during an ordinary fit. This
    function performs Monte Carlo simulations to estimate
    the uncertainties of the modelled period induced by
    the flux uncertainties.

    During each run the flux evolution associated to the
    dataset 'id' is varied using
      flux = flux + grand * flux_err
    such that synthetic flux evolutions are created. Then
    a fit is performed using only the given dataset 'id'.
    This results in many modelled period evolutions. Their
    standard deviation at each time is finally considered
    as an additional uncertainty in the period space.
    These uncertainties are returned by the function.

    If the 'modify'-qualifier is set, the dataset(s)
    assigned to that qualifier or, in case of NULL, the
    dataset 'id' is modified as follows:
      new_error = sqrt(sqr(data_error) + sqr(mc_error))
    where data_error is the current 'error'-field of the
    dataset and mc_error is the estimated additional
    uncertainty as calculated by this function. In case
    of multiple given datasets, the time range of 'id'
    should include all of these datasets to get a proper
    period evolution.
\seealso{pulsarorbit}
\done

\function{pulsartaylor}
\synopsis{fit-function modelling the pulse period evolution with a Taylor series}
\usage{fit_fun("pulsartaylor");}
 altusage{fit_fun("dopplerorbit*pulsartaylor");}
\description
    This fit-function computes the pulse period evolution of, e.g.,
    a neutron star based on a Taylor series. That is
      p(t) = p0 + Pdot*(t-t0) + .5*Pddot*(t-t0)^2 + ...
    Here, p0 is the pulse period at the reference time t0 and
    Pdot, Pddot,... are the higher order derivatives.

    The parameters of the fit-function are:
    p0    - spin-period at t0 (s)
    t0    - reference time t0 (MJD), has to be fixed
    pdot  - period derivative (s/s)
    p2dot - 2nd period derivative (s/s^2)
    etc.

    By default the Taylor series is computed up to the second
    order. If your data requires higher orders, you can change this
    limit using 'pulsartaylor_set_order'.

    NOTE: in case you add the Doppler shift of orbital motion to the
          model using the 'dopplerorbit' fit-function, make sure that
          this fit-function is calculated *first* in order to
          transform the times ('lo' parameter of the fit-function)
          into the barycenter of the binary! These corrected times
          are shared among these fit-functions via the ISISscripts
          caching extension. Then the reference time, t0, is
          interpreted as binary corrected!

    NOTE: a Taylor series describes the pulse period evolution
          phenomenologically. In case of mass-accretion from a donor
          star the evolution is driven by the mass accretion rate and
          a Taylor series does not model this correctly. It is likely
          that further fit-parameters, such as orbital parameters,
          get biased due to this imperfect modelling (see, e.g., PhD
          thesis of M. Bissinger)!
\example
    % define the measured period evolution as ISIS dataset
    id = define_counts(time, make_hi_grid(time), period, period_err);

    % set the fit-function
    fit_fun("pulsartaylor(1)");
\seealso{pulseperiod, taylor, pulsartaylor_set_order, fitfun_cache}
\done

\function{pulsartaylor_set_order}
\synopsis{changes the order of the Taylor series used in 'pulsartaylor'}
\usage{pulsartaylor_set_order(Integer_Type new_order);}
\description
    By default the Taylor series used in the 'pulsartaylor' fit-
    function is computed up to the second order. This function allows
    to change this order starting at zero up to the 7th order. The
    number of fit-parameters are adapted and named accordingly.

    NOTE: changing a fit-function and its parameters on the fly is
          not supported by ISIS. In order to still achieve this
          feature here, the fit-function is first deleted using
          'del_function' and defined again using 'add_slang_function'.
          Testing revealed, however, that this trick only works once!
\seealso{pulsartaylor}
\done

\function{pulsartorque}
\synopsis{fit-function modelling the accretion torque of a neutron star (simple way)}
\usage{fit_fun("pulsartorque");}
 altusage{fit_fun("dopplerorbit*pulsartorque");}
\description
    The spin-up of an accreting neutron star is connected with its
    luminosity via
      Pdot = - b * P^2 L^alpha
    as found by Ghosh & Lamb (1979). This fit-function solves this
    differential equation in order to calculate the spin-evolution of
    the neutron star.

    The parameters of the fit-function are:
    p     - spin-period at t0 (s)
    t0    - reference time t0 (MJD), has to be fixed
    a     - constant spin-up or -down (s/s)
    b     - torque strength (s/s @ Lnorm and Pnorm)
    alpha - exponent of b (try to freeze it)
    Lnorm - luminosity used for normalization, has to be fixed
    Pnorm - spin-period user for normaliziation, has to be fixed

    In order to express the torque strength, b, in the usual unit of
    a spin-change (s/s), the equation is modfied by two normalization
    constants:
      Pdot = - b * (P/Pnorm)^2 (L/Lnorm)^alpha
    where Pnorm is a reference spin-period and Lnorm a reference
    luminosity. For instance, Pnorm could be tied to parameter p,
    i.e., the period p at t0. This should be avoided, however, when
    multiple dataset are fitted with the same torque strength b, but
    different values for the parameter p. Here Pnorm has to be the
    same among all datasets! The same applies to Lnorm, which should
    be fixed to an (arbitrary) flux or rate, for instance
    corresponding to the flux of the Crab.
    NOTE: setting Pnorm = 0 (the default) uses Pnorm = p internally
          in order to ensure backward compatibility.

    The exponent, alpha, of the torque strength, b, depends on the
    accretion mechanism. After Ghosh & Lamb, alpha=6/7 for disk-, and
    alpha=1 for wind- accretion.

    In order to calculate the spin-evolution, the count rate over
    time, i.e., a light curve has to be assigned to the dataset using
    'set_dataset_metadata' (see example below). The period
    measurements have to be within the time range of the light
    curve. The structure assigned to the dataset must contain the
    following fields:
      time - time (MJD, ordered)
      rate - count rate at 'time'

    NOTE: in case you add the Doppler shift of orbital motion to the
          model using the 'dopplerorbit' fit-function, make sure that
          this fit-function is calculated *first* in order to
          transform the times ('lo' parameter of the fit-function
          /and/ light curve 'time' grid) into the barycenter of the
          binary! These corrected times are shared among these
          fit-functions via the ISISscripts caching extension. Then
          the reference time, t0, is interpreted as binary corrected!

    NOTE: the full Ghosh & Lamb accretion torque theory provides
          equations for the torque strength, b, as a function of the
          neutron star parameters. This is not implemented here for
          simplicity. The full theory is implemented in the
          'pulsarGL79' fit-function.

    NOTE: the model is calculated on the light curve time grid in
          order to ensure that the integration includes the reference
          time, t0. This model is interpolated onto the period time
          grid in the end.
\example
    % define the measured period evolution as ISIS dataset
    id = define_counts(time, make_hi_grid(time), period, period_err);

    % assign the light curve to this dataset
    set_dataset_metadata(id, struct {
      time = lc.time,
      rate = lc.rate
    });

    % examples for setting the fit-function
    fit_fun("pulsartorque(1)");
    fit_fun("dopplerorbit(1)*pulsartorque(1)"); % add orbital Doppler shift
\seealso{pulsarGL79, dopplerorbit, set_dataset_metadata, fitfun_cache}
\done

\function{pulsar_GTI}
\synopsis{Calculates GTIs for pulse phase resolved spectroscopy}
\usage{pulsar_GTI(Double_Type tstart, tstop, String_Type satellite, basefilename, Double_Type t0, p, phase_lo, phase_hi);}
\qualifiers{
\qualifier{pdot}{first derivative of the pulse period in s/s (default: 0) }
\qualifier{pddot}{second derivative of the pulse period in s/s^2 (default: 0) }
\qualifier{MJD}{If set, tstart, tstop and t0 are assumed to in MJD}
\qualifier{local}{create GTIs in satellite's local time system. Requires qualifiers 'nobarevt' and 'barevt' to be set.}
\qualifier{barevt}{event file in barycentered time frame}
\qualifier{nobarevt}{event file in local time frame}
\qualifier{}{all other qualifiers are passed to BinaryPos}
}
\description
   This function calculates the GTIs for pulse phase resolved spectroscopy. Input arguments are\n
   
      tstart - start time of the time interval to be covered (typically the start of the observation)
      tstart - end time of the time interval to be covered (typically the end of the observation) 
      satellite - the name of the satellite (needed for setting the reference MJD correctly)
      basefilename - the file created will be named 'basefilename.gti'
      t0 - the reference time (time of phase zero)
      p - the pulse period
      phase_lo - the lower phase boundary
      phase_hi - the upper phase boundary
   \n
   Upon qualifier request, tstart, tstop, and t0 can be given in MJD instead of seconds. The pulse
   period is always in seconds.\n
   \n
   If the pulse period and the reference time are corrected for the binary
   motion, the orbital parameters should be provided by qualifiers such that
   the GTIs can be transformed into the observers time system. The
   qualifiers are passed and equal to the \code{BinaryPos} function.
   \n
   If GTIs in the satellite's local reference time system are needed (e.g., for Suzaku-XIS),
   an event file in both local and barycenterd time can be passed via qualifiers and the GTIs
   are interpolated to the local time system. Header keywords are set accordingly.
\example
   pulsar_GTI (2.7082e8, 2.7097e8, "nustar", "phase_0.25-0.35", 2.708242e8, 443.07, 0.25, 0.35);
   \n
   creates a GTI file named 'phase_0.25-0.35.gti', ranging from 2.7082e8--2.7097e8 seconds
   in NuSTAR's mission specific time system for the pulsar 4U 1907+09 with pulse period 443.07 seconds
   for the pulse phase interval 0.25--0.35 where phase 0.0 is set to be at 2.708242e8 seconds.

\seealso{MJDref_satellite, BinaryPos, pulse_time}
\done

\function{pulse2pulse_flux_lc}
\synopsis{averages the given lightcurve over the pulsar's period}
\usage{Struct_Type = pulse2pulse_flux_lc(
      Struct_Type lightcurve,
      Double_Type or Struct_Type period,
      [, Struct_Type orbit]
    );}
\qualifiers{
    \qualifier{remap}{interpolate the resulting flux lightcurve
               on the input time grid (default: no)}
    \qualifier{interpol}{function reference to perform the time
               interpolation (default: &interpol_points)}
    \qualifier{dphitol}{minimum phase coverage (dphi) a pulse in
               the lc has to have at least (default: .95)}
    \qualifier{gaptol}{minimum time difference between bins which
               defines a gap (default: 2*min(diff(lc.time)))}
    \qualifier{t0}{time of pulse phase zero (can be provided
               using the ephemeris structure as well;
               default: first time bin of the lightcurve)}
    \qualifier{phase}{array of the pulse phases corresponding to
               the lightcurve time array (default: its
               calculated using 'pulseperiod2phase'). If
               the qualifier is set to a reference the
               calculated phases are assigned to the
               given variable.}
}
\description
    The underlying slope in a lightcurve of a pulsar
    might affect any timing analysis of its pulsations
    due to pulse to pulse variations of the luminosity.
    This functions averages the count rates over each
    pulse to get this luminosity dependance, which is
    afterwards interpolated to the binning of the input
    lightcurve.

    The input lightcurve must be of struct {
      Double_Type[] time, rate, error
    }
 
    The input pulse period may be either a single number
    implying a constant pulse period or a structure
    containing the pulse ephemeris as defined in
    'check_pulseperiod_orbit_struct'. In addition, the
    orbital parameters might be also given as a
    structure to take also the Doppler shift of the
    pulse period into account. This is necessary only
    in case of a non binary corrected lightcurve.

    The output lightcurve is a struct {
      Double_Type[] time (center of pulse), rate, error,
      dphi (phase coverage of each pulse, <1 for a gap)
    }

    By default, bad sampled pulses are ignored in the
    final lightcurve, which would lead to wrong mean
    count rates otherwise.

    Note, that all parameters must have the same time
    unit, if applicable.

    Note further, that the lightcurve's time grid has
    to be evenly spaced!
\seealso{pulseperiod2phase, interpol_points}
\done

\function{pulseperiod}
\synopsis{calculates the pulse period at the given time depending
    on the given pulse period evolution}
\usage{Double_Type[] pulse_period(Double_Type[] time, Struct_Type pulseperiod[, Struct_Type orbit]);}
\qualifiers{
    \qualifier{interpol}{reference to a function to interpolate the
               pulse period evolution on the requested
               time grid (default: &interpol_points)}
    \qualifier{sameunit}{set if the unit of 'time' is in the same
               unit as the pulse period or set to the
               conversion factor from days to 'time'}
}
\description
    Calculates the expected pulse period at the given time
    (in days), which may be a single value or an array.
    The structures containing the pulse period and the
    optional orbital parameters must follow the conditions
    described in 'check_pulseperiod_orbit_struct'.

    If the pulse period is given as evolution (time vs.
    period) the period at the requested time is calculated
    by interpolation. Otherwise the pulse period is
    calculated by a 'taylor' series using a given period
    and its derivatives at a reference time in. In any
    case the requested time has to be in MJD and the period
    (and its Nth derivative) in seconds (/seconds^N)

    Finally, if any orbital parameters are provided, the
    returned period gets modified by the binary motion.
\seealso{check_pulseperiod_orbit_struct, taylor, radial_velocity_binary}
\done

\function{pulseperiod2phase}
\synopsis{calculates the pulse phase from a given pulse period}
\usage{Double_Type[] pulseperiod2phase(
      Double_Type[] time,
      Struct_Type pulseperiod[, Struct_Type orbit]
    );}
\qualifiers{
    \qualifier{interpol}{interpolation method to re-map
               the period- onto the input time-
               grid (default: &interpol_points)}
    \qualifier{getphi}{variable reference to return the
               phase on the given period over time
               grid (method (a), see below).}
    \qualifier{sameunit}{set if the unit of 'time' is in the same
               unit as the pulse period or set to the
               conversion factor from days to 'time'}
}
\description
    The pulse period p(t) of a pulsar and its pulse
    phase phi(t) are connected by
      dphi(t) / dt = 1. / p(t)
    Thus, from a given pulse period the phase can be
    calculated by integration, which this function
    provides.

    The pulse period may be given as
      a) the pulse period over time
         struct { time, period }
      b) the taylor coefficients of the pulse
         ephemeris via
         struct { p0[, t0, pdot[, p2dot]] }
    as defined in 'check_pulseperiod_orbit_struct'.

    It is assumed that the unit of the pulse period
    is a factor of 86400 larger than the time unit of
    the light curve, i.e., light curve in days and
    pulse period in seconds. If both have the same
    unit (regardless whether its seconds or days)
    use the sameunit-qualifier.
    
    In case a), the phase is numerically integrated
    using the trapez method on the full provided period
    time grid. Finally, the calculated phase is re-mapped
    onto the input time grid by interpolation.

    In case b), the phase is calculated analytically by
    a taylor-series:
      phi(t) = (t-t0)/p0 - pdot/p0^2*(t-t0)^2 + ...

    Note, that the analytical calculation only supports
    a pulse ephemeris up the the second order (p2dot).
    If you need higher orders, you can first calculate
    the pulse period over time via a taylor-series and
    finally use method a).

    In case orbital elements are provided, the additional
    phase shift is calculated after Hilditch Eq 3.43:
      delta phi = z(t)/c (f0 zdot(t)/c - f(t-t0))
    with the projected position z(t) of the neutron star
    and its spin frequency evolution f(t) = 1 / p(t),
    where p(t) is the given pulse period evolution.
\seealso{pulse_period, taylor, BinaryPos, radial_velocity_binary}
\done

\function{pulseperiod_epfold}
\synopsis{UNDER DEVELOPMENT; performs an automatic pulse period search on the given light curve(s)}
\usage{Struct_Type pulseperiod_epfold(Struct_Type[] lc, Double_Type p0);}
\qualifiers{
    \qualifier{nbins}{profile bins (default: 32)}
    \qualifier{fracexp}{minimum fracexp all bins should have (default: 1.0)}
    \qualifier{dpscale}{period search range relative to formal resolution
               of epfold (default: 3)}
    \qualifier{dpmin}{minimum period search range (default: 0.01 s)}
    \qualifier{gapscale}{factor for maximum allowed gap length relative to
               formal resolution o f epfold (default: .5)}
    \qualifier{goodness}{threshold for the goodness of any signal
               (default: 3)}
    \qualifier{pbins}{mininum number of consecutive bins in epfold with
               'goodness' to define a signal (default: 3)}
    \qualifier{chatty}{chattiness (default: 1)}
    \qualifier{plotlc}{reference to function(Struct_Type[] lc)}
    \qualifier{plotepf}{reference to
               function(Struct_Type epf, Double_Type median, norm)}
}
\description
    UNDER DEVELOPMENT, USE WITH CAUTION! Send questions or bugs to
    matthias.kuehnel@sternwarte.uni-erlangen.de

    lc[] = struct { time, rate, error, fracexp }
    with time in MJD

    p0 = pulse period in seconds
    
    returns struct {
      time (mean in MJD)
      period (in s)
      error (in s)
      epfold (structure returned by epfold)
      lc (input light curves, but maybe splitted)
    }    
\seealso{epfold}
\done

\function{pulseperiod_magic}
\synopsis{UNDER HEAVY DEVELOPMENT; USAGE ON YOUR OWN RISK}
\usage{Struct_Type pulseperiod_magic(Struct_Type[] lc, Double_Type p0, dpmax);}
\qualifiers{
    \qualifier{ccflim}{cross-correlation threshold (default: .7)}
}
\description
    UNDER DEVELOPMENT, USE WITH CAUTION! Send questions or bugs to
    matthias.kuehnel@sternwarte.uni-erlangen.de
    see pulseperiod_epfold

    lc[] = struct { time, rate, error, fracexp }
    with time in MJD

    p0 = pulse period in seconds
    dpmax = maximum allowed period derivative
    
    returns struct {
      time (mean in MJD)
      period (in s)
      error (in s)
    }    
\seealso{pulseperiod_epfold, pulseperiod_phase_connect, bayesian_blocks}
\done

\function{pulseperiod_search}
\synopsis{Looks for periodic signal in a lightcurve.}
\usage{Struct_Type pulseperiod_search( lc [, p0 [, sigma ] ] )};
\description

 The input lightcurve should be a structure of the form
     lc = struct { time, rate, [ error ], [ fracexp ] }
 If error resp. fracexp are not given, they will be filled with
 sqrt(rate) resp. ones. All fields are arrays of doubles of the
 same length.
 p0 and sigma determine an initial guess for the period and an
 approximate error.
 The output is a structure of the form
     { ep, pp, lc } 
 Each field is a list, in which each element corresponds to one
 segment of the lightcurve (multiple elements if the lightcurve
 contains considerable gaps).
 ep[i] contains epoch folding information, pp[i] the folded pulse profile
 and lc[i] the lightcurve under consideration.
 The function works the following way: (the functions used are given)

     (1) If no p0 is given, a Fourier method is used to find an
         initial guess for the period. A splitting is performed first
         if necessary
            * split_and_epfold_lc (; split_only), foucalc
     (2) The input lightcurve is split if it contains gaps.
            * split_and_epfold_lc (; split_only)
     (3) A correction for any underlying variation in flux of the
         lightcurve is performed.
            * pulse2pulse_flux_lc
     (4) For each segment an epochfolding is performed.
            * split_and_epfold_lc
     (5) The peak in the statistics of epfold is found.
            * find_peak

 Qualifiers for the individual functions can be passed as structures
 with the names given below. See their helps for further information.

\qualifiers{
\qualifier{compact}{output contains only the essential information}
\qualifier{epfold_fourier_qu}{contains qualifiers for step (1)}
\qualifier{fourier_qu}{contains qualifiers for step (1)}
\qualifier{epfold_split_qu}{contains qualifiers for step (2)}
\qualifier{pulse2pulse_slope_qu}{contains qualifiers for step (3)}
\qualifier{epfold_qu}{contains qualifiers for step (4)}
\qualifier{find_peak_qu}{contains qualifiers for step (5)}
\qualifier{no_slope_correction}{step (3) is not done}
\qualifier{no_splitting}{step (2) is not done}
\qualifier{exact}{forces epfold to use exact}
\qualifier{not_exact}{forces epfold not to use exact}
\qualifier{fourier}{fourier is used, even if p0 is given -- can be used to pass sigma}
\qualifier{chatty}{boolean value (default: 1)}

}
\seealso{foucalc, split_and_epfold_lc, pulse2pulse_flux_lc, find_peak}
\done

\function{pulseperiod_transform}
\synopsis{transforms a period evolution given as taylor coefficients to a new t0}
\usage{Struct_Type pulse_transform(Double_Type new_t0, Struct_Type pulseperiod);}
\qualifiers{
    \qualifier{sameunit}{set if the unit of 'time' is in the same
               unit as the pulse period or set to the
               conversion factor from days to 'time'}
}
\description
    The structure containing the pulse period and its
    derivatives at a certain time t0 must fullfil the
    conditions given in 'check_pulseperiod_orbit_struct'.
    This description of the pulse period evolution is
    transformed to a new given t0. In case this time is
    given as an array an array of transformed structures
    is returned.

    Note, that the uncertainty of the transformed pulse
    ephemeris scales with (t - new_t0)^N * pNdot with
    the highest order N of the taylor series.
\seealso{check_pulseperiod_orbit_struct}
\done

\function{pulseprofile_compose}
\synopsis{composes a pulse profile from sine and cosine functions}
\usage{Struct_Type pulseprofile_compose(Struct_Type decomposition);}
\description
    Inverse of pulseprofile_decompose, see its help for details.
\seealso{pulseprofile_decompose, pfold}
\done

\function{pulseprofile_decompose}
\synopsis{decomposes a pulse profile into sine and cosine functions}
\usage{Struct_Type pulseprofile_decompose(
      Struct_Type profile[, Integer_Type[] a_orders, b_orders]
    );}
\qualifiers{
    \qualifier{amin/amax}{array of min/max values for the allowed range
                of coefficient a_n during the fit (same order
                as a_orders). Needs a_orders and b_orders to
                be specified (default: NULL)}
    \qualifier{bmin}{same as amin/amax for b_n}
    \qualifier{phi0rng}{range of allowed phase offets [min,max]
                (default: [-1,1])}
    \qualifier{amplrng}{range of allowed amplitudes [min,max]
                (default: [-_Inf,+_Inf])}
    \qualifier{initpars}{array of initial parameters in the form
                [phi0[, ampl[, a_n..., b_n...]]]
                The values for a_n and b_n only apply if
                a_orders and b_orders are specified
                (default: [0, mean(profile.value), 0..., 0...])}
    \qualifier{maxord}{highest order to use (if no specific orders for
            a or b are given; default: nbins/2-1)}
}
\description
    The given pulse profile, F, of the form
       struct { Double_Type[] bin_lo, bin_hi, value }
    is fitted by a series of sine and cosine functions,
    
    F(phi) = ampl + sum_n^N a_n*sin(2PI*(phi+phi_0)*n)
                            + b_n*cos(2PI*(phi+phi_0)*n)

    where phi is the phase bin, ampl is the mean flux, N is
    the highest order to use, phi_0 is a phase offset, and
    a_n and a_b are the coefficients of the sine and cosine,
    respectively.
    By default the series is calculated up to the highest
    possible order, which is related to the number of phase
    bins (Nyquist frequency). It is also possible to specify
    the orders, which has to be taken into account during the
    fitting, for the sine and cosine function (a_orders and
    b_orders, respectively).
    The returned structure contains the resulting fit
    parameters,
    
    struct {
      Integer_Type nbins, % number of phase bins
      Double_Type phi0,   % phase offset
      Double_Type ampl,   % mean amplitude (zero order)
      Double_Type[] a,    % sine coefficients
      Double_Type[] b     % cosine coefficients
    }

    where the indices of the arrays a and b specify the order,
    n, of the coefficient (starting with n=1). This structure
    can be passed to 'pulseprofile_compose' to calculate the
    modelled pulse profile from the coefficients.
\example
    % synthetic example profile
    variable nbins = 32;
    variable prof = struct { bin_lo, bin_hi, value };
    (prof.bin_lo, prof.bin_hi) = linear_grid(0, 1, nbins);
    prof.value  = 2 + 8*sin((prof.bin_lo)*2*PI)^2;
    prof.value += 4*sin((prof.bin_lo)*PI);
    hplot(prof);
    
    % decomposition using the first order sine and second
    % order cosine coefficients only
    variable decomp = pulseprofile_decompose(prof, [1], [2]);
    ohplot(pulseprofile_compose(decomp));
\seealso{pfold, pulseprofile_compose}
\done

\function{pulseprofile_energy_interpolate}
\synopsis{interpolates a pulsephase-energy-histogram to a finer grid}
\usage{Double_Type[egrid*e_interp,phigrid*p_interp] pulseprofile_energy_interpolate(
      Struct_Type map, Integer_Type p_interp, Integer_Type e_interp
    );}
\description
\example
\seealso{pulseprofile_energy_map}
\done

\function{pulseprofile_energy_lag}
\synopsis{calculates a lag (=shift) in a pulsephase-energy-histogram}
\usage{Double_Type[] pulseprofile_energy_lag(
      Struct_Type map, 'p' or 'e'[, Double_Type[] reference]
    );}
\description
\example
\seealso{pulseprofile_energy_map, CCF_1d}
\done

\function{pulseprofile_energy_map}
\synopsis{sorts the given events into a pulsephase-energy-histogram}
\usage{Double_Type[egrid,phigrid] pfold_event_energy_map(
      Double_Type[] events, energies, Double_Type pulse_period;
      gti = Struct_Type, egrid = Struct_Type, pgrid = Struct_Type
    );}
\description
\example
\seealso{pfold}
\done

\function{pulseprofile_energy_normalize}
\synopsis{normalize a given pulse profile, spectrum, or pulseprofile-energy-map}
\usage{Double_Type[] pulseprofile_energy_normalize(
       Double_Type[] profile_or_spectrum,
       String_Type method
     );}
\altusage{Double_Type[egrid,pgrid] pulseprofile_energy_normalize(
          Double_Type[egrid,pgrid] map,
          String_Type method,
          Char_Type dimension
          );}
\description
    A function to normalize either a 1D array representing a pulse profile or
    spectrum, or a 2D pulse profile energy map. In the latter case the dimension
    the normalization should apply to has to be specified, as the normalization
    is performed in 1D, i.e., either for each phase normalize the energies
    (dim = 'e') or for each energy normalize the pulse profile (dmi = 'p').
    
    Following normalization methods are available:

       "sdev": Substracts the mean value and devides by the standard deviation:
               isis> mom = moment( value );
               isis> normvalue = ( value - mom.ave ) / mom.sdev;

     "minmax": Substracts the min. value and deviedes by the min-max range.
               isis> normvalue = ( value - min(value) ) / (max(value)-min(value));
    
\example
\seealso{pulseprofile_energy_map}
\done

\function{pulseprofile_phase_connect}
\synopsis{UNDER DEVELOPMENT; returns the phase shift between two pulse profiles}
\usage{(Double Type phi, ccf) = pulseprofile_phase_connect(Struct_Type prof, ref);}
\qualifiers{
    \qualifier{shift}{automatically shifts the given profile in order to match
            the phase of the reference profile (caution: overwrites
            the input!)}
}
\description
    UNDER DEVELOPMENT, USE WITH CAUTION! Send questions or bugs to
    matthias.kuehnel@sternwarte.uni-erlangen.de

    Determines the phase shift of the pulse profile 'prof' with
    respect to the reference profile 'ref' by a cross-correlation.
    The cross-correlation is interpolated to enhance the precision
    beyond the pulse profile binning. Both pulse profiles need to
    have the same number of bins.

    The input structures have to have the same fields as returned
    by 'epfold'.
    
    The resulting phase shift 'phi' and the maximum value of the
    cross-correlation 'ccf' are returned.
\seealso{pfold, CCF_1d}
\done

\function{pulse_fraction}
\synopsis{Calculate pulse fraction given a pulse profile}
#c%{{{
\usage{Double_Type (pf, pf_err) = pulse_fraction(Struct_Type pp; method);
  \altusage{(pf, pf_err) = pulse_fraction(Struct_Type pp; method=value);}}

\description
  This function calculates the pulse fraction from the given
  pulse profile (output of pfold) and given method.

  Available methods are:
    ma  : Modulation amplitude (min max), if a value 'a' is given,
          calculate the amplitude between integral of pulse profile
          that falls below the fraction 'a' and that which is above.

    fft : Calculate the pulse fraction as sqrt(sumsq(A_k))/abs(A_0) where
          A_k are the FFT amplitudes (k=1..N) and A_0 is the constant
          factor. If value is given and of type Int_Type, take only the
          FFT factors up to this value into account. If it is of type
          Double_Type, take all the harmonics into account until the
          remaining factors have a power of this fraction or less.

    dft:  Same as fft.

    rms : Calculate the RMS of the pulse profile, normalized by its average.

    area: Calculate the area, that is, the integral of the pulse profile
          subtracted the minimum. Normalized by the total area. The value,
          if given, specifies the fraction that is considered the minimum.

    fit:  Similar to fft. Takes all harmonics from the fft (or up to value
          tolerance of a best fit, given in value). The resulting series
          is integrated. If value is of type Int_Type, the series is
          truncated at this harmonic.

  To compute the pulse fraction only the fields 'values' and 'error' are
  used. So if the PF is not obtained from \code{pfold} one can also run
  it as
    % RMS method example
    pf = pulse_fraction(struct{value=..., error=...}; rms);

  The input should have only positive values (i.e., not mean subtracted).

  A rule of thumb is, that if the method is given without a value, that
  the input is exact (i.e., values have no uncertainty). This is
  appropriate for modelled pulse profiles. For measured data, giving a
  value cuts down on the noise contributions and gives a more robust
  estimate.

  All methods also return an uncertainty estimate. Warning! Take the word
  'estimate' here very seriously!

\seealso{pfold}
\done

\function{pulse_time}
\synopsis{returns the pulse arrival time of the given pulse number with
    respect to the pulse ephemeris and orbit}
\usage{Double_Type pulse_time(Integer_Type[] number, Struct_Type ephemeris[, Struct_Type orbit]);
 or Double_Type pulse_time(Integer_Type[] number, Double_Type pulseperiod[, Struct_Type orbit]);}
\qualifiers{
    \qualifier{MJD}{the values of the pulse ephemeris are given
           in days (default: seconds)}
    \qualifier{eph}{may be set to the pulse ephemeris structure}
    \qualifier{orb}{may be set to the orbital structure}
    \qualifier{dphi}{an additive constant phase shift}
}
\description
    Calculates the expected pulse arrival time of the
    given pulse number, which may be a single value
    or an array. The structures containing the pulse
    ephemeris and the orbital parameters must follow
    the conditions decribed in 'check_pulseperiod_orbit_struct'.
    Instead of the pulse ephemeris the pulse period
    may be given only. These structures may be also
    passed to the function by qualifiers, hence the
    structure parameters can be omitted.
    The equation to calculate the arrival time is
    similar to Hilditch eq. 3.53, including the terms
    of the fourth order (p3dot). The numerical calcu-
    lation is optimized using the Horner schema. For
    details see 'arrtimes'.
\seealso{check_pulseperiod_orbit_struct, pulseperiod, arrtimes}
\done

\function{pushStructFieldArray}
\synopsis{appends a value to an array which is the field of structure}
\usage{pushStructFieldArray(s, field, value);}
\description
    \code{s.field = [s.field, value];  %} if \code{s.field} was not \code{NULL}
\done

\function{__push_array}
\synopsis{pushes the values of an array onto the stack}
\usage{(Any_Type v1, v2, ...) = __push_array(Any_Type a[]);}
\description
   NOTE that the Stack is limted to 2499 Elements, if the length
   of a exceeds this value a Stack_Overflow Error is thrown!
\done

\function{pvm_fit_pars}
\synopsis{computes confidence intervals with PVM}
\usage{Struct_Type results = pvm_fit_pars(String_Type SetupFile[, Integer_Type pars[]]);}
\description
    With SetupFile, one has to provide an ISIS-script which loads/rebins the spectra
    and loads/assigns the response. If the model requires additional modules,
    they have to be activated as well in \code{SetupFile}. It is, however, not necessary
    to define/load the model itself, as the currently defined model will be saved
    into a file <\code{dir}>/<\code{basefilename}>\code{_initial.par}.

    \code{pars} is an array of parameters, for which the confidence levels are to be fitted.
    If \code{pars} is not specified, all free parameters of the current model are used.
    The best fit which is eventually found is always saved in <\code{dir}>/<\code{basefilename}>\code{.par}.

    The verbosity of \code{pvm_fit_pars} is controlled by the intrinsic variable \code{Fit_Verbose}.

    The return value \code{results = struct { index, name, value, min, max, conf_min, conf_max, buf_below, buf_above, tex }}
    is a table with the following information for each parameter:\n
    \code{min} and \code{max} are the minimum/maximum values allowed.
    \code{conf_min} and \code{conf_max} are the confidence limits.
    \code{buf_below} (\code{buf_above}) is the fraction of the allowed range \code{[min:max]}
    which seperates the lower (upper) confidence limit from \code{min} (\code{max}).
    If one of these buffers is 0, your confidence interval has bounced.

    The same infomation is stored in the files <\code{dir}>/<\code{basefilename}>\code{_conf.txt}
    and <\code{dir}>/<\code{basefilename}>\code{_conf.fits}.
    In case of any error, the return value is \code{NULL}.
\qualifiers{
\qualifier{level}{confidence level to be computed.
             As for conf, 0 means 68%, 1 means 90% [default], and 2 means 99% confidence level.}
\qualifier{tolerance}{the tolerance for chi^2 improvements without interrupting the search
                 for the confidence intervals, see \code{help("conf");}. The default is \code{1e-3}.}
\qualifier{fitmethod}{fit-method to be used, see \code{help("set_fit_method");}
                 Default is the currently used fit-method returned by \code{get_fit_method()}.}
\qualifier{nph}{the number of processes per host, see \code{pvm_ms}.}
\qualifier{debug}{[=1] prints additional debug information from \code{pvm_ms}. (Default=0)}
\qualifier{dir}{[="."] specifies the directory in which the logfiles shall be stored.
                  It may be a relative path to the current working directory.}
\qualifier{basefilename}{[=startdate]}
\qualifier{verbose}{log every output -- from the master script or any slave --
               in the file <\code{dir}>/<\code{basefilename}>\code{_stdout.log},
               and keep the initial parameters.}
\qualifier{isisscript}{[="isis-script"] command to start ISIS for slaves.}
}
\seealso{fit_pars; conf, set_fit_method, cl_master/cl_slave [Houck/Noble], pvm_ms [S-Lang module]}
\done

\function{pvm_fit_pars_txt2fits}
\usage{pvm_fit_pars_txt2fits(String_Type txt_filename);}
\done

\function{qromb}
\synopsis{Integrate using Rombergs's rule to specified accuracy.}
\usage{Double_Type int = qromb (Ref_Type function, Double_Type min, Double_Type max);}
\description
    Integrate a function between the limits \code{min} and \code{max} to specified
    accuracy using the extended trapezoidal rule. Adapted from algorithm
    in Numerical Recipes, by Press et al. (1992, 2nd edition), Section 4.3.
    The precision and number of maximal iterations can be set via qualifiers.
    Any other keywords are passed directly to the user-supplied function.
    NOTE: Romberg is more efficient then Simpson. 
\qualifiers{
\qualifier{qromb_eps [=1e-6]}{:    Scalar specifying the fractional accuracy before
                            ending the iteration.}
\qualifier{qromb_max_iter [=16]}{: Integer specifying the total number iterations
                            at which \code{qsimp} will terminate even if the
                            specified accuracy has not yet been met.
                            The maximum number of function evaluations is
                            2^(qromb_max_iter-2)-1.}
}
\qualifier{k [=5]}{:    Integration is performed by Rombergs method
                       of order 2k, where, e.g., k=2 is Simpsons rule.
}
\example
    %  Compute the integral of sin(x) from 0 to PI/3.
    %  The value obtained should be cos(PI/3) = 0.5
    variable val = qromb( &sin, 0, PI/3);

\seealso{integrate_trapez,polint,qsimp}
\done

\function{qsimp}
\synopsis{Integrate using Simpson's rule to specified accuracy.}
\usage{Double_Type int = qsimp (Ref_Type function, Double_Type min, Double_Type max);}
\description
    Integrate a function between the limits \code{min} and \code{max}to specified
    accuracy using the extended trapezoidal rule. Adapted from algorithm
    in Numerical Recipes, by Press et al. (1992, 2nd edition), Section 4.2.
    The precision and number of maximal iterations can be set via qualifiers.
    Any other keywords are passed directly to the user-supplied function.
\qualifiers{
\qualifier{qsimp_eps [=1e-6]}{:    Scalar specifying the fractional accuracy before
                            ending the iteration.}
\qualifier{qsimp_max_iter [=16]}{: Integer specifying the total number iterations
                            at which \code{qsimp} will terminate even if the
                            specified accuracy has not yet been met.
                            The maximum number of function evaluations is
                            2^(qsimp_max_iter+5)-1.}
}
\example
    %  Compute the integral of sin(x) from 0 to PI/3.
    %  The value obtained should be cos(PI/3) = 0.5
    variable val = qsimp( &sin, 0, PI/3);
       
\seealso{integrate_trapez,qromb}
\done

\function{quantile}
\synopsis{gets an arbitrary quantile of an array}
\usage{Any_Type quantile(Double_Type p, Any_Type a[]);}
\description
    \code{quantile(0, a) = min(a)} and \code{quantile(1, a) = max(a)}.
    For values \code{0 < p < 1}, intermediate values of the \code{a} will be returned.
\seealso{median}
\done

\function{quasar_accr (fit-function)}
\synopsis{fits a composite quasar spectra in the optical/UV}
\description        
	This function fits a composite quasar spectrm to the data, taking into
       account the redshift z. see Vanden Berk et al. 2001
  

\examples
    % data definition:
    load_data("optical.pha");
    fit_fun("quasar_accr(1)+powerlaw(1)");
    

\done

\function{rad2RD}
\synopsis{Convert right ascension and declination from radians to (h, m, s) and (deg, arcmin, arcsec)}
\usage{rad2RD(Double_Types RA[], D[])}
\description
    Given right ascension and declination in radians, the function converts them to
    hours, minutes, seconds and degrees, minutes of arc, seconds of arc. In case of
    negative declinations, the minus sign is assigned only to the highest non-vanishing
    term, i.e., to degrees if degrees is nonzero, to minutes if minutes is nonzero but
    degrees is zero, ... . Negative right ascensions are not expected.
\example
    (ah, am, as, dd, dm, ds) = rad2RD(0,-PI/4.);
    (ah, am, as, dd, dm, ds) = rad2RD(0,-PI/181.);
    (ah, am, as, dd, dm, ds) = rad2RD(3/4.*PI,0);
    (ah, am, as, dd, dm, ds) = rad2RD([0,3/4.*PI],[-PI/4,0]);
    (ah, am, as, dd, dm, ds) = rad2RD(RD2rad(01, 45, 12.54, 87, 55, 45.34));
\seealso{RD2rad}
\done

\function{RAdec_from_AzEl}
\synopsis{computes equatorial (RA, dec) coordinates from a point's (azimut, elevation) at a time MJD}
\usage{(RA, dec) = RAdec_from_AzEl(az, el, MJD,  longw, lat);}
\description
    This function is deprecated. Please use horizon2equatorial instead.
\seealso{horizon2equatorial,equatorial2horizon}
\done

\function{RAdec_from_galLB}
\synopsis{convert galactic (l, b) coordinates to equatorial (RA, dec) coordinates}
\usage{(RA, dec) = RAdec_from_galLB(l, b);}
\qualifiers{
\qualifier{l_unit}{ [\code{="deg"}]: set the unit of the galactic longitude
                       \code{l_unit="rad"} \code{=>}  l in rad
                       \code{l_unit="hms"} \code{=>}  l in hours as a scalar
                       or an array of the form \code{[H, M]} or \code{[H, M, S]} }
\qualifier{b_unit}{ [\code{="deg"}]: set the unit of the galactic lattitude}
}
\description
    This function is deprecated. Please use galactic2equatorial instead.
\seealso{galLB_from_RAdec, AzEl_from_RAdec, RAdec_from_AzEl, angle_to_rad}
\done

\function{radial_velocity_binary}
\synopsis{computes the radial velocity of a binary system as a function of orbital phase or time}
\usage{Double_Type v = radial_velocity_binary(Double_Type x);}
\qualifiers{
\qualifier{v0}{systemic velocity}
\qualifier{K}{velocity semi amplitude, in km/s}
\qualifier{asini}{value of a * sin i, in kilometers}
\qualifier{P}{orbital period, in days}
\qualifier{T0}{epoch of periastron, x means time in days}
\qualifier{T90}{epoch of mean longitude 90 degr, x means time in days}
\qualifier{e}{eccentricity (0 <= e < 1)}
\qualifier{omega}{longitude of periastron in radian (unless degrees is set), default = 0}
\qualifier{degrees}{omega is measured in degrees instead of radian}
}
\description
    x has the meaning of orbital phase (0 <= x < 1) unless T0 or T90 is specified.
\seealso{Ch. 3 of R.W. Hilditch, An Introduction to Close Binary Stars, Cambridge Univ. Press, 2001}
\done

\function{radial_velocity_LSR}
\synopsis{computes the radial velocity component of the local standard of rest}
\usage{Double_Type radial_velocity_LSR(Doule_Type RA, dec, MJD)
\altusage{Double_Type radial_velocity_LSR(Doule_Type az, el, MJD, longw, lat)}
}
\description
    right ascension and declination, Modified Julian Date
    azimuth, elevation, geographic longitude (west), geographic lattitude
\done

\function{radio_mod2img}
\synopsis{creates an image using a model and a beam}
\usage{Struct_Type img = radio_mod2img( Struct_Type \code{mdl}, Double_Type \code{beam});}
\qualifiers{
\qualifier{src_name}{[=NULL] name of the source}
\qualifier{date_mjd}{[=_NaN] observation date (MJD)}
\qualifier{obs_date}{[=NULL] observation date (string). if only date_mjd or only obs_date
                         are set, the other one is calculated.}
\qualifier{nrpix_ra}{[=512] number of pixels for right ascension}
\qualifier{nrpix_dec}{number of pixels in declination (by default calculated from RA-DEC-range)}
\qualifier{ra}{[=[ra_min,ra_max]]  RA range in mas}
\qualifier{dec}{[=[dec_min,dec_max]] DEC range in mas, by default the RA-DEC-range is
                         calculated from the distribution of model components and the beam size}
\qualifier{delt}{[=[ra_delt,dec_delt]] resolution in mas/pixel, overwrites nrpix qualifiers}
\qualifier{sigma}{[=1e-3*max(img)] value for 1 sigma (used by plot_vlbi_map)}
}
\description
    This function uses a model to generate an image and convolves it with the beam.
    Here the "beam" is simply a Gaussian profile, which is given as an array:
    \code{beam = [smajor_axis,sminor_axis,position_angle];}
    The model has to have the fields:
        \code{flux}   flux of each model component [Jy]
        \code{ra}     relative RA of component (change to delta_x?)
        \code{dec}    relative RA of component (change to delta_y?)
        \code{smajor} component size (smajor axis) [mas] (0 for point source)
        \code{sminor} component size (sminor axis) [mas]
        \code{pang}   position angle of component's smajor axis
    Currently the last two fields (\code{sminor}, and \code{pang}) are
    ignored and only circular components (\code{smajor}) are used.
\example
    variable mdl = struct {
        flux   = [0.6 , 0.9,  0.2,  1.2,  3],
        ra     = [2   , 1.4,  0.7, 0.25,  0],
        dec    = [1   , 0.6, 0.35,  0.1,  0],
        smajor = [0.3 , 0.2,    0,    0,  0],
        sminor = [0.3 , 0.2,    0,    0,  0],
       pang   = [0    ,   0,    0,    0,  0] };
    variable beam = [0.2, 0.07, 0.4];
    variable img = radio_mod2img (mdl, beam);
    plot_vlbi_map (img, "test.pdf");
\seealso{plot_vlbi_map, read_difmap_fits}
\done

\function{radius_to_unit}
\synopsis{converts a radius from a given unit into another}
\usage{Double_Type = radius_to_unit(Double_Type radius, String_Type from_unit, to_unit); }
\qualifiers{
  \qualifier{asini}{projected semi major axis (lt-s)}
  \qualifier{i}{inclination (degrees)}
  \qualifier{mq}{mass ratio of star to companion}
}
\description
  Converts the 'radius' from a given unit 'from_unit'
  into another unit 'to_unit'. There might be additional
  information needed for the calculation, e.g. the
  projected semi major axis, which is passed via
  qualifiers.

  The following units are supported (with additional
  needed qualifiers):
    rsun - solar radii
    cm   - centimeters
    lts  - light seconds
    disp - binary displacement (asini, i, mq)
    fill - fill factor = radius/critical radius (asini, i, mq)

  Note:
  It might be possible, that less qualifiers may be passed,
  if the variables are canceled during the calculation. For
  example, converting from binary displacement to fill factor
  requires the mass ratio 'mq' only.

  Note:
  Let the projected semi major axis be the distance from CM
  to M1. Then, mq is defined as M1/M2.
  That means, to convert the radius of the companion, mq has
  to be inverted and asini has to be the distance from CM
  to M2, which is simply asini*mq[not inverted]
\seealso{Roche_potential, Roche_critical}
\done

\function{ratio_error_prop}
\synopsis{calculates a ratio and error propagation}
\usage{(rat, err) = ratio_error_prop(a, a_err, b, b_err);
\altusage{(rat, err) = ratio_error_prop(a, b; Poisson);}
}
\description
    \code{rat = a/b}\n
    \code{err = sqrt[ (1/b * a_err)^2 + (a/b^2 * b_err)^2 ]}
\qualifiers{
\qualifier{Poisson}{\code{a_err = sqrt(a);  b_err = sqrt(b);}}
}
\seealso{hardnessratio_error_prop}
\done

\function{RD2rad}
\synopsis{Convert right ascension (h, m, s) and declination (deg, arcmin, arcsec) to radians}
\usage{RD2rad(Double_Types ah[], am[], as[], dd[], dm[], ds[])}
\description
    Given right ascension in hours, minutes, seconds and declination in degrees,
    minutes of arc, seconds of arc, the function converts them to radians. Always
    provide six numbers, i.e., fill up with zeros where necessary. Right ascension
    is assumed to be positive. If the degrees argument of declination is negative
    the minus sign is automatically applied to the minutes and seconds parameters.
    If the degrees argument of declination is zero, the sign of the minutes argument
    (if nonzero) is applied to the seconds parameter.
\example
    (raInRad, declInRad) = RD2rad(01, 45, 12.54, 87, 55, 45.34);
    (raInRad, declInRad) = RD2rad(01, 45.209, 0, -87, 55, 45.34);
    (raInRad, declInRad) = RD2rad(01.753472, 0, 0, 0, -55, 45.34);
    (raInRad, declInRad) = RD2rad(01, 45, 12.54, 0, 0, 45.34);
    (raInRad, declInRad) = RD2rad([01, 01], [45,45.209], [12.54,0], [87,-87], [55,55], [45.34,45]);
    (raInRad, declInRad) = RD2rad(rad2RD(0,-PI/4.));
\seealso{hms2deg,dms2deg,rad2RD}
\done

\function{Read a FITS header}
\usage{Struct_Type fits_read_header(String_Type file or Fits_File_Type fp);}
\description
    This function reads the header of the fits file given by the
    `file' argument and returns it as a structure.  If `file' is
    a string, then the file will be opened, read-out, and closed
    automatically. Otherwise, `file' should represent an already
    opened FITS file (which will remain opened).
\qualifiers{
   \qualifier{lowercase}{return structure tags in lower case
                           (default: upper case)}
}
\seealso{fits_read_records, fits_open_file}
\done

\function{read_data_from_write_plot}
\synopsis{reads data that was saved with "write_plot" in a structure}
\usage{Structure str = read_data_from_write_plot(String_Type file)}
\qualifiers{
\qualifier{no_res}{: no residuals should be loaded}
\qualifier{no_mod}{: no model included in dat file, implies no_res}
\qualifier{y_fac[=0]}{: scale the y-axis by 10^{y_fac}}
}
\description
    This function reads data from a plot saved with "write_plot" and returns
    it in a structure which can be used for plotting.
   
    The structure tags are:
      lo:  low energy
      hi:  high energy
      val: data value
      err: uncertainty of the data value
      model: model value of fit
      res: residual
      res_min: lower error bar
      res_max: upper error bar

    IMPORTANT: The filename has to be given without the ".dat" ending!

    Note that if the data were saved with plot_unfold(...;...,power=3);
    the values are automatically converted to ergs/s/cm^2
\seealso{xfig_plot_unfold,xfig_plot_data,read_col,write_plot}
\done

\function{read_difmap_fits}
\synopsis{read a fits image provided by DIFMAP}
\usage{Struct_Type img_struct = read_difmap_fits(String_Type \code{fitsfile_name})}
\qualifiers{
\qualifier{fit_noise}{fit properties of the image noise}
}
\description
    This function reads a .fits file provided by DIFMAP and returns
    a structure containing the image and keywords.
    The returned structure can be used as input for the function
    \code{plot_vlbi_map}.
\seealso{plot_vlbi_map}
\done

\function{read_ep}
\synopsis{read in the epoch files}
\usage{read_ep([array of epoch fits files]);}
\qualifiers{
\qualifier{quadrant}{[=1] Quadrant which is defined as a positive distance ( RA, DEC > 0 == 1 || RA < 0, DEC > 0 == 2 || RA < 0, DEC < 0 == 3 || RA > 0, DEC < 0 == 4)
}
}
\description
 This functions returns a structure of an Epoch . The require input is an array of epoch fits files.
\done

\function{read_histo}
\synopsis{read text data into histogram structure}
\usage{Struct_Type hist = read_histo(String_Type filename);}
\qualifiers{
\qualifier{cols}{[=4] number of columns in the data file}
\qualifier{collist}{array of column index for bin_lo, bin_hi,
                    value, err in that order. Needs col=-1.}
\qualifier{bin_lo}{column index for bin_lo. Needs col=-1.}
\qualifier{bin_hi}{column index for bin_hi. Needs col=-1.}
\qualifier{value}{column index for value. Needs col=-1.}
\qualifier{err}{column index for err. Needs col=-1.}
}
\description
    Read a text file with column data directly into a histogram
    structure struct{bin_lo,bin_hi,value,err};. 
    The default assumption is that the text file contains the four
    columns bin_lo, bin_hi, value, and err in that order as the first
    four columns in the file (further columns being ignored). 
    Non-standard files can be processed via qualifiers. Missing
    columns are populate with assumptions, e.g., the grid reflecting
    the row numbers and the uncertainty assuming Poisson statistics.
    If all columns are present, but out of order / other column
    numbers, a list with column numbers can be supplied.
    
    Qualifier cols:
        cols=4:  bin_lo, bin_hi, value, err
        cols=1:  value. 
                 Then bin_lo=[0:length(value)-1], bin_hi=bin_lo+1,
                 err=sqrt(value). 
        cols=2:  bin_lo, value. 
                 Then bin_hi=make_hi_grid(lo), err=sqrt(value).
        cols=3:  bin_lo, bin_hi, value.
                 Then err = sqrt(value).
        cols=-1: Either use collist qualifier to supply an array of
                 column indices for all of bin_lo, bin_hi, value, err 
                 (in this order). Or, for a sub-selection, use the
                 bin_lo, bin_hi, value, and err to supply column
                 indices individually. If any of those four equals 0,
                 this field is ignored. Missing columns are populated
                 as described above. 
                 Presence of collist takes presedence over the others. 
\seealso{init_histo, add_hist, shift_hist, 
scale_hist, stretch_hist}
\done

\function{read_par}
\synopsis{Read function parameter from file}
\usage{Struct_Type[] params = read_par(filename[, funp]);}
\description
  This function process a parameter file according to the ISIS
  file convention (safed, e.g., with save_par) and returns the
  paramter settings as an array of structs. This array is
  compatible to the array retireved by get_params and can
  therefore be used with set_params.

  If an additional parameter is given, it is expected to be a
  pointer to a variable in which the function string gets stored.

\seealso{get_params, set_params, save_par}
\done

\function{read_sixte_lc}
\synopsis{Reads a LC created by the SIXTE makelc tool}
\description
       makelc writes the lightcurve file according to the first OGIP
       standard, i.e. only a COUNTS column is written and the timing
       information is stored in the TSTART, TSTOP, TIMEDEL, and
       TIMEPIXR keywords. SIXTE uses the convention TIMEPIXR=0, which
       means that the time field of the lightcurve returned by
       read_sixte_lc equals the beginning of the bin (time_lo).
\usage{read_sixte_lc(filename);}
\done

\function{read_spix}
\synopsis{reads the FITS file created with write_spix}
\usage{Stuct_Type \code{spix} = read_spix( String_Type \code{filename});}
\seealso{make_spix, write_spix, plot_spix}
\done

\function{rebinGroup}
\synopsis{rebins a \code{{bin_lo, bin_hi, value, err} struct}ure by a grouping factor}
\usage{Struct_Type s_ = rebinGroup(Struct_Type s, Integer_Type n);}
\seealso{rebin}
\done

\function{rebin_combined_optimal}
\usage{rebin_combined_optimal (Int_Type group_index);
\altusage{rebin_combined_optimal Int_Type[] group_index);}}
\synopsis{Rebin a dataset combination based on Kaastra & Bleeker 2016}
\qualifiers{
\qualifier{min_counts}{[=0]: Ensure a minimum number of counts per bin per dataset.}
}
\description
  Rebin a dataset combination using the numerical optimum defined by the
  response. From the combination a combined response is calculated respecting
  the weights.
\seealso{combine_datasets, rebin_combined}
\done

\function{rebin_dataset_optimal}
\usage{rebin_dataset_optimal (Int_Type index);}
\altusage{rebin_dataset_optimal (Int_Type[] index);}
\synopsis{Rebin datasets addressed by <index> based on Kaastra & Bleeker 2016}
\qualifiers{
\qualifier{min_counts}{[=0]: Ensure a minimum number of counts per bin (alters statistical properties).}
}
\description
  Rebin a dataset to a numerical optimum defined by the associated
  response matrix. The algorithm tries to balance the maximum
  possible information with the smallest possible grid to represent
  it.
\seealso{rebin_data}
\done

\function{rebin_fouquan}
\synopsis{rebins a Fourier quantity from timing analysis with foucalc}
\usage{Struct_Type rebin_fouquan(freq, fouquan, numseg)}
\description
    \code{freq} is an array of the original Fourier frequencies.
    \code{fouquan} is the array of original Fourier quantities.
    \code{numseg} is the number of seqments used to calculate \code{fouquant}.

    The output structure has the following fields, which are arrays:
    - \code{freq_lo} and \code{freq_hi} define the new frequency bin.
    - \code{freq} is the average original frequency in each bin.
    - \code{n} is the number of original frequencies, and
    - \code{n_tot} is the total number of values (including segmentation) contributing to each bin.
    - \code{value} is the average Fourier quantity.
    - \code{error = value/sqrt(n_tot)} is the 1 sigma error for the \code{n_tot} values.
    - \code{sigma} is the standard deviation of the original quanities in each bin
\qualifiers{
\qualifier{logfreq}{}
\qualifier{linfreq}{}
\qualifier{newfreq}{array of new frequencies}
\qualifier{nofreq}{no rebinning}
\qualifier{verbose}{}
}
\done

\function{rebin_human2isis}
\synopsis{create isis binning from str = rebin_human2isis}
\usage{Array_Type binning = rebin_human2isis(String_Type str);}
\description
   This routine converts a binning string created with
   rebin_isis2human back to the ISIS conform binning array
   (see e.g. get_data_info(idx).rebin). It can be used to
   restore the binning of data directly.
   
\seealso{fits_save_fit, get_data_info, rebin_isis2human}
\done

\function{rebin_isis2human}
\synopsis{converts isis binning in a human readable string}
\usage{String_Type str = rebin_isis2human(Array_Type binning);}
\description
   This routine converts the binning as used by ISIS (see e.g. 
   get_data_info(idx).rebin) to a string which is easy to read.
   
   The routine fits_save_fit() uses this string to save the binning
   of the data. It can be converted back by using rebin_human2isis().
\seealso{fits_save_fit, get_data_info, rebin_human2isis}
\done

\function{rebin_lc}
\synopsis{rebins a lightcurve structure to a new time resolution}
\usage{Struct_Type new_lc = rebin_lc(Struct_Type lc, Double_Type new_dt);}
\qualifiers{
\qualifier{time}{[= \code{"time"}] field name in the structure containing the time}
\qualifier{rate}{[= \code{"rate"}] field name(s) in the structure containing the rate(s)}
\qualifier{error}{[= \code{"error"}] field name(s) containig the corresponding error(s)}
\qualifier{float}{type cast rate(s) and error(s) to Float_Type}
\qualifier{verbose}{shows the assumed time resolution of the initial light curve}
}
\description
    The original light curve \code{lc} may contain discontinuities.
    \code{new_lc.rate[i]} contains the average \code{lc.rate}
    where \code{new_lc.time[i] <= lc.time < new_lc.time[i]+new_dt}.
    \code{error^2} is rebinned accordingly.

    The structure fields "\code{time, rate, error}" may have arbitrary names,
    but these must then be specified by the according qualifiers.
    \code{new_lc} will also have  these field names. In addition, a "\code{time_hi}"
    field is added, which contains the upper boundary of the time bins.
\example
    \code{lc1 = struct { time=[1:10], rate=[1:10]^2, , error=[1:10] };}\n
    \code{LC1 = rebin_lc(lc1, 2);}\n

    \code{lc2 = struct { t=[1:10], r1=[1:10]^2, r2=[1:10]^3, e1=[1:10], e2=[1:10]^1.5 };}\n
    \code{LC2 = rebin_lc(lc2, 2; time="t", rate=["r1", "r2"], error=["e1", "e2"]);}\n
\seealso{rebin}
\done

\function{rebin_mean}
\synopsis{rebins units like intensity, where you expect the binning
to take the mean of the value, properly}
\usage{intens_new = rebin_mean(r_nlo,r_nhi,r_lo,r_hi,intens);}
\done

\function{rebin_to_energy_grid}
\synopsis{rebin data to a given energy grid}
\usage{rebin_to_energy_grid(Dataset ID, bin_lo, bin_hi);}
\description
   This routine bins the data to a given energy grid. Bins outside
   the given energy grid are left unbinned.
\seealso{rebin_data,rebin_satellite, rebin_human2isis}
\done

\function{rebin_to_instrument}
\synopsis{rebin data from one instrument to another instrument}
\usage{rebin_to_instrument(Dataset id_reference, Dataset id_to_be_rebinned);}
\description
   This routine tries its best to rebin the given dataset with the ID
   "id_to_be_rebinned" to the dataset with id "id_reference".
\seealso{rebin_to_energy_grid,rebin_data}
\done

\function{rectangles_overlap}
\synopsis{Checks whether two rectangles overlap}
\usage{res=rectangles_overlap(src,clp)}
\description

 This function checks whether the two rectangles src and clp
 overlap (return value=1) or not (return value =0).

 The rectangles are defined by (xmin,ymin,xmax,ymax)
 either as a 4 element array in this order or as a
 struct with these tags. Both boxes must be defined
 using the same format.

\done

\function{rectangle_where}
\synopsis{finds the rectangle in a 2d array, where an expression is true}
\usage{(Integer_Type Y[], X[]) = rectangle_where(Integer_Type expr);}
\description
    The rectangle in the 2d array defined by the index-arrays
    \code{Y} and \code{X} contains all elements for which \code{expr!=0},
    i.e., one can crop the elements for which \code{expr==0}.
    (Note that \code{all(expr[Y,X])} is not necessarily true.)
\example
    img = img[rectangle_where(img>0)];
\done

\function{redden (fit-function)}
\synopsis{fits the reddening of optical/UV data}
\description
	This function uses fm_unred to fit the reddening of optical/UV data.
	See fm_unred for details.

\examples
    % data definition:
    load_data("optical.pha");
    fit_fun("redden(1)*powerlaw(1)");
    

\seealso{fm_unred}
\done

\function{reduce_struct}
\synopsis{remove one or more fields from a strucure}
\usage{Struct_Type reduced_struct = reduce_struct(Struct_Type s, String_Type fieldsnames[]);}
\qualifiers{
\qualifier{extract:}{If given, the returned struct does only contain the fields 'fieldnames'!}
}
\description
  Either removing the fields 'fieldnames' from the given structure 's' (if they even exist) or
  if the qualifier 'extract' is given removing all other fields but those given with 'fieldnames'!
\done

\function{reflection_fraction_relxill}
\synopsis{calculates the reflection fraction as defined in relxill, using the rel_lp_table of the relxill code}
\usage{Double_Type fR = 
        reflection_fraction_relxill(double a, double/array height, double rin, double rout);}
\qualifiers{
\qualifier{path}{[getenv("RELXILL_TABLE_PATH")]: path to the table}
\qualifier{table}{[rel_lp_table_v0.5b.fits: name of the table}
\qualifier{struct}{if set returns full structure returning fR, f_inf, and f_bh }
}
\seealso{kerr_rms,kerr_rplus}
\description
   Assumptions:
   - height, rin, and rout have to be given in R_g, a is the
     dimensionless spin parameter
   - height, rin, and rout can also be given in negative values,
     which is interpreted the same way as the relxill definition:
     negative heights are in units of the event horizon (kerr_rplus) and
     and radii in units of the ISCO (kerr_rms)
   - photons are not allowed to cross the disk plane
   - produces identical results to relxill
   
   This function 
   
   If relxill and the RELXILL_TABLE_PATH environment variable is set
   up correctly, this function will work out of the box. Otherwise
   the path needs to be set manually.
   
   Note: this function has been tested only for
   rel_lp_table_v0.5b.fits. It is not recommended to change this. Any 
   deviations are < 0.01.
   
   Questions: contact Thomas Dauser
   
   Reference: Dauser et al., 2016, A&A, 590, A76
\done

\function{refraction}
\synopsis{calculate the correction for astronomical refraction}
\usage{Double_Type refraction(z0;qualifiers)}
\qualifiers{
    \qualifier{lambda}{wavelength (in A, between 3000 and 17000 A)}
    \qualifier{mum}{wavelength is in microns}
    \qualifier{nm}{wavelength is in nanometers}
    \qualifier{temperature}{temperature at observer [K; default: 288.15K=15C]}
    \qualifier{lapse_rate}{temperature lapse rate [K/m; default: 0.0065K/m = 6.5K/km]}
    \qualifier{centigrade}{temperature is given in C}
    \qualifier{pressure}{ambient total pressure at observer (Pa, default: 1013.25kPa)}
    \qualifier{kPa}{ambient pressure is in kPa}
    \qualifier{hPa}{ambient pressure is in hPa (or mbar)}
    \qualifier{rel_humidity}{relative humidity at observer (between 0 and 1)}
    \qualifier{altitude}{altitude of observer above geoid (m; below 11000m)}
    \qualifier{latitude}{geographical latitude of the observer (rad; default: 0)}
    \qualifier{deg}{all angles are given in degrees, not radians}
    \qualifier{exact}{use numerical integration also for z0 below 80deg}
}
\description
 For a given zenith distance z and local observing conditions, this function calculates
 the refraction angle, that is the difference R=z-z0 where z is the unrefracted zenith
 distance that would be measured if the Earth did not have an atmosphere, and z0 is the
 observed zenith distance.

 The most common use of this function will be to calculate z0 for a given topocentric
 zenith distance. For all practical purposes, R is so small and changes slowly enough,
 such that users can call the function with z and determine z0 from the return value.
 Note that the default arguments are in radians, use the deg qualifier to switch to
 degrees.

 The refraction depends very slightly on the atmospheric properties, i.e., its temperature
 profile. These can be set with the respective qualifiers. 

 The default settings of the routine are that for zenith distances smaller than 80 degrees
 the approximations given by Saastamoinen (1972, Bull. Geodesique 105, 279 and 1972, Bull.
 Geodesique 106, 383) are used. 

 For larger zenith distances, or if the "exact"-qualifier is set, a numerical approach
 is chosen, following the approach discussed by Auer & Standish (2000, AJ 119, 2472
 [first submitted in 1979!]), Hohenkerk and Sinclair (1985, HM Nautical Almanac Office,
 NAO Technical Note 63), and Mangum and Wallace (2015, PASP 127, 74). The function uses
 the atmospheric model discussed by Hohenkerk and Sinclair, but uses the exact refraction
 formula for air. This exact approach yields good results for zenith distances up to a
 few degrees larger than 90 degrees (i.e., observation of the horizon from a mountain
 top) and is the one on which the refraction formulae of the Astronomical Almanac are based.

 For large zenith angles, there are slight differences at the arcsecond or less level
 between the results discussed here and the numbers listed by Hohenkerk or Auer. These
 are due to the different treatment of numerical instabilities and the use of numerical
 differentiation formulae. Given that for large zenith angles the simplified atmospheric
 model of Hohenkerk (constant temperature lapse rate in the troposphere, constant temperature
 in the stratosphere) results in larger systematic errors anyway (see Nauenberg, 2017,
 PASP 129, 44503), this should not be seen as an error of the function.

 As a caveat, the calculations using the Saastamoinen formulae assume that all pressure
 terms there are in Pa, rather than in mbar. This reproduces the results with respect
 to the numerical simulations and values tabulated elsewhere to <1". But it is not
 what Saastamoinen claims. I (J. Wilms) am puzzled...

 If the exact qualifier is set or for large z, all input into the function MUST be
 scalars - in this case this routine is NOT array safe.

 For the Saastamoinen formulae, the routine is array safe in all relevant parameters.

\done

\function{refractive_index_air}
\synopsis{calculate the refractive index of dry and moist air}
\usage{Double_Type refractive_index_air(lambda;qualifiers)}
\qualifiers{
    \qualifier{mum}{wavelength is in microns}
    \qualifier{nm}{wavelength is in nanometers}
    \qualifier{temperature}{temperature [K; default: 288.15K]}
    \qualifier{centigrade}{temperature is given in C}
    \qualifier{pressure}{ambient total pressure (Pa, default: 1013.25kPa)}
    \qualifier{kPa}{all pressure arguments are in kPa}
    \qualifier{hPa}{all pressure arguments are in hPa (or mbar)}
    \qualifier{CO2_ppm}{CO2 fraction in ppm (default: 450)}
    \qualifier{water_pressure}{partial water vapor pressure}
    \qualifier{rel_humidity}{relative humidity}
    \qualifier{silent}{do not emit warning messages}
}
\description
 This function calculates the phase refractive index of dry and moist air
 for light in the optical and IR following the standard paper by
 Ciddor (1996, Appl. Optics 35(9), 1566) and the discussion by Stone
 and Zimmerman (NIST Engineering Metrology Toolbox;
 http://emtoolbox.nist.gov/Wavelength/Documentation.asp) for a given
 wavelength (default A, but see the mum and nm qualifiers) and ambient
 conditions (temperature, pressure,  humidity or partial vapor pressure,
 and CO2 concentration). The function has been verified against the values 
 given by Ciddor and in the NIST Engineering Metrology Toolbox.

 The relative uncertainty of the approximations used here is claimed to be 
 around 2e-8. The range of validity is 3000 A<lambda<17000 A, temperatures
 between -40C and 100C (233-373K), and pressures between 10 and 140kPa.
 The CO2 fraction is allowed to vary between 0 and 2000ppm.

 This routine is array safe. If lambda is an array, the other qualifiers
 can be either arrays of the same length as lambda or single valued.

\done

\function{rename_struct_fields}
\usage{Struct_Type new_s = rename_struct_fields(Struct_Type s, String_Type fieldnames[]);}
\done

\function{replicate_table}
\synopsis{repeats columns of a table, possibly with a periodic shift}
\usage{Struct_Type replicate_table(Struct_Type t);}
\qualifiers{
\qualifier{P}{[= 1] period}
\qualifier{back}{[= 0] repeat periods backwards}
\qualifier{ahead}{[= 1] repeat periods forwards}
\qualifier{periodic}{[\code{= ["bin_lo", "bin_hi"]}] periodic structure fields}
}
\description
    The return value is a structure with the same fields as \code{t}.
    All array fields are repeated (back+1+ahead) times; periodic ones as\n
      \code{[ val-back*P, ..., val-P, val, val+P, ..., val+P*ahead ]} ,\n
    while other ones are just replicated:\n
      \code{[ val       , ..., val  , val, val  , ..., val         ]} .\n
\done

\function{require_atoms}
\usage{require_atoms();}
\description
    This function loads the atomic data from the
    Astrophysical Plasma Emission Database via\n
       \code{atoms(aped);}\n
    unless \code{_isis->Dbase} is already initialized.
\seealso{atoms}
\done

\function{rescale_range}
\synopsis{rescales a value}
\usage{Double_Type y = rescale_range(Double_Type x);}
\qualifiers{
\qualifier{in}{inputrange}
\qualifier{out}{outputrange}
}
\description
    possibilities for in/out and the corresponding scaling function:\n
    \code{"0:1"}, no scaling\n
    \code{"-inf:inf"}, arctan
\done

\function{reset_plot_defaults}
\synopsis{Changes some of the defaults on the isis_fancy_plots package}
\usage{reset_plot_defaults(;dcol=Integer_Type, ...);}
\qualifiers{
\qualifier{dcol}{Data color value}
\qualifier{dsym}{Data symbol value}
\qualifier{mcol}{Model color value}
\qualifier{sum_exp}{!=0, Sum the exposures when combining data}
\qualifier{use_con_flux}{!=0, the unfolded model includes response smearing}
\qualifier{gap}{==0, plot models across data gaps.}
}
\description

 reset_plot_defaults(; dcol=#, dsym=#, mcol=#, sum_exp=#,
                   use_con_flux=#, gap=#);

 Resets some of the plot defaults to a user's specifications. (See
 help messages for individual plotting functions, and pg_info, to
 understand these settings.)  Use with no arguments to see current
 values.
\seealso{pg_info}
\done

\function{residual_runs}
\synopsis{Perform runs test on model and data}
\usage{status = residual_runs([&stat]);}
\description
    While the Chi2 test tests for the absolute difference
    of model and data, the runs test (Wald-Wolfowitz test)
    accounts for the sign difference.

    The test is in particular usefull for checking if the
    model misses essential features of the data. As a simple
    example concider data coming from a linear relation
    described by a constant function. Although the chi2 test
    might give a reasonable result the improper model is
    easily spotted in the residuals.

    The results are presented for 3 different confidence
    regions. Each model passing the test is added to the
    entries of the result matrix.

    Note: The Confidence region is the total confidence. It
    can happen that the test fails for the two tailed test but
    succeeds for both one tailed tests!

    If a reference is given as argument the same information
    is stored as struct in that reference.
\seealso{runs_test, normal_cdf}
\done

\function{resolution_rebin}
\synopsis{Rebin spectra based on approximated detector resolution.}
\usage{Array_Type resolution_rebin(dataset_id, instrument; oversampling, min_counts);}
\qualifiers{
\qualifier{sampling}{Float_Type, oversampling factor of detector resolution/bin width, default: 3}
\qualifier{min_sn}{Float_Type, minimum signal-to-noise ratio, default: 5}
}
\description
    This function rebins a dataset to fulfill the following two criteria:\n
    1: Each bin contains enough counts so that - assuming Poissonian noise -
       Signal-to-Noise is larger than the given minimum.\n
    2: If criterion 1 is fulfilled, the bin size is chosen as a fraction of the 
       detector resolution, an oversampling of 3 is recommended (Kaastra 2016).\n

    Required arguments are the dataset ID and a string that specifies the used 
    instrument. Supported are:\n
      XMM EPICpn: "epn"\n
      Suzaku XIS: "xis"\n
      NuSTAR FPM: "fpm"\n

\examples
     Load and plot EPICpn data with a histogram oversampling detector resolution by 5:\n
     variable EPN_data = load_data("src_s.pha");\n
     resolution_rebin(EPN_data,"epn"; sampling = 5);\n
     plot_data(EPN_data; dsym=0);\n

\seealso{rebin_data, group}
\done

\function{rgb2color}
\synopsis{converts r, g, b values to 24 bit color values.}
\usage{Integer_Type rgb2color(Double/Int_Type r, g, b)}
\qualifiers{
  \qualifier{string}{returns value as hex encoded string.}
}
\description
  Convert given color values to one 24 bit value.
  If values are doubles they are interpreted as ranging
  from 0 to 1. If integers they are interpreted as
  8 bit (0-255) color values.
\seealso{color2rgb}
\done

\function{rgb2hex}
\synopsis{converts r, g, b values between 0 and 1 to a 24-bit integer}
\usage{Integer_Type rgb2hex(Double_Type r, g, b)}
\qualifiers{
\qualifier{str}{return value is a string with preceding "#".}
\qualifier{des}{input color will be desaturated to gray-scale values.
           If a value is given, the saturation will be scaled accordingly.}
}
\seealso{rgb2hsl}
\done

\function{rgb2hpluv}
\usage{Double_Type h, s, l = rgb2hpluv(Int_Type r, g, b);}
\synopsis{Calculate HSPuv triplet from RGB space}
\description
  Inverse function of \code{hpluv2rgb}.

\seealso{hpluv2rgb}
\done

\function{rgb2hsl}
\synopsis{converts (red, green, blue) values to (hue, saturation, lightness)}
\usage{(Double_Type h, s, l) = rgb2hsl(Integer_Type r, g, b);}
\description
    Converts 8 bit RGB values (0-255) to HSL values.

    The return values are always normalized to [0, 1].
    See \code{hsl2rgb} for a definition of hue, saturation, lightness.
\seealso{hsl2rgb}
\done

\function{rgb2hsluv}
\usage{Double_Type h, s, l = rgb2hsluv(Int_Type r, g, b);}
\synopsis{Calculate HSLuv triplet from RGB space}
\description
  Inverse function of \code{hsluv2rgb}.

\seealso{hsluv2rgb}
\done

\function{rgb2hsv}
\synopsis{Convert RGB color to HSV}
\usage{(Double_Type h,s,v) = rgb2hsv(Int_Type r,g,b);}
\description
   RGB colores encoded as 8 bit (0-255) values get
   converted to HSV (Hue, Saturation, Value) as doubles
   (0-1) are. Works for array values two (lengths must
   match).
\done

\function{ridge_line}
\synopsis{calculates the ridge line}
\usage{Struct_Type ridge_line = ridge_line (Array_Type \code{img})}
\qualifiers{
\qualifier{ref_pix}{[=[y,x]] starting pixel of the ridge line (index convention: img[y,x])
                 brightest pixel used by default}
\qualifier{dr}{[=0.8] step size in pixels}
\qualifier{steps}{number of steps (default is maximal length of dimensions)}
}
\description
    This function calculates the ridge line in a (jet) image.
    Starting from the brightest point (assumed to be the core),
    or a point specified by the \code{ref_pix} qualifier, the ridge
    line is calculated. The ridge line is given in different ways:
    The brightest points at each distance (in steps of \code{dr} pixels)
    from the reference pixel, are given by the fields \code{peak_x} and
    \code{peak_y} of the returned structure. The field \code{peak_flux}
    contains the corresponding pixel value.
    As an alternative to the peak ridge line, a flux-centered ridge
    line is provided. The fields \code{flux_cent_x} and \code{flux_cent_y}
    specify the points, at which the integrated flux (along the same
    distance from the refence pixel) to the left is equal to that on
    the right side.
\notes
    - elliptical beams will create artifacts (wiggles in the ridge line),
      in order to avoid this effect, obtain the ridge line from an
      image convolved with a circular beam
    - valid points can be obtained, e.g., by filtering the radii at which
      the peak flux exceeds the map's noise level (fit_gauss_to_img_noise)
    - currently no smoothing of the points is done (e.g., fit spline to
      ridge line)
    - there can be problems if the radius (used for ridge line calculation)
      lies completely within the jet/beam, \code{ref_pix} qualifier can be
      used to select another starting point (on the "jet axis")
\example
    variable mdl = struct {
                   flux   = [0.3, 0.5, 0.1,  0.2, 0.3,  0.9,    3],
                   ra     = [3.7, 2.8, 2.2,  1.4, 0.9,  0.5,    0],
                   dec    = [1.2, 0.8, 0.8,  0.7, 0.6,  0.4,    0],
                   smajor = [0.4, 0.2, 0.1, 0.03, 0.1, 0.01, 0.01],
                   sminor = [0.4, 0.2, 0.1, 0.03, 0.1, 0.01, 0.01],
                   pang   = [0,      0,   0,    0,   0,    0,    0] };
    variable beam = [0.3, 0.3, 0.7];
    variable img = radio_mod2img (mdl, beam);
    variable r = ridge_line (img.img;);
    struct_filter(r, where(r.peak_value>1e-4));
    plot_image (log(img.img+1e-4));
    color(4);  oplot(r.peak_x,r.peak_y);
    color(13); oplot(r.flux_cent_x,r.flux_cent_y);
\seealso{fit_gauss_to_img_noise, radio_mod2img, plot_vlbi_map}
\done

\function{rndbknpwrlc}
\synopsis{simulates a light curve with a broken power law distributed power spectrum}
\usage{(t, rate) = rndbknpwrlc(Integer_Type nt);}
\qualifiers{
\qualifier{beta1}{[=1.5]: first power law index of the power spectrum}
\qualifier{beta1}{[=1.0]: second power law index of the power spectrum}
\qualifier{perc}{[]}
\qualifier{mean}{[=0]: mean rate of the simulated light curve}
\qualifier{sigma}{[=1]: standard deviation of the simulated light curve}
\qualifier{dt}{[=1]: time resolution of the simulated light curve}
}
\description
    \code{nt} is the number of bins of the simulated lightcurve.
    It should be a power of two best performance of the fast Fourier transform.
\done

\function{rndpwrlc}
\synopsis{simulates a light curve with a power law distributed power spectrum}
\usage{(t, rate) = rndpwrlc(Integer_Type nt);}
\qualifiers{
\qualifier{beta}{[=1.5] power law index of the power spectrum}
\qualifier{mean}{[=0] mean rate of the simulated light curve}
\qualifier{sigma}{[=1] standard deviation of the simulated light curve}
\qualifier{dt}{[=1] time resolution of the simulated light curve}
}
\description
    \code{nt} is the number of bins of the simulated lightcurve.
    It should be a power of two best performance of the fast Fourier transform.
    See also Timmer & Koenig (1995): "On generating power law noise",
    A&A 300, 707-710
\done

\function{Roche_critical}
\synopsis{calculates the critical potential at L1 and the corresponding radius in z-direction}
\usage{Struct_Type crit = Roche_critical(Double_Type mq);}
\description
  The Roche potential describes the motion of a binary consis-
  ting of two objects, M1 and M2. The mass ratio mq = M1/M2
  must be given. From the position of the first Lagrange point
  'xL1', which is a saddle point in the Roche potential, the
  value of the critical potential 'critpot' at this point is
  calculated. The corresponding equipotential surface describes
  the inner most closed surface, which is also known as 'Roche
  lobe'. Since the Roche potential does not depend on the
  binary rotation, the radius 'rcrit' of the star in z-direction
  (perpendicular to the orbital plane) is often used as the
  star's radius.
  
  All three values are returned by the following structure:
    xL1     - distance of M1 to L1 in units of the binary
              displacement
    critpot - value of the critical potential at L1
    rcrit   - radius of M1 in z-direction in units of the
              binary displacement
\seealso{Roche_potential}
\done

\function{Roche_equipotential_volume}
\synopsis{computes the volume inside an Roche equipotential surface}
\usage{Double_Type Roche_equipotential_volume}
\qualifiers{
\qualifier{phi}{[= critical potential] the surface's Roche potential}
\qualifier{N}{[= 100] number of subdivisions in x and y for numerical integration}
}
\done

\function{Roche_lobe_dimension}
\synopsis{determines the size of the Roche lobe}
\usage{Struct_Type Roche_lobe_dimension(Double_Type q)}
\description
    The structure has the following fields:\n
    \code{potential}: the critical Roche potential of the Roche lobe\n
    {\code{x}/\code{y}/\code{z}}{\code{min}/\code{max}}: the coordinates of the envelopping box
\done

\function{Roche_lobe_surface}
\synopsis{calculates surface elements of a binary star}
\usage{Struct_Type = Roche_lobe_surface(Double_Type mq, Double_Type fill, Integer_Type ntheta);}
\qualifiers{
  \qualifier{noRoche}{neglect the Roche potential -> spherical star}
  \qualifier{noNorm}{do not return the normal vectors}
  \qualifier{noArea}{do not return the areas}
  \qualifier{poly}{return the polygons defining the elements}
  \qualifier{rotate}{rotate the star around its (z-)axis by the
            given degrees (default: 0)}
  \qualifier{eps}{numerical precision (default: 1e-8)}
  \qualifier{chatty}{be chatty}
}
\description
  This function calculates the surface of the primary star
  in a binary, which is deformed by the Roche potential.
  Since this potential can not be solved analytically the
  surface must be divided into elements. The number of
  elements into theta direction can be specified by the
  'ntheta' argument. The total number of surface elements
  taking into account the singularity at the poles and the
  symmetry in theta is here
    2*ntheta * (ntheta-2) + 2
  The Roche potential for a spherical orbit depends on the
  mass ratio mq = M_primary / M_secondary
  To determine the size of the star, the undeformed radius
  Rz in direction of the angular velocity vector (omega)
  can be used via the so-called fill factor:
    fill = Rz / R_crit
  where R_crit is the critical undeformed radius for a
  star, which fills its Roche lobe completely. In that
  case, the potential value at the surface is equal to
  that at the first Lagrange point, which might lead to
  mass transfer onto the companion (Roche lobe overflow).

  The returned structure contains the following fields:
    Vector_Type[] r - position vector to each element
                      starting at the stars center
    Vector_Type[] n - normal vector of each element
    Vector_Type[] A - area of each element
    Vector_Type[] p - if the 'poly' qualifier is given,
                      it contains the polygons defining
                      each surface element

  NOTE: The origin of the reference frame is in the center
        of the star and the first Lagrange point is on
        the x-axis, while the rotation axis of the star is
        along the z-axis!
        If you want to rotate the star around its rotation
        axis (z-axis), use the 'rotate'-qualifier!
\seealso{Roche_critical, Roche_potential, radius_to_unit, Vector_Type}
\done

\function{Roche_potential}
\usage{Double_Type Roche_potential(Double_Type x[, y[, z]]);}
\qualifiers{
\qualifier{q}{mass ratio}
\qualifier{sph}{coordinates are considered as spherical coordinates r, phi, theta}
}
\description
   The default values for y and z is 0.

   The Roche potential has the following form:
   \code{-1/sqrt(x^2 + y^2 + z^2 ) - q/sqrt( (x-1.)^2 + y^2 + z^2 ) - (1+q)/2.*( (x-q/(1.+q))^2 + y^2 )}
\done

\function{Roman}
\synopsis{translates n to upper-case string with roman numeral}
\usage{String_Type res = Roman(Integer_Type n)}
\qualifiers{
\qualifier{latex}{typeset minus sign ("$-$"R)}
\qualifier{toobig}{[=""] string that is returned of n is larger
                    than the largest known Roman numeral (3999) }
}
\description
 Converts an integer into a uppercase roman numeral.
 Even though not known in Roman times, negative numbers are allowed.

 Algorithm based on
 https://www.geeksforgeeks.org/converting-decimal-number-lying-between-1-to-3999-to-roman-numerals/

\seealso{roman}
\done

\function{roman}
\synopsis{replaces an integer with lowercase Roman numeral strings}
\usage{String_Type rom = roman(Integer_Type);}
\synopsis{translates n to lower-case string with roman numeral}
\usage{String_Type roman = romann(Integer_Type n)}
\qualifiers{
\qualifier{latex}{typeset minus sign ("$-$"R)}
\qualifier{toobig [=""]}{string that is returned of n is larger
                    than the largest known Roman numeral}
}
\seealso{Roman}
\done

\function{roman2int}
\usage{Integer_Type=roman2int(String_Type)}
\synopsis{translates a roman numeral into an integer}
\description
 This function converts roman numerals into integers.
 The function is case insensitive and array safe.
\seealso{roman,Roman}
\done

\function{round2}
\synopsis{Round to the nearest integral value or to given digit}
\usage{Double_Type[] = round2( Double_Type[] value);}
\altusage{Double_Type[] = round2( Double_Type[] value, Integer_Type digit );}
\description
   This function rounds its argument to the nearest integral value and
   returns it as a floating point result. If the argument is an array,
   an array of the corresponding values will be returned.
   If a 2nd argument is given it is used as digit the value is supposed
   to be rounded to.
\seealso{round, floor2, ceil2, nint}
\done

\function{round_conf}
\synopsis{converts confidence intervals after DIN 1333 and gives the rounded decimal place.}
\usage{Struct_Type round_conf(Double_Type conf_lo, conf_val, conf_hi);}
\altusage{Struct_Type round_conf(Double_Type val, sym_err);}
\description
    \code{conf_lo} is the lower confidence limit
    \code{conf_val} is the best fit
    \code{conf_hi} is the upper confidence limit
    The alternative usage with two arguemts allows to specify a
    value \code{val} with a symmetric uncertainty \code{sym_err}.

    The returned structure contains the fields
    - \code{err_lo} (the rounded lower error)
    - \code{value_val} (the rounded best fit)
    - \code{err_hi} (the rounded upper error)
    - \code{digit} (the rounded decimal place of the error)

\seealso{round_err, TeX_value_pm_error}
\done

\function{round_err}
\synopsis{rounds an error after DIN 1333 and gives the rounded decimal place}
\usage{Struct_Type round_err(Double_Type x);}
\qualifiers{
    \qualifier{digits}{decimal place where rounding will be applied (suspends DIN 1333)}
    \qualifier{sloppy}{Deactivates moving rounding decimal place if significant decimal place < 3 (suspends DIN 1333)}
}
\description
    \code{x} is the error which will be rounded

    The returned structure contains the fields
    - \code{value} (the rounded error)
    - \code{digit} (the rounded decimal place of the error)

 EXAMPLES
     round_err(0.1278) will return value=0.13 and digit=-2
     round_err(1278)   will return value=1300 and digit= 2
     since after DIN 1333, the decimal point may only be right after the rounded digit or left of it,
     the error should be in LaTeX assigned as $13.0\\times 10^{2}$
     (? comment: in order to have the correct number of significant digits shouldn't it be:
     $0.13\\times 10^{4}$, $1.3\\times 10^{3}$, or $13\\times 10^{2}$)
\seealso{round_conf}
\done

\function{runs_test}
\synopsis{Perform runs test on sequence}
\usage{Int_Type = runs_test(Num_Type[]);}
\qualifiers{
  \qualifier{confidence}{[=0.05]: Critical test probability}
  \qualifier{overmixing}{Test for overmixing}
  \qualifier{undermixing}{Test for undermixing}
}
\description
    This function performs the runs test (Wald-Wolfowitz test)
    on the given array. The sequence is assumend to represent
    a dichotom set with n1 where sequence > 0 and n2 the
    complementary.

    The test returns true if the sequence is sufficiently
    randomly divided into the two sets (determined by the
    confidence qualifier).

    Per default the sequence is tested against over- and
    undermixing but can be adjusted to only test for one with
    the appropriate qualifier.

\seealso{normal_cdf}
\done

\function{RXTE_ASM_countrate}
\synopsis{estimates the RXTE-ASM countrate from the current model}
\usage{(A, B, C) = RXTE_ASM_countrate();}
\description
     Zdziarski et al. (http://adsabs.harvard.edu/abs/2002ApJ...578..357Z)
     provide a "response matrix" for the RXTE-ASM.
     The inverse of this matrix is applied to the energy flux
     derived from the current fit-function and its parameters,
     in order to estimate the RXTE-ASM count rates \code{A}, \code{B} and \code{C} (cps).

     Note that these numbers may only give a rough estimate!
\seealso{energyflux}
\done

\function{RXTE_ASM_lightcurve}
\synopsis{retrieves ASM lightcurves for a given source}
\usage{Struct_Type RXTE_ASM_lightcurve(String_Type sourcename);}
\qualifiers{
\qualifier{MJDmin}{earliest MJD to be used}
\qualifier{MJDmax}{latest MJD to be used}
\qualifier{dt}{if specified, time resolution in MJD for rebinning}
\qualifier{no_filter_nan}{do not remove empty bins after rebinning (only with \code{dt})}
\qualifier{list}{lists available sources; \code{sourcename} may be omitted but can be a regular expression}
\qualifier{get_list}{as list, but the list of sources is returned as an array of strings}
\qualifier{save}{saves the light curve data in a local FITS file}
\qualifier{verbose}{}
}
\done

\function{RXTE_filter_file_info}
\synopsis{returns RXTE filter file information}
\usage{Struct_Type RXTE_filter_file_info(String_Type xflfiles[])}
\seealso{aitlib/rxte/readxfl.pro}
\done

\function{RXTE_nr_PCUs_from_filename}
\synopsis{counts how many PCUs were off from an *_xyoff_excl_* filename}
\usage{Integer_Type RXTE_nr_PCUs_from_filename(String_Type filename)}
\done

\function{RXTE_nr_PCUs_from_filterfile}
\synopsis{returns the number of PCUs switched on over time}
\usage{Integer_Type[] RXTE_nr_PCUs_from_filterfile(String_Type filterfile[, Double_Type[] time])}
\description
     The number of PCUs switched on during an observation
     may vary, which results in jumps in the lightcurve.
     The filter file (your_extraction/filter/*.xfl) provides
     time resolved information about the operating PCUs,
     which is read out and returned.

     ATTENTION:
     If no time array is given the number of PCUs is returned
     for the full length of the observation (no GTIs applied!).
     In the other case the given time array HAS TO be in the
     SATELLITE TIME SYSTEM and in SECONDS since RXTE started
     operating (see 'MJDref_satellite').
\seealso{MJDref_satellite, RXTE_nr_PCUs_from_filename}
\done

\function{RXTE_obscat_info}
\synopsis{reads an RXTE human-readable obs(ervation)cat(alogue)}
\usage{Struct_Type RXTE_obscat_info(Integer_Type obscat_days[]);}
\seealso{aitlib/rxte/readobscat.pro}
\done

\function{RXTE_PCA_info}
\synopsis{retrieves light curves for a given RXTE-PCA observation}
\usage{Struct_Type RXTE_PCA_info(String_Type dirs[])}
\qualifiers{
  \qualifier{binning}{how much datapoints are to be summed up into a single point}
  \qualifier{path}{path to the observation}
  \qualifier{dirs}{subdirectories to the individual observing blocks}

  \qualifier{noback}{set if no bkg subtraction is to be performed}
  \qualifier{earthvle}{set if EarthVLE background model is to be used}
  \qualifier{faint}{set if Faint background model is to be used}
  \qualifier{q6}{set if Q6 background model is to be used
                (default is to test for earthvle,faint,q6)}
  \qualifier{skyvle}{set if SkyVLE background model is to be used
             (default is 0 for noback,earthvle,faint,q6,skyvle)}
  \qualifier{exclusive}{set to search for data that was extracted
                with the exclusive keyword to pca_standard being set.}
  \qualifier{top}{set to read top-layer data}
  \qualifier{nopcu0}{set to search for data that was extracted
                ignoring PCU0}
  \qualifier{fivepcu}{plot count-rates wrt to whole PCA, i.e.,
                normalizing to five PCU; default is to plot the average
                countrate per PCU}
  \qualifier{bary}{Try to use barycenter time column in data. Must be
                   created with fxbary before into file with postfix _bary.}
  \qualifier{t}{time array of data in MJD.}
  \qualifier{c}{count array of data}
  \qualifier{err}{Estimated error by applying Poisson statistic.
            Binning/background subtraction will be acknowledged.}
\description
    The elements of \code{dirs} may contain globbing expressions.

    The returned structure has the following fields:\n
    \code{obstime = struct { start, stop}} with the times for each observation in \code{dirs}\n
    \code{lc = struct { time, rate, error }} with the PCA light curve\n
    \code{obscat = struct} with information from the observation catalogue (occultation, saa, good time)\n
    \code{gti}\n
    \code{xfl = struct} with information from the filter file
\seealso{aitlib/rxte/readxtedata.pro}
}
\done

\function{RXTE_PCA_modes}
\usage{RXTE_PCA_modes(String_Type obsids);}
\qualifiers{
\qualifier{compact}{only one row per ObsID, omit TSTART, TSTOP and duration}
\qualifier{get_struct}{return information on PCA modes in a structure, too}
\qualifier{quiet}{do not show information, implies \code{get_struct}}
}
\description
    Reads RXTE-PCA data modes from the PCA FITS-index (FIPC) file.
    The location of the RXTE data archive is determined from the
    \code{local_paths.RXTE_data} variable (defined within the isisscripts).
\done

\function{RXTE_plot_PCA_info}
\synopsis{plots an overview of an RXTE observation (lc, GTI, SAA, bkg)}
\usage{RXTE_plot_PCA_info(String_Type dirs[]);
\altusage{Struct_Type RXTE_plot_PCA_info(String_Type dirs[]; get_info)}
}
\qualifiers{
\qualifier{electron}{set to electron ratio threshold for data extraction}
\qualifier{noback}{set if no bkg subtraction is to be performed}
\qualifier{earthvle}{set if EarthVLE background model is to be used}
\qualifier{faint}{set if Faint background model is to be used}
\qualifier{q6}{set if Q6 background model is to be used (default is to test for earthvle,faint,q6)}
\qualifier{skyvle}{set if SkyVLE background model is to be used (default)}
\qualifier{exclusive}{set to search for data that was extracted
                 with the exclusive keyword to pca_standard being set.}
\qualifier{top}{set to read top-layer data}
\qualifier{nopcu0}{set to search for data that was extracted ignoring PCU0}
\qualifier{fivepcu}{plot count-rates wrt to whole PCA, i.e., normalizing to 5 PCUs.
               Default is to plot the average countrate per PCU.}
\qualifier{charsize_obsid}{}
\qualifier{get_info}{returns the info structure}
}
\description
    The elements of \code{dirs} may contain globbing expressions.
\seealso{aitlib/rxte/rxtescreen.pro}
\done

\function{r_in_from_disk}
\synopsis{Calculate inner disk radius from a continuum fitting disk model}
\usage{Struct_Type r_in_from_disk(norm, distance, inclination)}
\qualifiers{
    \qualifier{f}{Color-correction factor [default: 1.0]}
    \qualifier{norm_err}{Error on normalization [default: 0]}
    \qualifier{distance_err}{Error on distance in units of kpc [default: 0]}
    \qualifier{inclination_err}{Error on inclination in degree [default: 0]}
    \qualifier{mass}{Mass of the black hole in units of solar mass [default: 0]}
    \qualifier{mass_err}{Error on mass in units of solar mass [default: 0]}
}
\description
    This function can be used to calculate the values and errors of
    the inner radius (in units of km) from a continuum fitting model
    such as diskbb or ezdiskbb. See Mitsuda et al. (1984), Makishima
    et al. (1986), and, e.g., Zimmerman et al. (2005) and Kubota et
    al. (1998) on the color-correction. See also the HEASARC model
    description of diskbb and ezdiskbb.
    
    The distance is given in units of kpc and the inclination in
    degrees. If a mass is given (units of solar mass), the struct
    will also contain the radius in units of r_g as well as the size
    of the gravitational radius in km.
\usage{r_in_from_disk(norm, distance, inclination)}
\example
    r_in_from_disk(1000, 8, 30; f=1.7, mass=10);
\seealso{gravitational_radius}
\done

\function{SaturationVaporPressure}
\synopsis{calculate the saturation vapor pressure of water in air}
\usage{Double_Type[] SaturationWaterPressure(T;qualifiers)}
\qualifiers{
    \qualifier{centigrade}{temperature argument is in centigrade (default: K)}
    \qualifier{kPa}{Return saturation vapor pressure in kilopascals}
    \qualifier{hPa}{Return saturation vapor pressure in hectopascals (=mbar)}
    \qualifier{water}{Calculate saturation vapor pressure over water}
    \qualifier{ice}{Calculate saturation vapor pressure over ice}
    \qualifier{iapws}{Use the more precise IAPWS prescription (see below)}
}
\description
 This function calculates the saturation vapor pressure (svp) of water over
 ice and water for a given temperature (default: K, but see the
 centigrade qualifier), the routine returns the svp in Pa unless
 the kPa or hPa keywords are given. By default, for T>0C (273.15K)
 the svp over water is returned, for smaller temperatures that for
 ice. Use the "water" and "ice" qualifiers if this is not what you want.

 As a default, for the svp over water the equation of
 Davis (1992), Metrologia 29, 67-70 is used, while for ice the
 prescription given by Marti and Mauersberger (1993), Geophys. Res.
 Lett. 20, 363-366 (1993) is applied. For the default settings,
 the relative deviation between these equations and the IAPWS recommended
 procedure is very small. It does not exceed 0.03% in the 0-100C range,
 and is <1.5% in the -50-0C range and less than 2.5% between -100 and -50C,
 while the absolute difference in the range below 50C does not exceed
 1 Pa.

 If the iapws qualifier is given, the routine uses the recommendations
 of the International Association for the Properties of Water and Steam
 (IAPWS), which was originally given by Peter H. Huang, "New equations
 for water vapor pressure in the temperature range -100 deg C to 100 deg C
 for use with the 1997 NIST/ASME steam tables," in: Papers and abstracts
 from the third international symposium on humidity and moisture, Vol. 1,
 p. 69-76, National Physical Laboratory, Teddington, Middlesex, UK,
 April 1998. The algorithm used here is as described by 
 http://emtoolbox.nist.gov/Wavelength/Documentation.asp
 The uncertainty is 20kPa at 100 C, less than 2 kPa at 45 C and
 0.7 kPa at 20C. 

 Note that formally the calculation is only valid in the range from
 -100 to +100 centigrade. The function returns "NaN" for T>100C and
 extrapolates formalism smoothly to lower temperatures (the values
 are very small in this regime anyway).


 This function is array safe. 

\done

\function{save_atime}
\synopsis{saves a structure of arrival times into a FITS-file}
\usage{save_atime(String_Type filename, Struct_Type atime, String_Type extname[, String_Type[] comments]);}
\qualifiers{
    \qualifier{obj}{observed object, written into FITS header}
    \qualifier{sat}{used satellite, written into extension header}
    \qualifier{nfold}{number of arrival times, which were merged
              during the determination. Corresponds to the
              'indiv' qualifier for the 'atime_det' function.
              Written into extension header}
    \qualifier{hfits}{structure for additional FITS-header fields}
    \qualifier{hext}{structure for additional extension-header fields}
    \qualifier{newfile}{if the given filename exists a new file is
              created instead of updating the existing one}
    \qualifier{newext}{if the given extension already exists in the
              FITS-file a further one with the same name is
              added instead of updating the existing one}
}
\description
    A structure of arrival times, e.g. as returned by the
    'atime_det' function, is save into the given FITS-file
    creating or updating the extension of the geiven name.
    Most of the qualifiers allow to store addiotional
    informations into the header of the FITS-file or the
    extension. The optional fourth parameter can be used
    to write additional comments into the FITS-file.
\seealso{atime_det, atime_merge, load_atime}
\done

\function{save_atime_beta}
\synopsis{saves a structure of arrival times into a FITS-file}
\usage{save_atime(String_Type filename, Struct_Type atime, String_Type extname[, String_Type[] comments]);}
\qualifiers{
    \qualifier{obj}{observed object, written into FITS header}
    \qualifier{sat}{used satellite, written into extension header}
    \qualifier{nfold}{number of arrival times, which were merged
              during the determination. Corresponds to the
              'indiv' qualifier for the 'atime_det' function.
              Written into extension header}
    \qualifier{hfits}{structure for additional FITS-header fields}
    \qualifier{hext}{structure for additional extension-header fields}
    \qualifier{newfile}{if the given filename exists a new file is
              created instead of updating the existing one}
    \qualifier{newext}{if the given extension already exists in the
              FITS-file a further one with the same name is
              added instead of updating the existing one}
}
\description
    A structure of arrival times, e.g. as returned by the
    'atime_det' function, is save into the given FITS-file
    creating or updating the extension of the geiven name.
    Most of the qualifiers allow to store addiotional
    informations into the header of the FITS-file or the
    extension. The optional fourth parameter can be used
    to write additional comments into the FITS-file.
\seealso{atime_det, atime_merge, load_atime}
\done

\function{save_par_to_FITS_header_struct}
\synopsis{saves fit-function and paramters to a FITS header structure}
\usage{Struct_Type save_par_to_FITS_header_struct()}
\description
    The returned structure can be used as header keys
    that are written to a FITS file.
    This header can be read with \code{load_par_from_FITS_header}.
\seealso{load_par_from_FITS_header, load_par, save_par, fits_write_binary_table}
\done

\function{save_plot}
\synopsis{saves ISIS spectral data into a FITS file}
\usage{save_plot([filename[, ids]]);}
\qualifiers{
\qualifier{A}{Save the data in Angstrom and not in keV.}
}
\description
   This functions saves all data and model points as currently
   noticed and rebinned to the file 'filename'.fits. Each dataset is
   stored in an own extension. The current model is save in
   'filename'.par. By default the name 'save_plot' is chosen.
   
   Hereby the values are given in counts/bin and in 
   photons/s/cm^2/keV. To calculate the flux from the observed 
   data, the function get_convolved_model_flux() was used.
   
   Additionally, information on the instrument, the target, the grating,
   the filename of the model, the functions used for the model 
   and the frame time are stored in the header.
\done

\function{save_slang_variable}
\synopsis{allows to save S-Lang variables into a file}
\usage{save_slang_variable(file, &var1, &var2, ...);}
\qualifiers{
    \qualifier{edit}{open an editor to modify the variables;
             in that case all variables have to be passed
             as references and will be set to their new
             values after the editor is closed}
    \qualifier{delete}{delete the file after editing (requires the
             'edit' qualifier to be set); note that the
             given variables are still modified!}
}
\description
    The S-Lang code defining the given variables is saved
    into a file, specified by either the filename or an
    already opened file-pointer. In order to handle arrays
    with a large number (>1000) of items as well as
    complex structures, the S-Lang code uses temporary
    variable names. Their values are assigned step by step
    to avoid a stack overflow. The file can be evaluated
    later to push the saved variables onto the stack (see
    the example).

    This function allows to modify the given variables as
    well. In that case the 'edit'-qualifier has to be set
    and all passed variables have to be given as references.
    The file the variables are saved into is shown in the
    editor specified by the EDITOR environment variable
    or jed, if EDITOR is undefined. After saving the file
    and closing the editor, the file is evaluated, which
    should push the (modified) S-Lang objects onto the
    stack. These objects are finally assigned to the given
    variables.

    NOTE: the latter feature is based on the function
          'edit_var', which does the same except that the
          main purpose is to edit the variables using a
          temporary file. Here, the S-lang code is human
          readable as well, since no temporary variables
          are used to assign the values stepwise. In that
          case, however, stack overflow errors may occure.
          But who wants to edit such large variables...?
    
    The function supports the following data-types:
      Integer_Type, Double_Type, Complex_Type,
      Char_Type, String_Type, BString_Type,
      Null_Tpye, Void_Type (=Undefined_Type),
    as well as
      Array_Type, Assoc_Type, Struct_Type, List_Type
      Vector_Type, Ref_Type (as structure fields)
\example
    % define a structure 
    variable a = struct { example = "foo" };
    
    % save the structure into a file
    save_slang_variable("mystruct.sl", a);

    % restore the variable into a new one
    variable b;
    (b,) = evalfile("mystruct.sl");
    
    % edit the original variable
    % using a temporary filename
    save_slang_variable("/tmp/myedit", &a; edit, delete);

    % this operation can be performed using
    % 'edit_var' as well (but better readable)
    edit_var(&a);
\seealso{evalfile, edit_var}
\done

\function{save_statistics}
\synopsis{saves the fit-statistic in a textfile}
\usage{save_statistics(String_Type filename);}
\seealso{eval_counts}
\done

\function{save_xypar}
\synopsis{save current xy-fit-parameter in file}
#c%{{{
\usage{save_xypar(String_Type file);}
\description
    Save current xy-parameter and xy-function in file.
\seealso{load_xypar, get_xyfit_fun, list_xypar}
\done

\function{savitzky_golay_coefficients}
\synopsis{Calculate the Savitzky-Golay coefficients used for data smoothing}
\usage{Double_Type[] c = savitzky_golay_coefficients(positive Integer_Type nl, nr, p; qualifiers)}
\description
    Calculate the Savitzky-Golay coefficients used for data smoothing. The arguments
    `nl'/`nr' are hereby the data points to the left/right while `p' is the polynomial
    degree (all have to be positive integer numbers; otherwise their absolute value is
    rounded to the next nearest integer). The quanity `p' must not exceed `nl'+`nr'!
    The returned array of coefficients is ordered as [c_-nl, ..., c_0, ..., c_nr].
\notes
    Requires GSL module.
\qualifiers{
\qualifier{derivative [=0]}{Savitzky-Golay coefficients used for calculating the numerical
      derivative of the corresponding order (must not exceed `p', i.e., the order of
      the polynomial).}
}
\example
    % coefficients to smooth data:
    c = savitzky_golay_coefficients(15,15,4);
    % -> coefficients to compute first derivative:
    c = savitzky_golay_coefficients(15,15,4; derivative=1);
\seealso{savitzky_golay_smoothing}
\done

\function{savitzky_golay_smoothing}
\synopsis{Smooth noisy data by using a Savitzky-Golay filter}
\usage{Double_Type[] smoothed_data = savitzky_golay_smoothing(Double_Type[] data,
                                  positive Integer_Type nl, nr, p; qualifiers)}
\description
    The idea of Savitzky-Golay filtering is to approximate the given data within
    a moving window (ranging from `nl' data points to the left to `nr' data points
    to the right) by a polynomial of order `p'. The respective polynomials are -
    in principle - determined by least-squares fits to the window data points.
    Luckily, Savitzky & Golay found a way to replace the fitting and evaluating
    of the polynomial by taking just linear combinations of neighboring data points
    tremendously speeding up the smoothing process. However, their method, which
    is implemented here, is valid only for regularly spaced data points. For more
    details on the method, see ``Numerical Recipes'', Third Edition, Section 14.9.

    Note that the input parameters `nl', `nr', and `p' have to be positive integer
    numbers (otherwise their absolute value is automatically rounded to the next
    nearest integer). The quantity `p' must not exceed the minimum window semi-
    length min(`nl',`nr') and `nl'+`nr' has to be smaller than or equal to the
    number of total data points.

    Practical hint: Best results are obtained when the full width of the degree 4
    Savitzky-Golay filter is between 1 and 2 times the full width at half maximum
    of the desired features in the data.
\notes
    Requires GSL module.
\qualifiers{
\qualifier{derivative [=0]}{Apart from smoothing noisy data, the Savitzky-Golay method
      is also capable to compute numerical derivatives from the fitted polynomials.
      In order to calculate the `k'-th derivative of the data, set this qualifier
      equal to `k' and divide the returned array by the data stepsize to the power
      of `k'. Note that `k' has to be a positive integer (otherwise it is replaced
      by the next nearest integer of its absolute value). Note that `k' must not
      exceed `p'.}
\qualifier{periodic}{Set this qualifer to assume periodic boundary conditions for your
      data avoiding special treatment of data points close to the edges and thus
      speeding up the computation. In this case, the condition `p'<min(`nl',`nr')
      is replaced by `p'<`nl'+`nr'.}
}
\example
    x = [0:20:#1000];
    data = sin(x);
    data = data + 0.1*grand(length(x));
    smoothed_data = savitzky_golay_smoothing(data,100,100,4);
    first_derivative = savitzky_golay_smoothing(data,100,100,4; derivative=1)/(20./1000.);
    second_derivative = savitzky_golay_smoothing(data,100,100,4; derivative=2)/(20./1000.)^2;
\seealso{savitzky_golay_coefficients}
\done

\function{scale_hist}
\synopsis{scale a histogram}
\usage{Struct_Type hist = scale_hist(hist, scal);}
\description
    Scale a histogram structure with fields bin_lo, bin_hi, value, and
    err. The value field is scaled by the factor scal. The err field,
    if present is scaled accordingly. If other fields are present,
    those are preserved and passed on. Only presence of value and 
    err are checked for, but err is optional.
\seealso{add_hist, shift_hist, stretch_hist}
\done

\function{scargle}
\synopsis{Computes the lomb-scargle periodogram of an unevenly sampled lightcurve}
\usage{Struct_Type res = scargle (t, c); % where t and c contain time and counts of the lc}
\qualifiers{
\qualifier{fmin}{minimum frequency to be used (NOT ANGULAR FREQ!), has precede over pmin, pmax}
\qualifier{fmax}{maximum frequency to be used (NOT ANGULAR FREQ!), has precede over pmin, pmax}
\qualifier{pmin}{minimum period to be used}
\qualifier{pmax}{maximum period to be used}
\qualifier{omega}{array of angular frequencies for which the PSD values are desired;
                    if set, value for numf will be reset to length of omega during code}
\qualifier{noise}{for the normalization of the periodogram.
                    if not set, equal to the variance of the original lc.}
\qualifier{numf}{number of independent frequencies}
\qualifier{old}{if set computing the periodogram according to J.D. Scargle, 1982, ApJ 263, 835
                  if not set, computing the periodogram with the fast algorithm
                  of W.H. Press and G.B. Rybicki, 1989, ApJ 338, 277.}
\qualifier{nu}{if set, output structure also contains frequency}
\qualifier{om}{if set, output structure also contains angluar frequency}
}
\description
     (transcribed from IDL-program scargle.pro)

     The Lomb Scargle PSD is computed according to the definitions
     given by Scargle, 1982, ApJ 263, 835, and Horne and Baliunas,
     1986, ApJ 302, 757. Beware of patterns and clustered data
     points as the Horne results break down in this case! Read and
     understand the papers and this code before using it! For the
     fast algorithm read W.H. Press and G.B. Rybicki, 1989, ApJ 338,
     277.

     The code is still stupid in the sense that it wants normal
     frequencies, but returns angular frequency...

     The transcribed version is version 1.7, 2000.07.28

     Unlike the IDL function, the isis code returns a structure
     containing the power spectral density (psd) and period and, if
     specified by qualifiers, also the frequency (nu) and angular
     frequency (om).

\example
     variable t = [0:100:0.1];
     variable c = sin(t)+((rand(1000)/(2^32-1)*0.2)+0.9);
     variable res = scargle(t, c; nu);
     plot(res.period,res.psd);
     plot(res.nu,res.psd);
\done

\function{segment_lc_for_psd}
\synopsis{Function to segment a lightcurve in preparation for a PSD,
    filtering out the segments which do not fit into multiples of
    the segmentation length.}
\usage{segment_lc_for_psd(lc,dt,dimseg)}
\qualifiers{
    \qualifier{gapfactor}{relative factor, telling how large the gap is relative
                to dt [default: 1.1]}
    \qualifier{ratefield}{Name of rate field in lightcurve [default: rate]}
    \qualifier{verbose}{Increase verbosity to display which parts of the
                lightcurve and what proportion was rejected [default=0].}
}
\description
    Given a lightcurve with gaps, this function segments it such that
    only intervals of length "dimseg" (in bins) are present. Data
    that does not fit into integer multiples of the segmentation
    length are cut off.
    
    ATTENTION: Never use a discrete Fourier transform (e.g., 
    foucalc) with a segmentation length ("dimseg") larger than chosen
    in this function!
\notes
    The time array is not altered, so the lightcurve will still
    contain gaps, however none that have different length than
    "dimseg". In other words: The output data arrays can begin at
    arbitrary times, so are *not* sorted to always start at integer
    multiples of the segmentation length.
\example
    dt=1.; % (s)
    len=64.; % (s)
    offset=10; % background level
    T=5.; % (s), sinusoid periodidity
    omega=2*PI/T; % Angular frequency (rad/s)
    time_arr=[0:len:dt];
    lc = struct{ time = time_arr,
                 rate = offset+sin(omega*time_arr)+0.1*grand(int(len/dt)) };
    lc_gaps=struct{
             time=[lc.time[[0:20]],lc.time[[30:55]]],
             rate=[lc.rate[[0:20]],lc.rate[[30:55]]] };
    dimseg=16; % (bins)
    lc_split = segment_lc_for_psd(lc_gaps, dt, dimseg);
    res=foucalc(struct{time=lc_split.time,rate1=lc_split.rate},dimseg);
\seealso{split_lc_at_gaps, foucalc}
\done

\function{setPGPLOTenv}
\synopsis{sets environment variables used by PGPLOT}
\usage{setPGPLOTenv(Double_Type w, h[, hoff[, voff]]);}
\qualifiers{
\qualifier{gif}{sets the PGPLOT_GIF enviroment variables}
}
\description
    The environment variables for the PGPLOT postscript driver,
    \code{PGPLOT_PS_{WIDTH,HEIGHT,HOFFSET,VOFFSET}}, are set
    to the specified width \code{w} and height \code{h}. The default
    horizontal and vertical offset is \code{hoff=0} and \code{voff=0}.
    The parameters \code{w}, \code{h}, \code{hoff}, \code{voff} specify the size in cm,
    while the enviroment variables are measured in milli-inches.
    
    If the \code{gif} qualifier is given, \code{PGPLOT_GIF_{WIDTH,HEIGHT}}
    are set to \code{w} and \code{h} (in pixels).
\seealso{putenv, http://www.astro.caltech.edu/~tjp/pgplot/devices.html}
\done

\function{set_2d_data_grid}
\usage{set_2d_data_grid(Double_Type X[], Double_Type Y[]);}
\altusage{set_2d_data_grid(Double_Type X_lo[], Double_Type X_hi[], Double_Type Y_lo[], Double_Type Y_hi[]);}
\synopsis{define a two dimensional data grid required for 2D fits}
\description
    For fitting 2d data the corresponding grid is set with this function.
    If \code{set_2d_data_grid} is not called, the function \code{define_counts_2d}
    uses the indices of the image as grid.
    For binned fit functions bin_lo and bin_hi have to be provided.
    If only single X and Y arrays are provided, only fit functions
    which are evaluated on these grid points can be used.
\seealso{define_counts_2d, gauss_2d_integrated, gauss_2d}
\done

\function{set_bin_corr_factor}
\synopsis{sets a simple bin correction factor (used in plot_data/unfold)}
\usage{set_bin_corr_factor(Integer_Type data_id, Double_Type[nbins] corr_factor);}
\seealso{load_fermi,get_bin_corr_factor,plot_unfold}
\done

\function{set_gauss_line_par}
\synopsis{initializes a line in the lines-model with the parameters of a gauss-line}
\usage{set_gauss_line_par([id,] line, area, center, sigma);}
\description
    If \code{id} is not specified, \code{id=1} is used.
    \code{line} is the name in the lines-model, appearing as parameters
    \code{line_lam}, \code{line_EW}, \code{line_FWHM} and \code{line_A}.
\seealso{gauss, lines}
\done

\function{set_lines_par_fun}
\synopsis{sets the derived amplitude parameter in a lines-model}
\usage{set_lines_par_fun([Integer_Type id]);}
\description
    The amplitude parameters of the lines-model are set for every line:\n
    \code{set_par_fun("lines(id).line_A", "lines(id).line_EW/lines(id).line_FWHM");}\n
    If code{id} is not specified, id=1 is used.
\seealso{gauss, lines, unset_lines_par_fun}
\done

\function{set_params_interpol}
\synopsis{interpolates between two parameter sets}
\usage{Struct_Type params[] = interpol_params(Struct_Type p1[], Struct_Type p2[]);
\altusage{Struct_Type params[] = interpol_params(Struct_Type p1[], Struct_Type p2[], Double_Type frac);}
\altusage{Struct_Type params[] = interpol_params(Struct_Type p1[], Struct_Type p2[], par, Double_Type value);}
}
\description
   \code{p1} and \code{p2} are parameter lists for the same fit-function
   as obtained with \code{get_params}. Changing \code{frac} from 0 to 1,
   the parameter set is interpolated from \code{p1} to \code{p2}.
   If \code{frac} is not specified, \code{frac=0.5} is assumed.
   \code{frac} can also be obtained by interpolating
   the parameter \code{par} to the value \code{value}.
\done

\function{set_par_from_confmap_table}
\synopsis{set parameters from a table obtained by get_confmap}
\usage{set_par_from_confmap_results(Struct_Type table, Integer_Type i);}
\description
    The function sets the parameters of \code{table}'s row \code{i}.
    As parameter names are infered from column names, the FITS
    table may have to be read with the \code{casesen} qualifier.
\seealso{get_confmap}
\done

\function{set_plot_labels}
\synopsis{Restore default plot labels of isis_fancy_plots package}
\usage{set_plot_labels(;pg_font="\\\\fr") -or- set_plot_labels(;pg_font=``\\\\fr``)}
\qualifiers{
\qualifier{pg_font}{="\\\\fn", "\\\\fr", "\\\\fs", or "\\\\fi"}
}
\description
\seealso{new_plot_labels, fancy_plot_unit, add_plot_unit}
\done

\function{set_plot_widths}
\synopsis{Sets plot widths for isis_fancy_plots package}
\usage{set_plot_widths([;qualifiers]);}
\qualifiers{
\qualifier{m_width}{Model line width}
\qualifier{d_width}{Data line width}
\qualifier{de_width}{Data error bar line width}
\qualifier{r_width}{Residual line width}
\qualifier{re_width}{Residual error bar line width}
\qualifier{ebar_x}{X error bar term cap length}
\qualifier{ebar_y}{Y error bar term cap length}
\qualifier{data_err}{!=0 X error bars plotted}
}
\description

 set_plot_widths(; d_width=#, de_width=#, r_width=#, re_width=#, m_width=#,
                   ebar_x=#, ebar_y=#, data_err=#);

 Sets line widths on data, residuals, error bars, and models, sets the
 length of x/y error bar term caps (ebar_x, ebar_y), and toggles the
 X error bar plotting. Values are retained until explicitly overwritten.
\seealso{nice_width, pg_info}
\done

\function{set_power_scale}
\synopsis{Determine the y-axis is scaled by energy when using the isis_fancy_plot package.}
\usage{set_power_scale(Integer_Type);}
\description
set_power_scale(a);  

  Determine how the y-axis is scaled by energy or wavelength when using
  the fancy_plot routines. a=1, E|lambda is set to the midpoint of the 
  bin. a=2, E|lambda is set to the geometric midpoint (i.e., sqrt(Elo*Ehi)).
  a=3, E|lambda is set to the average value assuming an x^-2 powerlaw.
  Any other values, and this message is printed.
\seealso{fancy_plot_unit, add_plot_unit, plot_unfold, plot_counts}
\done

\function{set_simputfile_model_grid}
\synopsis{set the model grid in the SIMPUT structure}
\usage{set_simputfile_model_grid(Struct_Type str);}
%\altusage{set_simputfile_model_grid(Struct_Type str, Elow, Eup, Estep);}
\description
    All energies are given in keV. By default, the spectrum is
    evaluated from 0.05-24.0 keV in steps of 0.01 keV.
\seealso{create_basic_simputfile,get_simputfile_struct,eval_simputfile,set_simputfile_flux}
\done

\function{set_xyfit_qualifier}
\synopsis{modify the meta data of an xy-dataset defined by \code{define_xydata} and used for an xy-fit}
#c%{{{
\usage{set_xyfit_qualifier(data_id; qualifiers); }
\description
    This function can be used to modify the information for the xy-data.
    The used qualifiers are combined with the data structure. Normally
    the qualifiers to be set should be \code{x_mdl} or \code{curve_parameter}.
\example
    set_xyfit_qualifier(id; curve_parameter=[0:2*PI:#3000]);
\seealso{define_xydata, xyfit_fun, plot_xyfit}
\done

\function{set_xyfit_sys_err_frac}
\synopsis{adds systematic uncertainties to an xy-dataset defined by \code{define_xydata}}
#c%{{{
\usage{set_xyfit_sys_err_frac(data_id, [xsyserr,] ysyserr);}
\description
    A systematic uncertainty is added in quadrature to either
    the x- and y-data or to the latter only. The combined
    uncertainty considered by \code{xyfit_residuals} then is

      err_new = sqrt( err^2 + (data * syserr)^2 )

    where \code{data} is the x- or y-data as defined using
    \code{define_xydata} and \code{err} is the corresponding
    defined uncertainty.

    By default, no systematics uncertainties are considered.
\example
    % adds 0.5% systematics to the y-data only
    set_xyfit_sys_err_frac(1, .005);
    
    % adds systematics to both, x- (0.5%) and y-data (1%)
    set_xyfit_sys_err_frac(1, .005, 0.01);
\seealso{xyfit_residuals, define_xydata, set_sys_err_frac}
\done

\function{shift_hist}
\synopsis{shift a histogram}
\usage{Struct_Type hist = shift_hist(hist, shift);}
\description
    Shift the grid of a histogram structure with fields bin_lo, 
    bin_hi, value, and err. The bin_lo and bin_hi fields are shifted
    by the amount shift. If other fields are present,
    those are preserved and passed on. Only presence of bin_lo and 
    bin_hi are checked for. 
\seealso{add_hist, scale_hist, stretch_hist}
\done

\function{shift_intpol}
\synopsis{Shifts the elements of an array continuously}
\usage{Array_Type shift_intpol(Array_Type array, Double_Type n)}
\description
    This function does in principle work like the 'shift'
    function, with the exception that the amount of the
    shift may be a floating point number. The values of
    the resulting array are in that case re-distributed
    by linear interpolation. Thereby, the the sum of the
    array values is still preserved.
\seealso{shift}
\done

\function{show_slang_code}
\usage{show_slang_code(String_Type function);}
\description
    The function \code{show_slang_code} reads all \code{.sl} files
    in the directories contained in the S-Lang load path,
    and *tries* to find the definition of \code{function}
    by parsing the code for {}-brackets and comments.
    In its current version, \code{show_slang_code} gets confused, e.g.,
    from {}-brackets and % characters in strings.
\done

\function{simbad2ds9}
\synopsis{Converts a SIMBAD cone search ASCII file into a DS9 region
    file (default outfile: "ds9.reg")}
\qualifiers{
\qualifier{radius}{circle radius (arcsec), default: 20arcsec}
\qualifier{type}{regex to filter for source type, default ""}
}
\notes
    See http://simbad.u-strasbg.fr/simbad/sim-display?data=otypes
    for a list of type abbreviations
\example
    simbad2ds9("simbadascii.txt","output.reg" ; radius=10, type="HXB");
\usage{simbad2ds9(String_Type filename, [String_Type outfile]);}
\done

\function{simfit_namespace}
\synopsis{implements all SimFit-functions into a namespace}
\usage{simfit_namespace(Struct_Type SimFit);}
\qualifiers{
    \qualifier{name}{name of the new namespace (default: simfit)}
    \qualifier{chatty}{chattiness of this function (default: 1)}
}
\description
    Takes a simultaneous fit structure as input and
    implements all available functions defined in there
    into a new namespace. In this way the user no longer
    has to type the structure's name in front of the
    functions, but use the functions directly.

    Note that in this namespace the ISIS intrinsic
    functions, such as 'fit_fun', 'set_par', etc. are
    being overwritten by the SimFit-versions.

    Furthermore, 'eval_counts' is defined as a combination
    of 'eval_groups' and 'eval_global', and 'fit_counts'
    now points to 'fit_smart'.

    To access the original ISIS functions just switch the
    namespace back to 'isis' using 'use_namespace' or
    access the functions via isis->function_name

    WARNING: there is no check implemented yet that the
    given structure actually is a SimFit-structure, so
    every structure containing reference to functions may
    be passed, which might HARM your ISIS-session or
    machine!

    FINALLY, if there will be any error the namespace is
    most likely set to 'isis' afterwards (we tried to
    catch errors in the SimFit-functions but this does
    not work for, e.g., syntax errors within the shell).
\seealso{simultaneous_fit, use_namespace}
\done

\function{simplify_polygon}
\synopsis{Simplify polygons using the Douglas Peucker Algorithm}
\usage{(xx,yy)=simplify_polygon(x,y,d);}
\description
 Use the Douglas Peucker Algorithm to simplify the polygon P
 defined by the positions in the arrays x,y such that the
 maximum distance between all segments  of the resulting
 polygon P2 defined by (xx,yy) is smaller than d.
 See the help for the function douglas_peucker for caveats
 Note that the argument d in simplify_polyon defines the
 distance, while the corresponding argument in douglas_peucker
 defines the distance squared!

 \seealso{douglas_peucker}
\done

\function{simput_athenacrab}
\synopsis{returns command to create a standard Athena Crab Simput File}
\usage{String_Type cmd = simput_athenacrab(Double_Type flux);}
\description
    Flux has to be given in Crab. The naming convention is that the
    flux (in micro Crab = 1e-6 Crab or in erg/cm^2/s if unit=cgs) is
    encoded in the filename. The spectrum has an absorption of 4E21
    cm^-2, norm of 9.5 ph/kev/cm^2/s at 1keV, and powerlaw index of
    2.1. The source position is RA=Dec=0.
\qualifiers{
\qualifier{unit}{crab or cgs, default: crab}
\qualifier{dir}{To append a full directory path, default: cwd}
\qualifier{logEgrid}{use a logarithmic energy grid (from Elow to
            Eup with Nbins), default: no}
\qualifier{Nbins}{number of energy bins created from Elow to Eup,
            default: 1000}
}
\done

\function{simulate_data}
\synopsis{fakes a spectrum}
\usage{Integer_Type id = simulate_data(String_Type RSPfile, Double_Type exposure);}
\description
    The currently defined fit-function is used to fake the spectrum.
\seealso{fakeit}
\done

\function{simultaneous_fit}
\synopsis{initializes a structure to handle simultaneous/combined
    fits of a lot of data}
\usage{Struct_Type simultaneous_fit();}
\description
    The advantage of fitting a lot of data at the same is
    that fit parameters, which seem to be the same for all
    data, can be fitted properly. That results in a reduced
    number of free parameters for individual observations
    or data groups, and thus constrains parameters even
    better at low data quality.

    The disadvantages are, that a) a lot of painful work
    has to be done on tieing and freezing parameters and
    b) a fit using 'fit_counts' will take hours to days.
    There are several functions implemented within the
    returned structure, which can help to solve that issues.

    To access the help of a function, simply call it with
    the 'help' qualifier (like for the xfig functions).

    References for simultaneous fits in ISIS are
      Kuehnel et al. 2015, Acta Polytechnica 55(2), 123127
      Kuehnel et al. 2016, Acta Polytechnica 56(1), 4146
    
    NOTE: The simultaneous fits are still in development.
\done

\function{simultaneous_fit.add_data}
\synopsis{adds the given data to the simultaneous/combined fit}
\usage{simultaneous_fit.add_data(List_Type data_for_one_group);
 or simultaneous_fit.add_data(List_Type[] multiple_groups);}
\qualifiers{
    \qualifier{nosort}{do not sort the parameters (implies 'nologic')}
    \qualifier{nologic}{do not apply parameter logic}
}
\description
    NOTE: compared to the previous version of this function the data
          has to be provided as a list or array of lists!

    The given data will be loaded or defined and added to the
    simultaneous/combined fit. The data can be provided in different
    formats inside a surrounding list. All data within this list is
    treated as a single data group. Further groups can be defined by
    either calling add_data again or by providing an array of lists,
    where each item corresponds to a group. The data inside the
    list(s) can be given as
    - String_Type filename: the given filename is loaded by a call
      to 'load_data'.
    - Integer_Type dataset: the ID of an already existing ISIS
      dataset.
    - Struct_Type, the field layouts allowed are
      - { Double_Type[] bin_lo, bin_hi, value, error }: is passed
        to 'define_counts' by default.
      - { String_Type filename }: is passed to 'load_data' by
        default.
      Further fields allow additional features:
        - { ..., Integer_Type roc }: Rmf_OGIP_Compliance is set to
           'roc' before the data is defined or loaded.
        - { ..., Ref_Type loadfun }: reference to a function which
          should be used instead of 'load_data' or 'define_counts'.
          This function has to return the new ISIS dataset ID.
        - { ..., Struct_Type qualifiers } - the structure is passed
          as qualifiers to the function for loading or defining
          the data.
     
    After the data and corresponding data groups have been defined
    the resulting parameter logic is analyzed by
    'simultaneous_fit.sort_params'. As this step might take a long
    time and is only needed once (during the last call to 'add_data'
    or manually), you can skip this step using the 'nosort'
    qualifier.
    Finally, the analyzed parameter logic is applied to the defined
    model and parameters using 'simultaneous_fit.apply_logic' unless
    the 'nologic' qualifier is set.
\example
    % define a simultaneous fit structure first
    variable simfit = simultaneous_fit();
    
    % add a single RXTE observation (consisting of a PCA-
    % and a HEXTE-spectrum, both taken simultaenously, of
    % course)
    simfit.add_data({"pca.pha", "hexte.pha"});

    % add two data groups consisting of an RXTE (PCA and HEXTE) and
    % a Swift-XRT observation
    simfit.add_data([ % note the array
      {"pha.pha", "hexte.pha"}, % first data group
      {struct { filename = "xrt.pha", roc = 0 }} % second data group
    ]);

    % add one already existing ISIS datasets and load an PHA-file
    % both defining a single data group
    % fit grouped into two data groups
    simfit.add_data({1, "file.pha"});
\seealso{load_data, define_counts, simultaneous_fit.sort_params,
    simultaneous_fit.apply_logic}
\done

\function{simultaneous_fit.add_data_old}
\synopsis{adds the given data to the simultaneous/combined fit (old version)}
\usage{simultaneous_fit.add_data(String_Type[] phafiles);
 or simultaneous_fit.add_data(Struct_Type[] { bin_lo, bin_hi, value, error });
 or simultaneous_fit.add_data(List_Type datagroups);}
\qualifiers{
    \qualifier{loadfun}{function reference to load the given filename
              (default &load_data), further arguments are passed}
    \qualifier{loadfunlist}{indicates that 'loadfun' already returns an
              array of datagroups, i.e. a list with integer
              arrays representing the datasets in each group}
    \qualifier{nosort}{do not sort the parameters (implies 'nologic')}
    \qualifier{nologic}{do not apply parameter logic}
    \qualifier{ROC}{value for Rmf_OGIP_Compliance for each data,
              has to be given in the same structure as the
              input data, i.e., as integer array or list of
              integer arrays}
}
\description
    This function is deprecated and is superseded by the default
    one in the simultaneous-fit-structure.

    The given data will be loaded or defined and added to the
    simultaneous/combined fit. This data can be given as either
    a file name to the spectrum as accepted by 'load_data' or a
    structure as accepted by 'define_counts'.

    If an array of data is given, this data will be treated
    as simultaneously recorded data (called a data group).
    That means, if any parameter logic is applied afterwards,
    the parameters of all datasets are tied to each other.
    This logic will be applied automatically unless the
    'nologic' qualifier is given. To apply it later,
    'simultaneous_fit.apply_logic' may be called. This logic,
    however, has to be known first, which is done by
    'simultaneous_fit.sort_params' automatically. Again,
    this may be skipped by the 'nosort' qualifier.

    Further on, a list of several data groups can be given.
    This list may contain both, filenames to spectra and
    structures and arrays of these as well.
\example
    % define a simultaneous fit first
    variable simfit = simultaneous_fit();
    
    % add a single RXTE observation (consisting of a PCA-
    % and a HEXTE-spectrum, both taken simultaenously, of
    % course)
    simfit.add_data(["pca.pha", "hexte.pha"]);

    % add an RXTE (PCA and HEXTE) and a Swift-XRT
    % observation (both observations were NOT taken
    % simultaneously -> List_Type)
    simfit.add_data({["pha.pha", "hexte.pha"], "xrt.pha"});
\seealso{load_data, define_counts, simultaneous_fit.sort_params,
    simultaneous_fit.apply_logic}
\done

\function{simultaneous_fit.apply_logic}
\synopsis{ties parameters of simultaneous data to each other}
\usage{simultaneous_fit.apply_logic();}
\qualifiers{
    \qualifier{keepparfun}{do not reset all parameter functions}
    \qualifier{chatty}{chattiness of this function (default: 1)}
}

\description
    For each defined data an individual set of parameters
    exist. If some of the data were taken simultaneously
    the parameters should be the same. This function applies
    this logic by tieing the parameters of these data to
    each other by setting the corresponding parameter
    functions via set_par_fun. To do so the functions for
    all parameters are set to NULL first (this can be
    inhibited using the keepparfun-qualifier).
\seealso{simultaneous_fit.add_data, simultaneous_fit.fit_fun,
    set_par_fun}
\done

\function{simultaneous_fit.copy_par}
\synopsis{copy the parameters of one group to another}
\usage{simultaneous_fit.copy_par(Integer_Type from_group, to_group);}
\qualifiers{
    \qualifier{limits}{copy parameter limits as well}
}
\description
    The values of the group parameters of the first given
    datagroup are copied to the second datagroup. Parameter
    limits are not copied unless the limits-qualifier is
    set.

    Instead of a single target data group, 'to_group', an
    array of groups, Integer_Type[], may be given.
 \example
    % copy the parameter values of group 5 to group 6
    simultaneous_fit.copy_par(5, 6);
\seealso{simultaneous_fit.list_groups}
\done

\function{simultaneous_fit.delete_data}
\synopsis{deletes the given data from the simultaneous/combined fit}
\usage{simultaneous_fit.delete_data(Integer_Type[] group[, Integer_Type[] data]);}
\qualifiers{
    \qualifier{keep}{the associated data are not deleted by
           'delete_data', thus only the logic of the
           simultaneous fit is modified}
}
\description
    The given data 'group' is removed from the simultaneous
    fit and the associated data is deleted (using the ISIS
    internal function 'delete_data'. The second, optional
    argument can be used to delete specific data from a
    group specified by its number within the group (starting
    at one).

    If the first group is deleted, the global parameters
    might change. In that case a warning message will be
    raised and you should check the parameter dependencies!

    Further on, if the first dataset of a group is deleted,
    the parameter logic will change and paramaters will be
    tied again according to that logic (but of that group
    only). Please check the resulting parameter dependencies
    (in particular the value-functions of global parameters)
    as suggested by the warning message!
\example
    % delete data group 3 and 5
    simultaneous_fit.delete_data([3,5]);

    % delete data group 1
    % this might cause that a global parameter gets deleted
    simultaneous_fit.delete_data(1);
    
    % delete the first spectrum from group 4 only
    % this will probably change the group parameter dependencies
    simultaneous_fit.delete_data(4, 1);
\seealso{delete_data, simultaneous_fit.add_data}
\done

\function{simultaneous_fit.eval_global}
\synopsis{evaluates the model for all global parameters}
\usage{Integer_Type simultaneous_fit.eval_global();}
\description
    All data groups are included within the evaulation
    of the model, but the only free parameters are the
    global parameters.
    The displayed fit statistic includes all data, but
    the global parameters only. The statistics are
    available afterwards in the model.stat struct-array.

    All qualifiers are passed to eval_counts.

\seealso{simultaneous_fit.eval_groups, eval_counts}
\done

\function{simultaneous_fit.eval_groups}
\synopsis{evaluates the model for each (given) data group}
\usage{Integer_Type simultaneous_fit.eval_groups([Integer_Type[] groups]);}
\description
    Loops over each (given) data group and evaulates the
    model using only those parameters assigned to that
    group. All other groups and parameters are excluded.
    If no group is given, every group is evaluated.
    Note, that the number of the first data group is 1
    (not 0 as for arrays).
    The displayed fit statistics are those of the
    individual group only. The statistics are available
    afterwards in the model.stat struct-array.

 All qualifiers are passed to eval_counts
\seealso{simultaneous_fit.eval_global, eval_counts}
\done

\function{simultaneous_fit.filter_groups}
\synopsis{return the group numbers matching a filter}
\usage{simultaneous_fit.filter_groups([Integer_Type or String_Type filter]);}
\description
    Returns the group numbers which reduced chi-square
    matches the given filter. The filter can be one of
    the following three options:
    1) no filter given - the group with the worst fit
       statistic is returned
    2) integer (n) given - sorts the groups after their
       fit statistic (beginning at the worst) and returns
       the first n groups
    3) string given - has to be given in the format
       "[operator][number]". Those group number are
       returned which reduced chi-squares (chi) matches
         chi [operator] [number]
       The operators >, >=, <, <=, and == are possible.
       Multiple of those rules can be specified if
       separated with ','.

    This function can be very useful if combined with
    .select_groups
\example
    % initialize the simultaneous fit
    simpi = simulteneous_fit();
    ...
    
    % return the three worst fitted groups
    grps = simpi.filter_groups(3);

    % return those which have 1.7 <= chi^2_red <= 3
    grps = simpi.filter_groups(">=1.7,<=3");

    % directly select the worst group for fitting,
    % plotting, etc.
    simpi.select_groups(simpi.filter_groups());
\seealso{simultaneous_fit.select_groups}
\done

\function{simultaneous_fit.fit_fun}
\synopsis{defines the model applied within the simultaneous fit}
\usage{simultaneous_fit.fit_fun(String_Type fit_fun);}
\qualifiers{
    \qualifier{nosort}{do not sort the parameters
                (implies 'nologic' and 'nohistory')}
    \qualifier{nologic}{do not apply parameter logic}
    \qualifier{nohistory}{do not apply the set_par_fun-history}
    \qualifier{ask}{see set_par_fun_history}
    \qualifier{chatty}{chattiness of this function (default: 1)}
}
\description
    The given string is used to define the fit function.
    Usually, a lot of data is added to the simultaneous
    fit, which might lead to more complicated fit functions.
    In order to simplify its definition, some place holders
    can be used within the fit function:
      % - will be replaced by Isis_Active_Dataset
    More placeholders will be implemented in the future.

    Afterwards, the parameters are sorted into global and
    group ones and the parameter logic is applied to the
    model (see simultaneous_fit.sort_params and
    simultaneous_fit.apply_logic).

    Finally, any parameter functions previously define
    with simultaneous_fit.set_par_fun are restored by
    calling simultaneous_fit.set_par_fun_history(; apply).
\example
    % the model shall be an absorbed powerlaw, where there
    % are independent parameters of the powerlaw for each
    % data group, but a single absorber, which is applied
    % to all data
    simultaneous_fit.fit_fun("tbnew(1)*powerlaw(%)");
\seealso{fit_fun, simultaneous_fit.sort_params,
    simultaneous_fit.apply_logic, simultaneous_fit.set_par_fun_history}
\done

\function{simultaneous_fit.fit_global}
\synopsis{performs a fit of all global parameters}
\usage{Integer_Type simultaneous_fit.fit_global();}
\description
    All data groups are included within the fit, but the
    only free parameters are the global parameters. In
    particular, all group parameters are fixed!
    The displayed fit statistic includes all data, but
    the global parameters only. The statistics are
    available afterwards in the model.stat struct-array.

    All qualifiers are passed to fit_counts.

\seealso{simultaneous_fit.fit_group, fit_counts}
\done

\function{simultaneous_fit.fit_groups}
\synopsis{performs a fit of each (given) data group}
\usage{Integer_Type simultaneous_fit.fit_groups([Integer_Type[] groups]);}
\description
    Loops over each (given) data group and performs a fit
    of only those parameters assigned to that group. All
    other groups and parameters are excluded from the fit.
    In particular, all global parameters are fixed! If no
    group is given, a fit for every group is performed.
    Note, that the number of the first data group is 1
    (not 0 as for arrays).
    The displayed fit statistics are those of the
    individual group only. The statistics are available
    afterwards in the model.stat struct-array.

    All qualifiers are passed to fit_counts.

\seealso{simultaneous_fit.fit_global, fit_counts}
\done

\function{simultaneous_fit.fit_pars}
\synopsis{runs fit_pars for the given data group}
\usage{Struct_Type simultaneous_fit.fit_pars( Integer_Type group );}
\altusage{Struct_Type simultaneous_fit.fit_pars( String_Type parname );}
\qualifiers{
\qualifier{frozen}{[=0] Perform fit_pars also for frozen Parameters.
                   NOTE: just works if 'parname' is given!}
\qualifier{force}{If group=0 (globals) force to perform fit_pars for all
                  globals. Without this qualifier the parname of ONE global
                  parameter is required!}
}
\description
    Calculates the confidence level(s) of the parameter(s)
    associated to the given data 'group' or 'parname'.
    The function uses 'fit_pars' and returns its result.

    Instead of the number of a data group, the name of
    a parameter may be given instead. In that
    case the confidence level of this parameter is
    calculated only.

    Note that for a global parameter the 'parname' has
    to be given!

    Also note that normally frozen parameters are ignored.
    If this function is called with 'parname' and the
    'frozen' qualifier is given, the confidence level
    is calculated nevertheless.

    It is important to keep in mind, that a 'fit_pars'
    can lead to a new best fit. If the confidence levels
    of a group as a whole are calculated a new best fit
    of one of the group parameter is automatically taken
    into account, but not if the confidence level of a
    single parameter (e.g., using 'parname') is looked
    at. This is a problem especially for the global
    parameters. Use 'mpi_fit_pars' for your advantage!
    
    Note, that a usual 'fit_pars' will take a tremendous
    amount of time (like 'fit_counts'). Thus, this
    function accepts one data group only and it is
    recommended to use only one parameter in addition.
    It might be helpful to reduce the accuracy with which
    fitting methods try to calculate chi square values
    and/or parameters (see set_fit_method).

    Further on, this approach is ideal to be used with
    Torque (see 'simultaneous_fit.fit_pars_jobfile').

    All qualifiers are also passed to fit_pars.

\seealso{fit_pars, simultaneous_fit.list_groups,
    simultaneous_fit.list_global, simultaneous_fit.fit_pars_jobfile}
\done

\function{simultaneous_fit.fit_pars_collect}
\synopsis{reads the results of a former fit_pars or mpi_fit_pars
    and checks for a new best-fit}
\usage{Struct_Type simultaneous_fit.fit_pars_collect(String_Type FITSpattern);}
\qualifiers{
    \qualifier{chitol}{how much a delta chi-square  has to be
                     to actually consider this fit as a new
                     best-fit or a worse-fit (default: 1e-10)}
    \qualifier{setnewbestpars}{sets the parameters to those of a
                     possible new best-fit (see below)}
    \qualifier{bestpars}{reference to a variable where all found
                     parameter values and other helpful
                     information is returned}
    \qualifier{parspriority}{in case parameters of a new best-fit
                     are set, this qualifiers decides
                     whether the changed "global"- or
                     "group"-parameters are set (both are
                     not possible!) (default: "global")}
    \qualifier{PARpattern}{the string which replaces "_conf.fits"
                     to match the parameter file
                     (default: "_best.par")}
    \qualifier{silent}{suppress warning messages (BIT wise)
                     1: "fit ... is worse"
                     2: "new best-fit ... found"
                     4: "no uncertainties ... found"
                     8: "setting new ... re-fit ..."}
}
\description
    If the uncertainties of a simultaneous fit have been
    calculated using Torque-jobfiles (e.g., by using the
    mpi_fit_pars_jobfile function), the results will be
    saved in several FITS-files.
    This function reads all FITS-files matching the given
    'FITSpattern' and returns a single structure containing
    all results (similar to a single fit_pars structure).
    Furthermore, if a new best-fit was found during one
    of the calculations, a warning message will be shown.
    In case of a crashed calculation (resulting in missing
    uncertainties) a warning message will be shown as well.

    If the qualifier "setnewbestpars" is set, the parameters
    of a new best-fit will be set automatically. However,
    new global parameters causes the group parameters to be
    fitted again and vise versa. For this reason, new group
    parameters are discarded in favor for new global ones
    (to change this priority use the 'parspriority'
    qualifier).

    Note, that the par-files saved by fit_pars are needed to
    set the remaining parameters in case of a new best-fit!
\seealso{simultaneous_fit.mpi_fit_pars_jobfile, simultaneous_fit.fit_pars_jobfile}
\done

\function{simultaneous_fit.fit_pars_jobfile}
\synopsis{writes a Torque-jobfile to calculate confidence levels}
\usage{simultaneous_fit.fit_pars_jobfile(String_Type jobfile, scriptfile);}
\qualifiers{
    \qualifier{force}{overwrites an existing jobfile or script
                  (default: don't overwrite)}
    \qualifier{writescript}{writes a template for the script into
                  the given filename (file will be overwritten)}
}
\description
    Writes a Torque 'jobfile' to calculate the confidence
    levels of all (free) parameters. Therefore, the filename
    of the script for loading the data, defining the model
    and running simultaneous_fit.fit_pars is mandatory.
    This script will be called by each job with a
    command line argument, which is either
      - the data group index, for which the confidence levels
        should be calculated for or
      - the parameter name to compute the level
        for a single parameter in the global group.
        This means that the parameters of a data group are
        treated in parallel by multiple jobs.
    In case of a global parameter, the data group argument
    is omitted and the name of the global parameter is
    given instead. The command lines arguments can be
    accessed using '__argv' and '__argc'.
    
    If the qualifier 'writescript' exists, a template for
    the script described above will be written into the
    given 'scriptfile'name. This has to be modified
    afterwards to ensure compatibility and should be
    treated as a support!
    Warning: in case of an already existing filename this
             file will be overwritten!
\seealso{simultaneous_fit.fit_pars}
\done

\function{simultaneous_fit.fit_pars_run_job}
\synopsis{submits the (MPI-)fit_pars-jobfiles until no better fit is found}
\usage{Struct_Type simultaneous_fit.fit_pars_run_job(
                  String_Type or String_Type[] globaljobs, groupjobs,
                  String_Type FITSpattern, String_Type or Ref_Type save
                );}
\qualifiers{
    \qualifier{first}{submits either the jobs for the global- (=0)
              or the group-parameters (=1, default) first}
    \qualifier{maxiter}{maximum number of iterations (default: 3)}
    \qualifier{wait}{seconds to wait before it is checked if the
              Torque-jobs have completed (default: 10)}
}
\description
    Usually, the uncertainties of the group- and global-
    parameters should be calculated using Torque-jobs.
    However, if one job finds a new best-fit of a, e.g.,
    group parameter, the uncertainties of the global
    parameters have to be re-calculated (and vise versa).
    Note, that the calculations of remaining groups in
    that case, which might still be running, don't have
    to be canceled, because the global parameters might
    stay the same even a better group parameter is found!

    This function submits the Torque-jobs for different
    kinds of parmeters (given as filename-patterns or the
    filename(s) for the 'globaljobs' and 'groupjobs')
    until no further best-fit has been found. By default,
    the group jobs are submitted first. If a new best-fit
    was found, the internal .save function is called with
    the given 'save'-filenam to ensure that the current
    (better) parameters can be loaded by the  re-submitted
    jobs. If the 'save'-parameter is a reference to a
    function, this one will be called instead of saving
    the fit automatically. This function does not get any
    parameters and should save parameter AND the model
    field of the simultaneous fit.
    Once no better fit is found a structure similar to
    that of fit_pars is returned.

    As a protection for an (infinite) loop, the above
    procedure is repeated only a few times (adjustable
    using the 'maxiter' qualifier).

    IMPORTANT:
    This function is only a control function! It has to
    know the current parameters and dataset logic, but
    does not require any CPU power! Thus, to free memory,
    the complete loaded DATA, ARFs, and RMFs are DELETED!
    So you CANNOT work with your data afterwards! If you
    agree with that procedure please set the 'agree'-
    qualifier. Otherwise, the function will exit.
\seealso{simultaneous_fit.fit_pars_jobfile}
\done

\function{simultaneous_fit.fit_smart}
\synopsis{combines group- and global-fits to achieve the best-fit}
\usage{Integer_Type simultaneous_fit.fit_smart();}
\qualifiers{
    \qualifier{maxiter}{maximum number of iterations (default: 10)}
    \qualifier{tol}{favored difference in chi square (default: 0.1)}
    \qualifier{chatty}{output statistics for each iteration (default: 1)}
}
\description
    By alternating between fitting the group parameters
    and then the global parameters only, a best-fit is
    tried to achieve. The fit is successful, if the
    difference in delta chi square compared to the
    previous iteration is less than the given tolerance.
    The loop interrupts, if a maximum number of allowed
    iterations is reached (the default number is low on
    purpose).

    Some tests showed that the final best-fit here is worse
    than that achieved by 'fit_counts' (in the order of 10%
    in reduced chi square). If a lot of data is fitted,
    this method is, however, much faster than a simple
    'fit_counts' (which might take days...).

    All qualifiers not listed above are passed to fit_counts
\seealso{simultaneous_fit.fit_global, simultaneous_fit.fit_groups}
\done

\function{simultaneous_fit.freeze}
\synopsis{freezes a/many parameter/parameters}
\usage{simultaneous_fit.freeze(String_Type parameter);}
\description
    In princible, this function does the same as 'freeze'
    with the modifcation that place holders are allowed:
      % - set the function for all defined data
    Note, that here the parameter name has to be provided,
    not the parameter id!
\seealso{freeze}
\done

\function{simultaneous_fit.get_par}
\synopsis{get the value of a/many fit parameter/parameters}
\usage{Double_Type[] simultaneous_fit.get_par(String_Type parameter);}
\description
    Does the same as 'get_par' with the modification that
    the following place holders are allowed:
      % - set the value for all defined data
    Note, that here the parameter name has to be provided,
    not the parameter id!

    All qualifiers are passed to get_par.
\example
    % get the normalization of all powerlaws
    norms = simultaneous_fit.get_par("powerlaw(%).norm");
\seealso{get_par}
\done

\function{simultaneous_fit.group_stats}
\synopsis{prints a summary about the groups current fit-statistics}
\usage{Integer_Type[] simultaneous_fit.group_stats();}
\description
    A histogram of the current red. chi-squares of all
    groups is printed into the terminal. The histogram
    ranges from the smallest to the largest found
    red. chi-square. This range is shown as two ticmarks
    below the x-Axis. The detailed statistics of each
    group can be found in the .model.stat-field.
    The returned array holds the numbers of groups
    sorted by their red. chi-square, starting at the
    worst.
\seealso{printhplot}
\done

\function{simultaneous_fit.list_data}
\synopsis{lists the data groups and their associated dataset IDs}
\usage{simultaneous_fit.list_data([&variable]);}
\description
    Lists the data groups and their associated dataset IDs and
    number of group parameters. If the optional reference to a
    variable is provided, then the IDs as an array of lists
    is put into this variable, where each item corresponds to
    a data group.
\done

\function{simultaneous_fit.list_global}
\synopsis{lists all global parameters}
\usage{simultaneous_fit.list_global();}
\description
    Like 'list_par' this function lists all global parameters.
\seealso{list_par}
\done

\function{simultaneous_fit.list_groups}
\synopsis{lists all parameters of specific data groups}
\usage{simultaneous_fit.list_groups([Integer_Type[] groups]);}
\qualifiers{
    \qualifier{tied}{list parameters, which are determined by a
           value function, also}
}
\description
    Like 'list_par' this function lists all parameters of
    one or more specific data groups. Note, that unlike
    arrays the index of the first data group is 1. If no
    data groups are given the parameters of all groups
    are listed. By default, parameters without a value
    function are listed only.
\seealso{list_par}
\done

\function{simultaneous_fit.load}
\synopsis{loads a simultaneous fit from a FITS-table}
\usage{simultaneous_fit.load(String_Type filename);}
\description
    Uses 'fits_load_fit' to first restore the fit and then
    to load the model-field of the simultaneous fit from
    the FITS-table. Because the data associations don't
    have to be analyzed, any additional use of
    'simultaenous_fit.add_data' before loading the fit (if
    the data is defined by, e.g, 'define_counts') should
    be called with the 'nosort' qualifier to save the time
    needed to create the model-field.

    The function recognizes if the simfit was saved with
    the 'alt'-qualifier of  simultaneous_fit.save. In this
    case 'fits_load_fit' is not used.

    All qualifiers are passed to fits_load_fit.

\example
    % initialize a new simultaneous_fit
    simfit = simultaneous_fit();
    % define the data
    simfit.add_data({
      [define_counts(...), ...],
      ...
    }; nosort);
    % restore the fit (parameters and model-field)
    % from a file previously created by simfit.save
    simfit.load("best-fit.fits"; nodata);
\seealso{simultaneous_fit.save, simultaneous_fit, fits_load_fit}
\done

\function{simultaneous_fit.load_model}
\description
    DEPRECATED, use simultaneous_fit.load
\seealso{simultaneous_fit.load}
\done

\function{simultaneous_fit.mpi_fit_pars}
\synopsis{runs mpi_fit_pars for the given data group}
\usage{Struct_Type simultaneous_fit.mpi_fit_pars( Integer_Type group );}
\description
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    ATTENTION: This function does not work in an open
               isis session, as it links to 'mpi_fit_pars'!
               It is supposed to be used in a script file
               submitted as a torque job only!
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    Calculates the confidence levels of the FREE parameters
    associated to the given data group.
    This function uses the function 'mpi_fit_pars' and its
    result is returned.

    The argument <group> represents the index  of the
    data GROUP, of which the confidence levels are supposed
    to be calculated.
    group = 0 corresponds to the 'group' of the GLOBAL
    parameters!
    
    Note, that a usual 'fit_pars' will take a tremendous
    amount of time (like 'fit_counts'). Thus, this
    function accepts one data group only and it is
    recommended to use only one parameter in addition.
    It might be helpful to reduce the accuracy with which
    fitting methods try to calculate chi square values
    and/or parameters (see set_fit_method).

    Further on, this approach is ideal to be used with
    Torque (see 'simultaneous_fit.mpi_fit_pars_jobfiles').

    All qualifiers are also passed to mpi_fit_pars

\seealso{fit_pars, mpi_fit_pars, simultaneous_fit.list_groups,
    simultaneous_fit.list_global, simultaneous_fit.mpi_fit_pars_jobfiles}
\done

\function{simultaneous_fit.mpi_fit_pars_jobfiles}
\synopsis{writes a Torque-jobfile to calculate confidence levels}
\usage{simultaneous_fit.mpi_fit_pars_jobfiles(String_Type jobfile, scriptfile);}
\qualifiers{
\qualifier{walltime: }{[="00:01:00"] Torque walltime for global & group
                     jobfile}
\qualifier{wt_global: }{[=walltime] Torque walltime for global jobfile}
\qualifier{wt_groups: }{[=walltime] Torque walltime for groups jobfile}
\qualifier{jobname: }{[="simpi_fitpars"] Name of Torque job (& files!)}
\qualifier{logpath: }{[=pwd+jobname] Directory for log files}
\qualifier{force: }{overwrites an existing jobfile or script
                  (default: don't overwrite)}
\qualifier{writescript: }{ writes a template for the script into
                  the given filename (file will be overwritten)}
}
\description
    Writes a Torque 'jobfile' to calculate the confidence
    levels of all parameters. Therefore, the filename of
    the script for loading the data, defining the model
    and running simultaneous_fit.fit_pars is mandatory.
    This script will be called by each job with a
    command line arguments, which represents the data
    group, for which the confidence levels should be
    calculated for. The command line arguments can be
    accessed using '__argv' and '__argc'.

    In case the 'scriptfile' requires additional command
    line arguments add them to the 'scriptfile'-string,e.g.,
    like "scriptfile.sl arg1 arg2", and make sure these
    are asigned correctly!
    
    If the qualifier 'writescript' exists, a template for
    the script described above will be written into the
    given 'scriptfile'name. This has to be modified
    afterwards to ensure compatibility and should be
    treated as a support!
    Warning: in case of an already existing filename this
             file will be overwritten!
\seealso{simultaneous_fit.fit_pars}
\done

\function{simultaneous_fit.plot_group}
\synopsis{plots data and model of a specific data group}
\usage{simultaneous_fit.plot_group([Integer_Type group]);}
\description
    This function plots the data and model of the given
    data group using the 'plot_data' function.

    If no group number is given the currently selected
    group is plotted (if it is the only one selected).
    A specific group can be selected using .select_groups

    All qualifiers are passed to plot_data

\seealso{plot_data, simultaneous_fit.select_groups}
\done

\function{simultaneous_fit.restore}
\synopsis{To be written}
\usage{simultaneous_fit.restore();}
\description
    To be written...
\seealso{simultaneous_fit.setrestore}
\done

\function{simultaneous_fit.save}
\synopsis{saves the simultaneous fit to a FITS-table}
\usage{simultaneous_fit.save_model(String_Type filename[, Struct/String_Type conf]);}
\qualifiers{
    \qualifier{alt}{use an alternative to 'fits_save_fit', see text;
  all other qualifiers are passed to 'fits_save_fit'}
 }
\description
    Uses 'fits_save_fit' to save the simultaneous fit and,
    most importantly, its model-field. The creation of this
    field may take some time due to the fact that the fit-
    function as well as the data associations have to be
    analyzed. Thus, saving it to a FITS-table speeds up
    the loading process if 'simultaneous_fit.load' is used.

    However, the large number of parameters of a simfit
    can lead to "out of memory" exception when using
    'fits_save_fit'. In this case, the 'alt'-qualifier
    uses an alternative FITS-structure for saving, i.e.,
    'fits_save_fit' is not used. Note that in this case
    only the parameters (like in 'save_par') are saved,
    but _not_ the loaded data, noticed energy bins, etc.
\seealso{simultaneous_fit.load, fits_save_fit}
\done

\function{simultaneous_fit.select_groups}
\synopsis{select the default groups to be worked on}
\usage{simultaneous_fit.select_groups([Integer_Type[] groups]);}
\description
    Some functions within in the simultaneous structure
    accept an optional array of group numbers, on which
    they perform their taks. For instance, .fit_groups
    either performs a fit for the given groups only. If
    none are given, the default is to fit all groups.
    This function, however, changes this default such
    that the here given group(s) are used by default.
    If no groups are given the default is set back to
    all groups. To get the current set default you may
    check the .model.current_groups variable (-1 means
    all groups are used).
\done

\function{simultaneous_fit.setrestore}
\synopsis{To be written}
\usage{simultaneous_fit.setrestore();}
\description
    To be written...
\seealso{simultaneous_fit.restore}
\done

\function{simultaneous_fit.set_global}
\synopsis{Sets a specific group parameter to be fitted globally}
\usage{simultaneous_fit.set_global(String_Type grouppar);}
\description
    A wrapper around simultaneous_fit.set_par_fun, which
    simply ties the given group parameter name (using the
    %-placeholder instead of the dataset index) to a single,
    global parameter.
    If successful, a warning message will be shown.
\example
    % use a single photon index for all data
    simultaneous_fit.fit_fun("powerlaw(%)");
    simultaneous_fit.set_global("powerlaw(%).PhoIndex");
\seealso{simultaneous_fit.unset_global,simultaneous_fit.set_par_fun}
\done

\function{simultaneous_fit.set_par}
\synopsis{set the value of a/many fit parameter/parameters}
\usage{simultaneous_fit.set_par(String_Type parameter[, Double_Type value[, Integer_Type freeze[, Double_Type min, max]]]);}
\description
    Does the same as 'set_par' with the modification that
    the following place holders are allowed:
      % - set the value for all defined data
    Note, that here the parameter name has to be provided,
    not the parameter id!

    All qualifiers are passed to set_par.
\example
    % set the normalization of all powerlaws
    simultaneous_fit.set_par("powerlaw(%).norm", 1, 0, 0, 10);
\seealso{set_par}
\done

\function{simultaneous_fit.set_par_fun}
\synopsis{Define the value of a/many fit parameter.parameters using a function}
\usage{simultaneous_fit.set_par_fun(String_Type parameter, String_Type valuefunction);}
\description
    In princible, this function does the same as 'set_par_fun'
    with the modifcation that place holders are allowed:
      % - set the function for all defined data
    Note, that here the parameter name has to be provided,
    not the parameter id!

    This function tries to recognize, if the value function
    is a parameter itself, which should be applied to a set
    of the same parameters (e.g. the photon indices are set
    to a single one). In that case the parameter given in
    the value function will be treated as a global one from
    now on.

    On the other side, if the value functions of a set of
    parameters are deleted (valuefunction=NULL), which were
    tied to a global parameter before, this parameter is
    treated as a group one.

    In both cases, a warning message will be shown in order
    to inform the user which parameter has been treated as
    global or group parameter.

    In an upcoming version, the place holders are allowed
    in the value function as well.

    Any call to this function is remembered in an internal
    history for applying the calls later again (see
    simultaneous_fit.set_par_fun_history).
\example
    % use a single photon index for all data, while
    % keeping individual normalizations
    simultaneous_fit.fit_fun("powerlaw(%)");
    simultaneous_fit.set_par_fun("powerlaw(%).PhoIndex",
      "powerlaw(1).PhoIndex");

    % allow individual photon indices again
    simultaneous_fit.set_par_fun("powerlaw(%).PhoIndex",
      NULL);

    % more fascinating, tie the photon index to a function
    % of the powerlaw normalization
    simultaneous_fit.set_par_fun("powerlaw(%).PhoIndex",
      "constant(1).factor + powerlaw(%).norm * constant(2).factor");
\seealso{set_par_fun, simultaneous_fit.set_par_fun_history}
\done

\function{simultaneous_fit.set_par_fun_history}
\synopsis{applies or clears the history of set_par_fun}
\usage{simultaneous_fit.set_par_fun_history(; qualifiers);}
\qualifiers{
    \qualifier{get}{returns the history of calls to set_par_fun}
    \qualifier{print}{print the history}
    \qualifier{apply}{applies all calls of the history again}
    \qualifier{ask}{prompts the user before applying each call
              of the history (apply-qualifier required)}
    \qualifier{clear}{clears the history}
    \qualifier{chatty}{chattiness of this function (default: 1)}
    \qualifier{nologic}{do not call apply_logic during the history
              is applied (apply-qualifier)}
}
\description
    Whenever simultaneous_fit.set_par_fun is called the
    arguments passed are remembered in an internal
    history. This functions allows to apply each call of
    the history again, retrieve the history, or clear it.

    This is particularly useful once the parameter logic
    is set by simultaneous_fit.apply_logic. Note that
    simultaneous_fit.fit_fun applies the history auto-
    matically.
    Note that applying the history first resets the
    parameter logic with simultaneous_fit.apply_logic.

    In case the history should be returned a list is
    returned, where each item is a call to set_par_fun
    with the two strings which have been passed.
\seealso{simultaneous_fit.set_par_fun,simultaneous_fit.apply_logic}
\done

\function{simultaneous_fit.sort_params}
\synopsis{sort all fit parameters into global and group ones}
\usage{simultaneous_fit.sort_params();}
\qualifiers{
    \qualifier{chatty}{chattyness of this function (default: 1)}
}
\description
    The defined data may contain both, simultaneous and non-
    simultaneous data. Thus, there are parameters which apply
    to individual data on one hand (called group parameters),
    and on the other hand parameters which act on all defined
    data (called global parameters). This function determines
    the relationship between the defined data in order to sort
    the existing parameters into global and group parameters.
\seealso{simultaneous_fit.add_data, simultaneous_fit.apply_logic}
\done

\function{simultaneous_fit.steppar}
\synopsis{runs steppar based on the simultanous fit logic}
\usage{Struct_Type simultaneous_fit.steppar( String_Type parname );}
\altusage{Struct_Type simultaneous_fit.steppar( Integer_Type idx );}
\qualifiers{
\qualifier{keys}{An additional structure is returned that can be used as FITS header keys}
\qualifier{frozen}{[=0] Perform steppar also for frozen Parameters.}
\qualifier{range}{[parmin,parmax] Stepping range. Default is the
                   minimal/maximal allowed parameter value.}
\qualifier{nsteps}{[=10] Number of steps the parameter 'par' is
                   stepped from range[0] to range[1].}
\qualifier{reset}{If given after each step the initial parameter set, which was
                    valid before this function was called, is restored.}
\qualifier{fit}{[=&fit_counts] Reference to the function running the fit algorithm.}
\qualifier{fitargs}{Arguments required by the 'fit' function. See __push_list for
                    format information!}
\qualifier{rerun}{Reruns a stepping procedure based on results of a previous run,
                    i.e., before each step the according parameter set of the previous
                    run is loaded, e.g., to improve the results using another fit_method.}
\qualifier{resume}{Missing steps in the given steppar-file will be calculated based
                     on the previous steps, i.e., resuming the stepping where it was
                     stopped.}
\qualifier{check}{[=0] Before each step saved steppar informations of other stepped
                        parameters are gathered and checked for a parameter set with a
                        better chi2 (using steppar_get_bestparams). In case a better
                        parameter set was found the stepping will be restarted.
                        * ATTENTION: 'save' qualifier is required !!!
                        * Gathered are steppar-files, which match the pattern
                          given with 'save',e.g., if save is "steppar_PID001.fits"
                          all files "steppar_PID???.fits" are globed!
                          Note that the affix "_PID???" is automaticcaly appended to
                          the save string if it does not exist (see 'save' qualifier!)
                        * The Integer 'check' is set to is the maximal number of
                          restarts.
                        * A parameter set is considered better if
                          chi2_new < chi2_init * ( 1 - dchi2 )
                        * NOTE: Parameter grouping is possible (see steppar_get_bestparams)
                        }
\qualifier{dchi2}{[=0.1] Fractional limit for the chi2 of a new parameter set to be
                  considered as a better parameter set (see 'check'):
                  chi2_new < chi2_init * ( 1 - dchi2 ).}
\qualifier{save}{After each stepping the result is saved in the file given
                   (as String_Type) with this qualifier. The chi2 of undone steps
                   are set to 0.! Also note that the filename is appended with
                   the parameterindex: 'steppar.fits' -> 'steppar_PID001.fits'}
\qualifier{force}{Forces to overwrite an existing file given with 'save'.}
\qualifier{chatty}{Prints fitting information.}
}
\description
      This function is calling 'steppar' with an adjusted fitting function:
      fit = &_simultaneous_fit_steppar_fit which first calls either %.fit_groups
      or %.fit_global depending on the affiliation of the given parameter.
      Afterwards %.fit_smart is called (with the passed qualifiers).
      For more detailed information see the help of 'steppar'!

      Qualifiers are also passed to 'steppar' & '%.fit_smart'!

\seealso{%.fit_smart, steppar}
\done

\function{simultaneous_fit.thaw}
\synopsis{thawes a/many parameter/parameters}
\usage{simultaneous_fit.thaw(String_Type parameter);}
\description
    In princible, this function does the same as 'thaw'
    with the modifcation that place holders are allowed:
      % - set the function for all defined data
    Note, that here the parameter name has to be provided,
    not the parameter id!
\seealso{thaw}
\done

\function{simultaneous_fit.unset_global}
\synopsis{A specific group parameter is fitted individually again}
\usage{simultaneous_fit.unset_global(String_Type grouppar);}
\description
    A wrapper around simultaneous_fit.set_par_fun, which
    simply unties the given group parameter name (using the
    %-placeholder instead of the dataset index), which was
    tied to a single global parameter before.
    If successful, a warning message will be shown.
\example
    % use a photon index for each data group
    simultaneous_fit.unset_global("powerlaw(%).PhoIndex");
\seealso{simultaneous_fit.set_global,simultaneous_fit.set_par_fun}
\done

\function{sinwave2 (fit-function)}
\description
    a*sin(b*alpha+c)+d
    (ISIS fit-function sinwave seems to be defined for keV space:
    sinwave({a,b,c,d};alpha) = sinwave2({a,b,c,d};_A(alpha)))
\done

\function{SIprefix}
\usage{Double_Type SIprefix(String_Type prefix)}
\done

\function{sitar_avg_cpd}
\synopsis{Create an averaged cross power spectral density.}
\usage{(f,psd_a,psd_b,cpd,navg,avg_a,avg_b,) = sitar_avg_cpd(cnts_a,cnts_b,l);}
\qualifiers{
\qualifier{dt}{[Length of evenly spaced bins. Default == 1.]}
\qualifier{times}{[Times of measurements (otherwise presumed to have no gaps).]}
\qualifier{norm}{[Determine PSD normalization convention. =1, 'Leahy normalization'. Poisson noise PSD == 2, in absence of deadtime effects. !=1, rms or Belloni-Hasinger or Miyamoto normalization, i.e., PSD == (rms)^2/Hz & Poisson noise== 2/Rate.]}
}
\description
     Take an evenly spaced lightcurve (presumed counts vs. time), and
     calculate the PSD and CPD in segments of length l, averaged over
     the whole lightcurve.  Segments with data gaps are skipped.
     Deadtime corrections to Poisson noise are *not* made. Poisson
     noise is *not* subtracted from average PSDs.

   Inputs:
     cnts_a/b: Arrays of total counts in each time bin
     l       : Length of individual PSD segments (use a power of 2!!!)

   Outputs:
     f       : PSD frequencies ( == 1/Input Time Unit)
     psd_a/b : Average PSDs 
     cpd     : Normalized Cross Power Spectral Density (complex array)
     navg    : Number of data segments going into the averages
     avg_a/b : Average number of counts per segment of length l
\seealso{sitar_avg_psd, sitar_lbin_cpd, sitar_lbin_psd, sitar_define_psd, sitar_lags}
\done

\function{sitar_avg_psd}
\synopsis{Create an averaged power spectral density.}
\usage{(f,psd,navg,avg_cnts [,psd_err]) = sitar_avg_psd(cnts,l);}
\qualifiers{
\qualifier{dt}{[Length of evenly spaced bins. Default == 1.]}
\qualifier{times}{[Times of measurements (otherwise presumed to have no gaps).]}
\qualifier{norm}{[Determine PSD normalization convention. =1, Leahy normalization. Poisson noise PSD == 2, in absence of deadtime effects. !=1, rms or Belloni-Hasinger or Miyamoto normalization, i.e., PSD == (rms)^2/Hz & Poisson noise== 2/Rate.]}
\qualifier{err}{[If it exists, also return the PSD error calculated directly by assuming each sample has 100% error.]}
}
\description
     Take an evenly spaced lightcurve (presumed counts vs. time), and
     calculate the PSD in segments of length l, averaged over the
     whole lightcurve.  Segments with data gaps are skipped.  Deadtime
     corrections to Poisson noise are *not* made. Poisson noise is
     *not* subtracted from average PSD.

   Inputs:
     cnts    : Array of total counts in each time bin
     l       : Length of individual PSD segments (use a power of 2!!!)

   Outputs:
     f       : PSD frequencies ( == 1/Input Time Unit)
     psd     : Average PSD.
     navg    : Number of data segments going into the average
     avg_cnts: Average number of counts per segment of length l

   Optional Output:
     psd_err : Estimated error on the PSD, calculated from lightcurve sample
\seealso{sitar_avg_cpd, sitar_lbin_cpd, sitar_lbin_psd, sitar_define_psd, sitar_lags}
\done

\function{sitar_bin_events}
\synopsis{Bin an event-based lightcurve.}
\usage{lc = sitar_bin_events(t,dt,gti_lo,gti_hi);}
\qualifiers{
\qualifier{tstart}{[Beginning of first output time bin. Default: min(t).]}
\qualifier{tstop}{[End of last output time bin. Default: max(t).]}
\qualifier{obin}{[For a tstop, last bin width might be < dt.  Include this overflow bin if width is > obin*dt. Default: obin=0.1.]}
\qualifier{user_bin.lo}{[Lower time bounds for user defined grid.]}
\qualifier{user_bin.hi}{[Upper time bounds for a user defined grid. Overrides dt, tstart, and tstop inputs.]}
\qualifier{minexp}{[Do not include bins with exposure < minexp. Default: 0.]}
\qualifier{delgap}{[If set, delete 0 exposure bins from the lightcurve, otherwise set their rate & error to 0.]}
}
\description
   Inputs:
     t          : Discrete times at which rates are measured
     dt         : Width of evenly spaced time bins in rebinned lightcurve
                  (Ignored if a user_bin is input.)

   Outputs:
     lc.rate    : Mean rate of resulting binning
     lc.err     : Error on bin rate (minimum = -log(0.32)/(bin exposure))
     lc.cts     : Counts in a bin
     lc.bin_lo  : Lower time bounds of binned lightcurve
     lc.bin_hi  : Upper time bounds of binned lightcurve
     lc.expos   : Exposure of a time bin (i.e., intersection with
                  good time intervals)
\seealso{sitar_rebin_rate}
\done

\function{sitar_define_psd}
\synopsis{Store a power spectral density as a fittable dataset.}
\usage{id = sitar_define_psd(flo,fhi,psd,err [,noise]);}
\qualifiers{
\qualifier{noise}{[Same as optional input, noise.]}
}
\description
   Inputs:
     f_lo   : Low value of PSD frequency bin (Hz -> keV)
     f_hi   : High value of PSD frequency bin (Hz -> keV)
     psd    : Array of PSD values (Power -> Counts/bin)
     err    : Array of PSD Errors

   Optional Input:
     noise  : Array (*or single value*) of Poisson noise levels.
              If undefined, background is undefined.

   Outputs:
     id     : Dataset Index
\seealso{sitar_avg_cpd, sitar_lbin_cpd, sitar_avg_psd, sitar_define_psd, sitar_lags}
\done

\function{sitar_global_optimum}
\synopsis{Loads a dataset with a non-unity AREASCAL keyword or column}
\usage{ans = sitar_global_optimum( cell, ncp_prior, type [; first, mean_prior, rate_prior, alpha=Double_Type, beta=Double_Type] );}
\description
   Inputs:
     cell      : A structure, with cell.pops, cell.size, cell.lo_t
                 cell.hi_t,cell.dtcor (see sitar_make_data_cells)
     ncp_prior : Parameter for prior on number of 'blocks' 
     type      : Identical to type from sitar_make_data_cells

   Qualifiers:
     first     : If set, the code runs in 'trigger' mode, and returns upon
                 the first sign of a change, so long as it is greater
                 than 'first' cells from the beginning of the
                 lightcurve

                EVENT MODE PRIORS (type=1 or 2)-

                 Default prior is that p_1, the probability of one or
                 more photons in a frame, is uniformly dsitributed
                 from 0->1.

     alpha     : >=0 implies p_1 prior is (1+alpha) (1-p_1)^alpha 

     mean_prior: if set, prior on *rate* is exp(-Lambda/Lambda_0),
                 where Lambda_0 is the mean rate from the entire
                 observation.  Supercedes any choice on alpha.

     rate_prior: if set, prior is chosen to be uniform between
     rate_low,   rates ranging from rate_low -- rate_high, with
     rate_high   defaults of rate_low = 1/3 mean rate and 
                 rate_high = 3 times mean rate. Supercedes any
                 choice of alpha or mean_prior.

                BINNED MODE PRIORS AND POSTERIORS (type=3)-

     alpha,    : The prior on the bin rate (sort of) goes as
     beta        (Lambda)^(alpha-1) Exp(-beta*Lambda), where
                 Lambda=rate, and the default is alpha=beta=1

     max_like  : Use maximum likelihood, log(Pmax) = N log(N/T) -N,
                 as the posterior test function.  Overrides any
                 choice of alpha & beta for binned mode.

   Outputs:
     results.cpt  : Array of change point locations for the maximum 
                    likelihood solution (indices are specific to the
                    cell input);
     results.last,: (Diagnostic purposes only) Arrays of the
            .best   location of the last change point and the
                    associated maximum log probability
     results.cts  : Counts in each block
     results.rate : Rate in each block
     results.err  : Poisson error for the block rate
     results.lo_t,: Times of lower (>=) and upper (<) block
            .hi_t   boundaries.
\seealso{sitar_make_data_cells}
\done

\function{sitar_glvary}
\synopsis{Create an optimal lightcurve using the Gregory-Loredo algorithm.}
\usage{gl = sitar_glvary(t);}
\qualifiers{
\qualifier{tmin}{[The minimum time to consider. Default=min(t)-1]}
\qualifier{tmax}{[The maximum time to consider. Default=max(t)+1]}
\qualifier{mmax}{[Consider lightcurve divisions from 2 to mmax evenly spaced bins. Default=300.]}
\qualifier{thresh }{[Truncate the number of partitions by ignoring those for which \\\\sum_m(odds ratio)<max(\\\\sum_n(odds ratio))/exp(thresh), where n = 2 -> mmax.  thresh sets a minimum probability: (1-p_min) ~ exp(thresh)*(1-p_peak). Default=2.]}
\qualifier{nbins}{[Create an output lightcurve with nbins. Default=mmax.]}
\qualifier{texp, frac_exp}{[Array pair that give for fractional exposure as a function of time, which will be interpolated and used to correct lightcurve rates. Arrays must have a minimum of five entries. Default is for no correction.]}
}
\description
     Use the Gregory-Loredo algorithm to find the odds ratios that
     even divisions of a lightcurve are better descriptions of the
     data than a constant lightcurve.  A 'best estimate' lightcurve
     can also be output for a lightcurve with nbins.

   Inputs:
     t      : Array of event times

   Outputs:
     gl.p         : Total probability that some evenly partitioned lightcurve, with up
                    to gl.mmax bins, is a better description than a constant lightcurve
     gl.ppart     : The probability for an individual evenly partitioned lightcurve
                    that it is a better description than a constant lightcurve
     gl.lodds_sum : The natural logarithm of the sum of the odds ratios comparing 
                    lightcurves with two or more partitions to a constant lightcurve.
                    gl.p == exp(gl.lodds_sum)/[1+exp(gl.lodds_sum)]
     gl.mpeak     : The number of partitions for the evenly partitioned lightcurve 
                    with the maximum probability.
     gl.mmax      : The maximum number of partitions actually used (influenced by
                    the setting of the thresh parameter)
     gl.m         : The number of partitions corresponding to each evenly partitioned
                    lightcurve considered (=[2:mmax])
     gl.pm        : Total probability that some evenly partitioned lightcurve
                    is a better description than a constant lightcurve for each
                    maximum number of partitions considered ([2:mmax])
     gl.nj        : The counts histogram corresponding to each partitioning above
     gl.aj        : The integrated fractional exposure for each partitioning above
     gl.a_avg     : The averaged fractional exposure.
     gl.tmin      : The value of tmin actually used (maximum of [tmin,min(texp)]);
     gl.tmax      : The value of tmax actually used (minimum of [tmax,max(texp)]);
     gl.tlc       : The output lightcurve times (an array with input nbins bins)
     gl.rate      : Best estimate of the lightcuve rates at the above times
     gl.erate     : Best estimate of the lightcuve rate errors at the above times
\seealso{sitar_global_optimum}
\done

\function{sitar_lags}
\synopsis{Create time lags and coherence function from input power spectra.}
\usage{(lag,dlag,g,dg) = sitar_lags(freq,psda,psdb,cpd,noisea,noiseb [,navg]);}
\description
     Given two input PSD (without noise subtracted), their CPD, and
     their associated noise levels, all as functions of Fourier
     frequency, calculate the time lag and coherence function (and
     associated errors) vs. f

   Inputs:
     freq    : Fourier frequency array
     psda/b  : Power spectra array associated with two lightcurves
               (Noise subtraction *not* applied!)
     cpd     : Cross power spectra array associated with two lightcurves
     noisea/b: Power spectra noise level. Array *or* a constant.

   Optional Unput:
     navg    : Number of averages (over independent data segments and frequency
               bins) associated with each input frequency bin. Can be a constant
               vs. f (defaulted to 1).

   Outputs:
     lag     : Time lag vs. frequency. Negative values mean psdb lags psda
     dlag    : Associated error on the lag.
     g       : Coherence function
     dg      : Error on the coherence function
\seealso{sitar_avg_cpd, sitar_lbin_cpd, sitar_avg_psd, sitar_lbin_psd, sitar_define_psd}
\done

\function{sitar_lbin_cpd}
\synopsis{Logarithmically rebin a cross power spectral density.}
\usage{(aflo,afhi,apsda,apsdb,acpd,nf,[anoisea,anoiseb]) =  sitar_lbin_cpd(f,psda,psdb,cpd,dff,[noisea,noiseb,&rev]);}
\description
     Logarithmically rebin two PSDs (and, optionally, their associated
     Poisson noise levels) and the associated Cross Power Spectral
     Density

   Inputs:
     f        : Array of Fourier frequencies
     psda/b   : Arrays of PSD values
     dff      : Delta f/f value for logarithmic bin spacing

   Optional Inputs:
     noisea/b : Arrays (*or single values*) of Poisson noise levels
     rev      : If declared (i.e., 'isis> variable rev;') and input,
                rev returns the reverse indices for the binning (i.e.,
                (apsda[i] = mean( psda[ rev[i] ] ), etc.)

   Outputs:
     aflo     : Lower boundary of Fourier frequency bin
     afhi     : Upper boundary of Fourier frequency bin
     apsda/b  : Rebinned Power Spectral Density values
     acpd     : Rebinned Cross Power Spectral Density values
     nf       : Number of frequencies going into the bin
     anoisea/b: Rebinned noise level (array, even for single value input)
\seealso{sitar_avg_cpd, sitar_avg_psd, sitar_lbin_psd, sitar_define_psd, sitar_lags}
\done

\function{sitar_lbin_psd}
\synopsis{Logarithmically rebin a power spectral density.}
\usage{(aflo,afhi,apsd,nf [,anoise,apsd_err]) = sitar_lbin_psd(f,psd,dff);}
\qualifiers{
\qualifier{noise}{[Array (*or single value*) of Poisson noise levels.]}
\qualifier{rev}{[If declared (i.e., 'isis> variable rev;') and input, rev returns the reverse indices for the binning (i.e., af[i] = mean( freq[ rev[i] ] ), etc.).]}
\qualifier{psd_err}{[Array of errors for the PSD, in which case a PSD error array will be returned.]}
}
\description
     Logarithmically rebin a PSD (and, optionally, its associated
Poisson noise and error values)

   Inputs:
     f      : Array of Fourier frequencies
     psd    : Array of PSD values
     dff    : Delta f/f value for logarithmic bin spacing 

   Outputs:
     aflo   : Lower bounds of Fourier frequency bins
     afhi   : Upper bounds of Fourier frequency bins
     apsd   : Rebinned PSD values
     nf     : Number of frequencies going into the bin

   Optional Outputs:
     anoise  : Rebinned noise level (array, even for single value input)
     apsd_err: Rebinned PSD error array
\seealso{sitar_avg_cpd, sitar_lbin_cpd, sitar_avg_psd, sitar_define_psd, sitar_lags}
\done

\function{sitar_make_data_cells}
\synopsis{Loads a dataset with a non-unity AREASCAL keyword or column}
\usage{cell = sitar_make_data_cells(tt,type,max_delt,frame,tstart,tstop);}
\description
    Create data cells from event data to be used with the Bayesian
    blocks algorithm. tt are the times of the events; type=1,2, or 3
    determines cell breaks (midway between events; right before
    subsequent event, or binned); events closer than max_delt are
    grouped together in a single cell; output cell sizes are in units
    of frame (or = bin size for type=3), tstart/tstop are the
    stop/start times for the output cells.

    Output is a structure with cell.pops (number of events per cell),
    cell.size (duration of cell in units for frame), cell.lo/hi_t
    (start/stop times of cell), cell.dtcor (array, currently set to
    unity, for storing cell deadtime corrections).
\seealso{sitar_global_optimum}
\done

\function{sitar_pfold_event}
\synopsis{Fold an event-based lightcurve on a given period (with derivatives).}
\usage{profile  = sitar_pfold_event(t, p, gti_lo, gti_hi);}
\qualifiers{
\qualifier{pdot}{[Period derivative.]}
\qualifier{pddot}{[Period second derivative.]}
\qualifier{nphs}{[Number of phase bins in output folded lightcurve.]}
\qualifier{tzero}{[Time of zero phase. Default = t[0].]}
\qualifier{phase_lo}{[Arrays for phase bin boundaries (e.g., for uneven phase bins).]}
\qualifier{phase_hi}{[Arrays must be same length with matched boundaries. Supercedes nphs.]}
}
\description
   Inputs:
     t             : Times at which rates are measured
     rate          : Lightcurve rate
     p             : Period on which to fold the lightcurve
     gti_lo        : Array of lower boundaries of good time intervals
     gti_hi        : Array of upper boundaries of good time intervals

   Outputs:
     profile.bin_lo: Start value of phase bin
     profile.bin_hi: Stop value of phase bin
     profile.cts   : Counts in phase bin
     profile.var   : Square root of counts in phase bin
     profile.expos : Integrated exposure in phase bin
\seealso{sitar_pfold_rate, sitar_epfold_rate}
\done

\function{sitar_pfold_rate}
\synopsis{Fold a rate-based lightcurve on a given period (with derivatives).}
\usage{profile  = sitar_pfold_rate(t,rate,p);}
\qualifiers{
\qualifier{pdot}{[Period derivative.]}
\qualifier{pddot}{[Period second derivative.]}
\qualifier{nphs}{[Number of phase bins in output folded lightcurve.]}
\qualifier{tzero}{[Time of zero phase. Default = t[0].]}
\qualifier{phase_lo}{[Arrays for phase bin boundaries (e.g., for uneven phase bins).]}
\qualifier{phase_hi}{[Arrays must be same length with matched boundaries. Supercedes nphs.]}
}
\description
   Inputs:
     t             : Times at which rates are measured
     rate          : Lightcurve rate
     p             : Period on which to fold the lightcurve

   Outputs:
     profile.bin_lo: Start value of phase bin
     profile.bin_hi: Stop value of phase bin
     profile.mean  : Mean rate in phase bin
     profile.var   : Variance of rate in phase bin
     profile.sdm   : Standard deviation of mean rate in a phase bin
     profile.num   : Number of data points in phase bin
\seealso{sitar_pfold_event, sitar_epfold_rate}
\done

\function{sitar_readasm}
\synopsis{Read and RXTE ASM data file}
\usage{event = sitar_readasm( file );}
\qualifiers{
\qualifier{tstart}{[Start time to be read; units of MET or MJD]}
\qualifier{tstop}{[Stop time to be read; units of MET or MJD]}
\qualifier{maxchi2}{[Maximum acceptable chi2 for ASM solution]}
\qualifier{chnl}{[0 for total band; !=0 for three ASM colors+total]}
\qualifier{mjd}{[Changes from default of Mission Elapsed Time (days) to MJD]}
\qualifier{jd}{[Changes from default of Mission Elapsed Time (days) to JD]}
}
\description
   Inputs:
     file      : ASM filename, e.g. 'xa_cygx1_d1.lc' or 'xa_cygx1_d1.col'

   Outputs: 
     event.time: Time of each event
     event.rate: Total ASM count-rate
     event.err : Uncertainty in count rate
     event.chi2: Chi2 of ASM solution
    
   Optional Outputs:
     event.[ch1,ch2,ch3]_rate: ASM count rate in each channel
     event.[ch1,ch2,ch3]_err : ASM count rate error in each channel
     event.[ch1,ch2,ch3]_chi2: Chi2 for ASM solution in each channel
                               (Overrides event.chi2, which won't be output)
\done

\function{sitar_rebin_rate}
\synopsis{Rebin a rate lightcurve}
\usage{lc = sitar_rebin_rate(t,dt,rate [,err]);}
\qualifiers{
\qualifier{tstart}{[Beginning of first output time bin]}
\qualifier{tstop}{[End of last output time bin]}
\qualifier{minbin}{[Minimum required events per bin, else the bin is set to 0,  or ignored. Default=1.]}
\qualifier{user_bin.lo}{[Lower time bounds for user defined grid]}
\qualifier{user_bin.hi}{[Upper time bounds for a user defined grid. Overrides  dt, tstart, and tstop inputs]}
\qualifier{delgap}{[If set, delete empty bins from the lightcurve, otherwise,  set them to 0]}
\qualifier{weight}{[If set and err input, the mean is weighted by the error.  I.e., mean = (\\\\sum rate/err^2)/(\\\\sum 1/err^2), and the output variance becomes (\\\\sum (mean-rate/err^2)^2)/(\\\\sum 1/err^2)^2]}
}
\description
     Variables in [] are optional, but are order specific unless a qualifier.
    
   Inputs:
     t          : Discrete times at which rates are measured
     dt         : Width of evenly spaced time bins in rebinned lightcurve
                  (Ignored if a user_bin is input.)
     rate       : (Presumed GTI & Exposure Corrected) rates

   Optional Input:
     err        : Error on the rates

   Outputs:
     lc.rate    : Mean rate of resulting rebinning
     lc.bin_lo  : Lower time bounds of rebinned lightcurve
     lc.bin_hi  : Upper time bounds of rebinned lightcurve
     lc.num     : Number of events going into a bin
     lc.var     : Variance of the rate in a time bin. (Only calculated
                  where lc.num >= 2, otherwise set to zero.) 
    
   Optional Outputs:
     lc.err     : Rate errors combined in quadrature, i.e., it's 
                  sqrt{\\\\sum{ err^2 }}/N, where N is the number of points
                  in that particular bin (i.e., lc.num).  Only computed if 
                  err is input [otherwise, just use sqrt(lc.var)].
\seealso{sitar_bin_events}
\done

\function{sixte_create_img_wfi}
\synopsis{returns cmd to create an Image from an EventFile for the XIFU Baseline config }
\usage{string cmd = sixte_create_img_wfi(double ra, double dec, string event_file, string image_file);}
\qualifiers{
\qualifier{mode}{["large"]: change mode of the WFI}
}
\done

\function{sixte_create_img_xifu}
\synopsis{returns cmd to create an Image from an EventFile for the XIFU Baseline config }
\usage{String cmd = sixte_create_img_xifu(double ra, double dec, string event_file, string image_file);}
\done

\function{sixte_psfgen(hew, npix, resolution_meter, focallength, filename)}
\synopsis{simple wrapper to Sixte-Tool "psfgen"}
\description
   hew: HEW of PSF [arcsec]
   npix: Number of pixels in X and Y direction
   resolution_meter: width of one pixel in the focal plane [meter]
   focallength: focal length of telescope [meter]
   filename: name of output file
   
   Note: this functions requires Sixte to be installed!
\done

\function{sltable}
\synopsis{Generate a LaTeX table}
\usage{String_Type table = sltable(par1[,par2,par3,...])}
\description
   sltable() is intended to streamline the production of tables of,
   e.g., spectral parameters with error bars, although it should be flexible
   enough to produce most simply-structured tables. There is a set of examples
   on the Remeis wiki:
   http://www.sternwarte.uni-erlangen.de/wiki/doku.php?id=isis:sltable

   By default this builds tables using the "deluxetable" package; this can be
   disabled to produce standard "tabular" tables by setting the "deluxe"
   qualifier to zero. The deluxetable environment has been tested using the
   latest aastex61.cls stylefile. Tabular tables will require the "booktabs"
   package, as they provide the toprule, midrule, and
   bottomrule LaXeX macros used here.

   Each argument given to sltable() provides the values of one
   column of the table (or one row, if the "horiz" qualifier is nonzero).
   The length of each argument (or the length of the "value" field of a
   Struct_Type argument) defines the number of rows (or columns, if horiz=1).
   The arguments can be String_Type (in which case they are printed
   literally), Double_Type (or some other numerical type), in which case they
   are printed to three decimal places (TODO: figure out a better, more
   flexible way of doing this), or Struct_Type, as defined below.

   Struct_Type arguments should at least have a "value" field, which should be
   an array of some type. If only the "value" field is present, the value
   printed to the table follows the rules for String_Type and Double_Type
   arguments above. Further fields which can be provided are:
     min: the minimum values of this parameter
     max: the maximum values of this parameter
     freeze: if 1, the parameter is frozen, if 0, it is thawed
     limit: if 1, this can be an upper/lower limit
     nodata: if 1, there is no data here and "..." should be printed
   These should all be arrays of the same length as the "value" field, and
   allow for the printing of error bars and indicating upper/lower limits and
   frozen parameters.

   If the min and max fields are provided and the parameter is not frozen and
   not an upper or lower limit, the confidence intervals are printed using the
   TeX_value_pm_error() function. If it is an upper or lower limit, it is
   rounded to three decimal places and indicated accordingly with "<" or ">"
   symbols. If frozen, it is rounded to three decimal places and surrounded by
   parentheses (these delimiters can be customized by changing the
   "frozendelim" qualifier).

   If the "nodata" field is nonzero, then a "no data" indicator will be
   printed. By default, for a deluxetable, this is "\\\\nodata" and for a
   tabular table it is "\\\\ldots". This is useful if you are putting multiple
   models which do not share the same parameters into the same table.

   This is a lot of words, so here is an
   EXAMPLE
   This will produce a simple 3x3 table with values with error bars, some
   frozen parameters, some upper limits, and some cells in the table filled
   with literal strings.
   \code{
   variable par1 = struct{value=[1,2,3],min=[0.9,1.9,0.0],max=[1.1,2.3,3.2],
     freeze=[0,1,0],limit=[1,1,1]};
   variable par2 = struct{value=[1,2,3],min=[0.9,1.9,2.9],max=[1.1,2.3,3.2],
     freeze=[1,0,0],limit=[1,1,1]};
   variable par3 = struct{value=["foo","bar","baz"]};
   variable parnames = ["par1","par2","par3"];
   variable parunits = ["unit1","unit2","unit3"];
   variable obsNames = ["obs1","obs2","obs3"];
   variable notes = ["note1","note2"];
   variable noteSym = ["1","2"];
   variable t = sltable(par1,par2,par3;
     colnames={parnames,parunits},rownames=obsNames,
     notes={notes,noteSym},label="tab:example",
     caption="this is the table's caption");
   }
   The variable "t" now contains the full LaTeX table - drop it into a LaTeX
   document and see what it ends up looking like. Note the use of the
   "colnames" and "rownames" qualifiers to define the names and units for the
   columns and rows, and how List_Type and Array_Type values are interpreted
   differently there. Footnotes are also possible via the "notes" qualifier
   (although currently we can't place the footnote markers into the table
   automatically - you'll have to handle that yourself).

   This script is under development. Contact Paul Hemphill (pbh@space.mit.edu)
   if you find any bugs.

\qualifiers{
\qualifier{deluxe}{Integer_Type. If nonzero, we'll produce a deluxetable.
        Otherwise, tabular. Tabular tables aren't as fancy and aren't
        implemented as well at the moment. In theory we could add other table
        types. Default 1.}
\qualifier{horiz}{Integer_Type. If nonzero, arguments to sltable
        indicate _rows_ of the table. If 0, they indicate _columns_.}
\qualifier{rownames}{String_Type[] or List_Type[]. Names for the rows
        of the table. Multiple columns for the row names (e.g., parameter names
        and units in separate columns) can be handled by providing a List_Type
        containing multiple String_Type arrays.}
\qualifier{colnames}{String_Type[] or List_Type[]. Names for the columns of
        the table (i.e., the column headers). Similar to rownames above,
        multi-line headers can be produced by passing a List_Type to this
        qualifier.} 
\qualifier{caption}{String_Type. LaTeX caption for the table. Default is
        blank. Don't include a label statement here - that's handled by the 
        "label" qualifier below.}
\qualifier{label}{String_Type. LaTeX label for the table. Default is
        "placeholder," so make sure to set this.}
\qualifier{nodata}{String_Type, set to what should be printed if there is no
         data for a particular field (e.g., if you are putting multiple models
         into the same table and they do not share the same parameters).}
\qualifier{frozendelim}{String_Type[], two-element string array containing
        opening and closing delimiters for a frozen value. By default this is
        ["(",")"], i.e., the frozen value is surrounded by parentheses.}
\qualifier{star}{Integer_Type. If nonzero, the table will be a table* or
        deluxetable*. If 0, the table will be a table or deluxetable.}
\qualifier{tabletypesize}{String_Type. Sets the font size of the table.
        Should be a LaTeX font size without the backslash. Default is
        "footnotesize".}
\qualifier{longtable}{Integer_Type. If nonzero (and we're making a
        deluxetable), this is a long table, so a startlongtable statement will
        be included in the table header. I don't think tabular tables do
        anything with this right now. Default 0.}
\qualifier{landscape}{Integer_Type. If nonzero, this is a
        landscape-orientation table. Includes a \\\\rotate command if
        deluxetable, or wraps everything in a begin/end landscape block if
        tabular. Default 0.}
\qualifier{notes}{List_Type[2]. Element 0 should be a String_Type[] array of
        the text of any footnotes; element 1 should be a String_Type[] array of
        the symbols associated with those notes.}
}

\seealso{TeX_value_pm_error}
\done

\function{sltableBuildBody}
\synopsis{Build a 2D array of table cells}
\usage{String_Type table[] = sltableBuildBody(Array_Type par1[, Array_Type par2,...])}
\qualifiers{
\qualifier{horiz}{Integer_Type. If zero, each argument defines one column of
           the table. If nonzero, each argument defines one row. Default 0.}
\qualifier{frozendelim}{String_Type[2]. Defines strings to use before and
           after a frozen parameter value. Default ["(",")"].}
\qualifier{nodata}{String_Type. Defines string to use when no data is
           present in a cell. Default is "\\\\nodata" for deluxetables and
           "\\\\ldots" for tabular tables.}
\qualifier{rownames}{String_Type[] or List_Type. Names for rows of table.
         If given as a List_Type of String_Type arrays, each element of the
         list will be included in its own column (this is useful for, e.g.,
         putting the parameter name and the units in separate columns).}
}

\description
   Builds the "body" of a table (in the form of a 2D array of strings,
   containing the fields of the table).

   Arguments should be either 1D arrays or structs, the same as one would give
   to sltable(). An array of strings will be used as-is. Arrays of numbers
   will be rounded to 3 significant figures.  Structs allow the handling of
   error bars and offer considerably more flexibility, as defined below:

   Struct arguments should, at minimum, have a "value" field, which must be
   an array containing the value of the parameter for each row (or column, if
   the table is horizontally-aligned). Further fields can be used to modify
   the output. All should be arrays of the same length as the value field,
   and they have the following names and uses:

   min: minimum value of the parameter.
   max: maximum value of the parameter. Min & max determine rounding & errors.
   freeze: if 1, parameter is frozen, if 0, parameter is thawed.
   limit: if 0, parameter can be zero without being an upper/lower limit.

   Note that most of these are the same as those in the struct returned by
   get_par_info(). The exception is the "limit" field, which determines how a
   value of 0 for the parameter is handled. If "limit" is 1, then a confidence
   interval containing zero is interpreted as an upper or lower limit. If
   "limit" is 0, the value and error bars are typeset as a standard confidence
   interval. E.g., one would set limit to 1 for a power-law normalization and
   to 0 for a power-law index (the system is not flexible enough at the moment
   to handle pathological cases like upper/lower limits on logarithmic
   parameters like photon indices).

   The number of "rows" in the table is determined by the length of the first
   array or the first struct's value field. All later arrays and struct fields
   should have the same number of elements.

\seealso{sltable}
\done

\function{socket_send_object, socket_get_object}
\synopsis{send or get an SLang object to or from another machine via a TCP socket}
\usage{         socket_send_object("[username@]to_host", Any_Type object);
    Any_Type socket_get_object("[username@]from_host");}
\description
    Established a connection to another machine in order to send or
    receive an SLang object. Of course, this requires user input
    from an active ISIS environment on the host. See the help of
    `pack_obj' for the supported object formats.

    socket_send_object:
      A TCP server is started and as soon as a client connects the
      object is sent. The server is shut down afterwards.
    socket_get_object:
      A TCP client tries to connect to the host in order to receive
      the object. The connection is closed after the object has been
      received.

    For security reasons, it is recommended to provide the username
    expected on the host. In this case the server and the client
    exchange and check the corresponding username before the object
    is transferred. If the username does not match the expected one
    the connection is not trusted and closed automatically.

    It might occur that machine A which is about to send an object
    is behind a router/firewall or does not has a public internet IP
    address, while the retrieving machine B does. Using the qualifier
    `re' machine B starts a server for retrieving the object and
    machine A a client connection for sending purposes (i.e., the
    connection is established with server/client inverted with
    respect to the default behavior).
\example
    % receive an object from user "falkner" on "indus"
    obj = socket_get_object("falkner@indus");
    
    % send an array of doubles to "ara" without a greet message
    socket_send_object("ara", Double_Type[10000]);

    % send a structure `s' to an outside machine, while the own
    % machine "laptop" is behind a router
    socket_send_object("volans.sternwarte.uni-erlangen.de", s; re);
    % the call on "volans" then would be
    obj = socket_get_object("laptop"; re); % starts a server
\seealso{tcp_server, tcp_client}
\done

\function{solarAbund}
\synopsis{returns the solar abundance of an element}
\usage{Double_Type solarAbund(Integer_Type Z)}
\description
    \code{Z} is the element's proton number.
\seealso{Grevesse et al., 1996: Standard Abundances}
\done

\function{solveODEbyIntegrate}
\synopsis{solves an ordinary differential equation on a grid by integration}
\usage{x = solveODEbyIntegrate(&f, tgrid);}
\qualifiers{
    \qualifier{rdt}{integration step size relative to 'tgrid' binsize (default: 0.1)}
    \qualifier{x0}{integration constant (default: 0)}
    \qualifier{t0}{time reference for x0 (default: first 'tgrid' bin)}
    \qualifier{method}{integratipn method (default: &integrateRK4)}
}
\description
    The integral of the ordinary differential equation (ODE)\n
      dx/dt = f(x(t), t)  with  x(t0) = x0\n
    reads\n
      x(t) = x0 + int_{t0}^{t} f(x(t'), t') dt'

    \code{&f} is a reference to a function with two arguments:
       \code{define f(x, t)}\n
       \code{\{}\n
       \code{  return ...;}\n
       \code{\}}
\seealso{integrateRK4}
\done

\function{solve_2d_system_of_equations}
\usage{(x, y) = solve_2d_system_of_equations(ax1, ay1, c1, ax2, ay2, c2)}
\description
    \code{ax1 * x  +  ay1 * y  =  c1}\n
    \code{ax2 * x  +  ay2 * y  =  c2}\n
\done

\function{solve_Eddington_quartic_equation}
\synopsis{Solve the Eddington quartic equation}
\usage{Double_Type beta = solve_Eddington_quartic_equation(Double_Type M, mu)}
\description
    For a given mass 'M' (in solar masses) and mean molecular
    weight 'mu', solve the Eddington quartic equation
      1-beta = 0.003*M^2*mu^4*beta^4
    numerically for 'beta' by using Newton's method to find
    the root of a function.
\notes
    According to Equation (2-15) in the book "Principles of
    Stellar Evolution and Nucleosynthesis" by D.D. Clayton,
    the mean molecular weight can be approximated by
      mu ~ 2/(1+3*X+0.5*Y)
    where 'X' and 'Y' are the mass fractions of hydrogen
    and helium.

    The Eddington quartic equation needs to be solved to
    compute the polytropic constant of the polytropic standard
    model, see the function 'polytropic_standard_model'.
\example
    X = 0.73; Y = 0.26; mu = 2/(1+3*X+0.5*Y);
    beta = solve_Eddington_quartic_equation(1, mu);
\seealso{polytropic_standard_model}
\done

\function{solve_Lane_Emden_equation}
\synopsis{Solve the Lane-Emden equation numerically}
\usage{Struct_Type s = solve_Lane_Emden_equation(Double_Type n)}
\description
    The Lane-Emden equation for index 0<'n'<5 is a second order
    differential equation of the form

         1   d (     dtheta)
      ---- --- (xi^2 ------) = -theta^n
      xi^2 dxi (        dxi)

    with the boundary conditions

                 dtheta
      theta = 1, ------ = 0 at xi = 0 .
                    dxi

    The solution 'theta' satisfying these boundary conditions
    is called Lane-Emden function of index 'n' and is returned
    by this function together with its derivative dtheta.

    The Lane-Emden functions are used to study the structures
    of so-called polytropes, that is, gaseous spheres in hydro-
    static equilibrium which obey a polytropic equation of state:
      pressure = constant times density^((n+1)/n).
    For details, see, e.g., Chapter 2.4 in the book "Principles
    of Stellar Evolution and Nucleosynthesis" by D.D. Clayton.
    The output structure contains also the polytropic constants
      Rn = first zero of the Lane-Emden function of index n,

                 dtheta
      Mn = -xi^2 ------ evaluated at Rn.
                    dxi

            ( 3 dtheta)^(-1)
      Dn = -(-- ------)      evaluated at Rn,
            (xi    dxi)

           Rn^((n-3)/n)(3Dn)^((3-n)/3n)
      Bn = ---------------------------- .
                (n+1)Mn^((n-1)/n)
\notes
    The numerical solution of the Lane-Emden equation is achieved
    here by rewritting it as a system of first order differential
    equations
           d
         --- [y_0,y_1] = [y_1, -(y_0)^n - 2/xi y_1 ]
         dxi
    which then allows an adaptive Runge-Kutta method of 4./5. order
    to be applied.
\qualifiers{
\qualifier{tol}{[=1e-10] Absolute error control tolerance. Lower limit is 1e-15.}
}
\example
    s = solve_Lane_Emden_equation(3);
    print(s);
\seealso{polytropic_standard_model}
\done

\function{sort_arrays}
\synopsis{}
\usage{Integer_Type[] sort_arrays( Array_Type[] A1 [, A2 [, A3 ...]] );}
\qualifiers{
\qualifier{dir [=1]:}{1: Ascending sorting; -1: Descending sorting}
\qualifier{method [='msort']:}{ Choose sorting method, 'msort' for merge-
                       and 'qsort' for quick-sort.}
}
\description
   Similar to 'array_sort' this funtion sorts arrays, but also sorts
   multiple appearing array entries based on an addition given array
   and so on. In other words this function allows to sort several arrays
   (e.g., columns in a FITS-file) depending on the sorting of the previous
   array.

   This is especially useful if the arrays represent parameter combinations
   and you want to perform a resorting. See example below.

   NOTE THAT all arrays must have the same length!
   
\example
   % Example 1:
    a1 = [ 5,6,1,7,5,6,7 ];
    a2 = [ 8,9,4,4,1,2,3 ];
    i12= sort_arrays(a1,a2);
    array_map( Void_Type, &vmessage, "%.2d %.2d", a1[i12], a2[i12] );
    i21 = sort_arrays(a2,a1);
    array_map( Void_Type, &vmessage, "%.2d %.2d", a1[i21], a2[i21] );

   % Example 2: resorting parameter combinations:
    par = struct{ a = [0:1:#3],
                  b = ["hello","world"],
                  c = [ 2001, 2012]
                };
    parcomb = get_par_combinations( par );
    print(parcomb);

    parcomb =  struct_array_2_struct_of_arrays(parcomb);
    struct_filter( parcomb, sort_arrays(parcomb.b,parcomb.a,parcomb.c) );
    print(struct_of_arrays_2_struct_array(parcomb));
    
\seealso{array_sort, get_par_combinations}
\done

\function{sort_struct_arrays}
\usage{Struct_Type sort_struct_arrays(Struct_Type s, String_Type fieldname);}
\done

\function{sort_struct_fields}
\synopsis{sorts structure tags in a predefined order}
\usage{ Struct_Type newstruc = empty_struct(Struct_Type s, String_Type taglist[]);}
\description
 This function creates a new structure newstruc with the fiels given by the
 array taglist, and fills them with the values from structure s.

 The order in which the fields are added is given by taglist, fields not
 listed in taglist are ignored.

 The function is useful, e.g., when using fits_binary_table to create a FITS-file 
 from a structure. In this case the order in which the FITS columns are created
 corresponds to the order in which they are contained in the structure. One can then
 use this function to create a FITS-file with the desired order of columns.

\done

\function{sov}
\synopsis{Set pgplot outer viewport (isis_fancy_plots package)}
\usage{sov(Double_Type, Double_Type, Double_Type, Double_Type);}
\description

 sov(xmin,xmax,ymin,ymax);

   Equivalent to:
      isis> v=struct{xmin,xmax,ymin,ymax};
      isis> v.xmin=xmin;
      isis> v.xmax=xmax;
      isis> v.ymin=ymin;
      isis> v.ymax=ymax;
      isis> set_outer_viewport(v)
   and:
      isis> set_outer_viewport(struct{xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax});
\seealso{set_outer_viewport, apj_size, keynote_size, nice_width, open_print, close_print, pg_color, pg_info}
\done

\function{spearmanrho}
\synopsis{Timing Tools: Spearman's Rank Correlation Coefficient}
\usage{(rho) = spearmanrho (a,b);}
\description
 rho = 1 - frac {6 sum d_i^2} {n(n^2 -1)}

 d_i = a_i - b_i :  difference between the ranks of corresponding values
               n :  number of values

 Alternative: The statistics module (\code{require("stats");}) includes
 several functions for determining (rank) correlations coefficients,
 such as \code{spearman_r}, which provides the p-value (as well as the
 correlation coefficient \code{rho}).
\done

\function{sphere_urand}
\synopsis{generate spherical coordinates uniformly distributed on a sphere}
\usage{(Double_Type theta, phi) = sphere_urand();
\altusage{(Double_Type theta[], phi[]) = sphere_urand(Integer_Type n);}
}
\description
    The function generates \code{n} (default: \code{1}) pairs \code{(theta[i], phi[i])}
    of spherical coordinates -- such that the points
     \code{[ sin(theta)*cos(phi), sin(theta)*sin(phi), cos(theta) ]}
    are uniformly distributed on the surface of the unit sphere.
\done

\function{spherical_volume}
\synopsis{computes the volume of an object parameterized in spherical coordinates}
\usage{Double_Type vol = spherical_volume(Ref_Type &r);}
\qualifiers{
\qualifier{Ntheta}{[=180]}
\qualifier{Nphi}{[=180]}
\qualifier{logfile}{[=NULL]: If logfile is a filehandle, the function evaluations are logged.}
}
\description
    \code{r} has to be a real function with two arguments (theta, phi).\n
    \code{vol = int_0^2pi dphi  int_0^pi dtheta sin(theta)  int_0^r(theta, phi) dr r^2}\n
    \code{    = int_0^2pi dphi  int_0^pi dtheta sin(theta)  r(theta, phi)^3/3}\n
    The numerical integration of phi and theta is performed on the following grid:\n
    \code{theta = [0 : Pi : #Ntheta];   phi = [0 : 2*Pi : #Nphi];}
\done

\function{split_and_epfold_lc}
\synopsis{Takes lightcurve and pulse period, splits if necessary and executes epfold for each segment.}
\usage{Struct_Type split_and_epfold_lc(Struct_Type OR Array_Type lc, Double_Type p0), all times in the same units}
\qualifiers{
\qualifier{exact}{Forces execution of pfold with exact qualifier}
\qualifier{not_exact}{Forces execution of pfold without exact qualifier}
\qualifier{exact_threshold}{see description (default: 6.)}
\qualifier{split_only}{Only the splitting is performed}
\qualifier{nbins}{Sampling of pfold (default: min(32, p0 / dt), dt time resolution}
\qualifier{nsrch}{Number of trial periods for epfold (default: 1e4)}
\qualifier{grid_steps}{Period resolution for epfold (default: not set)}
\qualifier{search_range}{Period interval considered left/right of p0 in terms of dp=p0^2/T (default: 2.)}
\qualifier{dpmin}{For very long lightcurves, this can replace dp (default: p0e-3)}
\qualifier{gap_scale}{the smaller, the higher is the sensitivity for gaps (default: .5)}
\qualifier{fracexp}{Only timebins with fracexp >= this value will be considered (default: 1.)}
\qualifier{chatty}{chattiness (default: 1)}
}
\description

 The input lightcurve(s) should be a structure or an array of
 structures of the form:
     lc = struct { time, rate, fracexp, error }
 where each field is an array of doubles. 
 The output is a structure of the form:
     out = struct { time, [ epfold ], lc, dp, exact }
 Each field is an array, where each element corresponds to one of the
 segments the split has produced. The individual fields are:
     * time: average time of segment
     * epfold: struct { p, stat, nbins, badp }
       (not output if 'split_only' qualifier has been used)
     * lc: of the same form as input structure lc
     * dp: either p^2 / T or dpmin
     * exact: 1 if exact qualifier was used for epfold, 0 otherwise
 Remarks regarding qualifiers:
     * by default, exact is only chosen if there are less than exact_threshold
       pulses in the timespan considered. However, as of Sep 16,
       there was a bug in pfold, so that exact is ONLY used if exact_threshold
       the 'exact_threshold' qualifier exists. When this bug is fixed, please
       change the constant at the beginning of the script
     * nsrch and grid_steps are mutually exclusive. If you set nsrch
       to 0, the default epfold routine will be
       used
     * All qualifiers can also be passed using a structure named
       'epfold_qualifiers'. If this qualifier is present, its
       content will overwrite all other qualifiers given. If passed
       using this structure, all qualifiers must be assigned values
       (e.g. exact = 0, split_only = 1). Makes scripts easier to
       read.

\seealso{pfold, epfold, pulseperiod_epfold, pulseperiod_search, find_peak}
\done

\function{split_lc_at_gaps}
\synopsis{splits a light curve at gaps of a certain length}
\usage{Struct_Type split_lc[] = split_lc_at_gaps(Struct_Type lc, Double_Type gap_threshold);}
\qualifiers{
\qualifier{time}{[\code{="time"}] time field}
}
\description
    \code{lc} has to be a structure containing a \code{time} field, which is an array of ascending values,
    and other fields, which are arrays of the same length. As soon as the difference of
    sequential times is larger than \code{gap_threshold}, the structure is split, such that finally
    an array of structures like \code{lc} is returned.

   The following constructions are equivalent:\n
      \code{split_lc_at_gaps(lc, gap_threshold)}\n
      \code{split_struct(lc, blocks_between_gaps(lc.time, gap_threshold))}
\seealso{split_struct, blocks_between_gaps}
\done

\function{split_struct}
\synopsis{splits a structure of related arrays into several structures of the same kind}
\usage{Struct_Type structs[] = split_struct(Struct_Type s, Integer_Type group[]);}
\description
    All fields of the structure \code{s}, as well as \code{group}, have to be related arrays
    of equal length. The array elements with the same corresponding \code{group} value
    are selected to form a new structure, unless the \code{group} value is negative.
    In the latter case (\code{group<0}), the array elements are discarded.
    The split structures are returned in an array, indexed by the \code{group} value.
    In other words:\n
       \code{structs[g].field = s.field[where(group==g)]}\n
    for \code{0 <= g <= max(group)} and any \code{field} of the structure \code{s}.
\examples
    \code{variable prime_struct = struct { i, p };}\n
    \code{prime_struct.i = [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10];}\n
    \code{prime_struct.p = [ 2,  3,  5,  7, 11, 13, 17, 19, 23, 29];}\n

    \code{variable group = [-1,  1, -1,  2,  0,  1,  2,  3,  1,  3];}\n
    \code{% which might be obtained from the following selection:}\n
    \code{group = Integer_Type[length(prime_struct.p)];}\n
    \code{group[*] = -1;}\n
    \code{group[where(prime_struct.p mod 10 == 1)] = 0;}\n
    \code{group[where(prime_struct.p mod 10 == 3)] = 1;}\n
    \code{group[where(prime_struct.p mod 10 == 7)] = 2;}\n
    \code{group[where(prime_struct.p mod 10 == 9)] = 3;}\n

    \code{variable primes = split_struct(prime_struct, group);}\n
    \code{writecol(stdout, primes[1].i, primes[1].p);  % primes with last digit 3}\n
\seealso{split_lc_at_gaps}
\done

\function{split_struct_cols}
\synopsis{split structure in an array of structures with maximal num
columns}
\usage{Struct_Type str_array = split_struct_cols(Struct_Type str,
Integer_Type num);}
\description

     This function splits a structure in an array of structures,
     with maximal 'num' columns in each structure.

     This can be useful, as fits-tables only allow 999 colums to
     be written in the same extension. With split_struct_cols these
     can be splitted an written into different extensions. Afterwards
     they can be easily combined with struct_combine().
     
\seealso{fits_save_fit, struct_combine}
\done

\function{SRT_get_mw_spectra}
\synopsis{computes Milky Way spectrum from SRT data}
\usage{Struct_Type[] spectra = SRT_get_mw_spectra(Struct_Type data, Integer_Type[] chunks}
\description
	This function requires as input the SRT data in the format
	given by SRT_read() and an array of integer numbers which are the chunks
	with the spectrum data. If an array of chunk numbers is given then all
	corresponding spectra are calculated and returned.
	The spectra are determined with SRT_spectrum and all qualifiers of this
	function can be used.
\example
	variable data = SRT_read("milkyway.rad");
	variable spectra = SRT_get_mw_spectra(data,[1,4,7]);
\seealso{SRT_spectrum, SRT_read}
\done

\function{SRT_image}
\synopsis{extracts an image from a SRT data structure containing an npoinscan}
\usage{Float_Type img[] = SRT_image(Struct_Type data);}
\qualifiers{
\qualifier{plot}{plots the image}
}
\seealso{SRT_read}
\done

\function{SRT_plot_mw_spectra}
\synopsis{plots Milky Way spectra from SRT data}
\usage{SRT_plot_mw_spectra(Struct_Type[] spectrum);}
\altusage{SRT_plot_mw_spectra(Struct_Type[] data, Integer_Type[] chunks);}
\qualifiers{
\qualifier{pad}{additional space between plot frame and data limits}
\qualifier{fMax}{give the frequency of the cloud to be plotted, has to be of same length as spectra/chunks}
\qualifier{fConfMax}{upper confidence level of the maximal frequency of the cloud to be plotted, has to be of same length as spectra/chunks}
\qualifier{fConfMin}{lower confidence level of the maximal frequency of the cloud to be plotted, has to be of same length as spectra/chunks}
\qualifier{fMaxLin}{type of line used to plot fMax}
\qualifier{fConfLine}{type of line used to plot fConfMin and fConfMax}
\qualifier{xlabel}{label for x-axis}
\qualifier{ylabel}{label for y-axis}
\qualifier{title}{title for plot, either as single title or for each plot individually as an array of strings}
\qualifier{name}{file name of plot, either a single one or for each plot as a an array of string, specify file format, i.e., mw_scan.pdf}
\qualifier{return_xfig}{return xfig-data instead of plotting}
}
\description
	The function automatically plots Milky Way spectra obtained with the SRT.
	It can be given a single or an array of spectra or the data and an array of chunk numbers. The required data format is based on the output of SRT_get_mw_spectrum and all of its qualifiers can be used.
	It is possible to give the previously determined maximal frequency of the cloud and its upper and lower confidence interval as qualifier. This will create lines in the plot at the provided frequency.
\seealso{SRT_get_mw_spectra, SRT_read, SRT_spectrum}
\done

\function{SRT_read}
\synopsis{reads SRT data structures from a SRT .rad file}
\usage{Struct_Type data[] = SRT_read(String_Type filename);}
\qualifiers{
\qualifier{verbose}{}
\qualifier{onechunk}{read all data into one structure instead of an array
                (default if file has no comments)}
\qualifier{bins_to_cut}{[=8]: number of bad bins at high and low frequencies}
\qualifier{position}{[lat,longw] position of the SRT
                (default: lat=49.90 and longw=349.10)}
}
\seealso{SRT_spectrum, SRT_image}
\done

\function{SRT_show_chunks}
\synopsis{prints information for each chunk in an array of SRT data structures}
\usage{SRT_show_chunks(Struct_Type data[]);}
\seealso{SRT_read}
\done

\function{SRT_spectrum}
\synopsis{extracts a spectrum from a SRT data structure}
\usage{Struct_Type spec = SRT_spectrum(Struct_Type data);}
\qualifiers{
\qualifier{verbose}{}
\qualifier{bins_to_cut}{[\code{=8}] number of bins at both sides of the spectrum that are discarded}
\qualifier{bins_cut_low}{[\code{=0}] number of bins to additionaly cut from lower side}
\qualifier{bins_cut_high}{[\code{=0}] number of bins to additionaly cut from upper side}
\qualifier{normalize}{\code{spec.value} is normalized between 0 and 1.}
\qualifier{vLSR}{transform the frequency grid to a velocity grid}
\qualifier{filename=name}{plots the spectrum as name.ps in the
     current directory}
}
\description
\seealso{SRT_read}
\done

\function{stacking_vlbi_images}
\synopsis{stack VLBI images of same size}
\usage{stacking_vlbi_images(Array_Type \code{string(cleanfiles)}, String_Type \code{outputfile});}
\qualifiers{
\qualifier{ra_mas}{[={20,-20}] {left,right} limits of image in mas}
\qualifier{dec_mas}{[={-20,20}] {top,bottom} limits of image in mas}
\qualifier{plot_size}{[=15] size of plot}
\qualifier{n_sigma}{[=3.0] lowest contour of clean image}
\qualifier{sourcename}{[=default] name of the source, by default the name is read from the
                             .fits file, set to NULL for not plotting a source name}
\qualifier{obs_date1}{[=default] observation date, by default the date of the first
                              observation is used}
\qualifier{obs_date2}{[=default] 2nd observation date, by default the date of the last
                              observation is used}
\qualifier{cont_scl}{[="2"] set factor to change the separation between contour levels}
\qualifier{cont_lvl}{[=[c1,c2,..] contour levels [Jy] manually, overwrites other contour parameters}
\qualifier{major}{specify a vector for the major tic marks of the plot}
\qualifier{minor}{specify a vector for the minor tic marks of the plot}
}
\description
    This function creates a stacked image of individual VLBI-clean images.\n 
    Important note: all images need to be of the same size! Different beam sizes are 
    NOT recognized or corrected.\n
    The required input format are fits-file for both the clean and the modelfit images.
    The format of the output file depends on the suffix of the given\n
    \code{filename}. Possible formats of the output file are PDF, EPS,\n
    PNG, GIF, etc.
\done

\function{step1 (fit-function)}
\synopsis{a step function which is 1 above x0}
\description
    If one bin encloses \code{x0}, the value of step1 function
    is the fraction of the bin which is above \code{x0}.
\seealso{step2}
\done

\function{step2 (fit-function)}
\synopsis{a step function which is 1 between x1 and x2}
    If one bin encloses \code{x0} or \code{x2}, the value of the step2 function
    is the fraction of the bin which is between \code{x1} and \code{x2}.
\seealso{step1}
\done

\function{steppar}
\synopsis{performs a fit while stepping the value of a fit parameter through a given range}
\usage{Struct_Type info = steppar(String/Integer_Type par [, Double_Type val1, val2, step] );}
\altusage{(Struct_Type info, keys) = steppar( String/Integer_Type par [, Double_Type val1, val2, step]; keys);}
\qualifiers{
\qualifier{keys}{An additional structure is returned that can be used as FITS header keys}
\qualifier{frozen}{[=0] Perform steppar also for frozen Parameters.}
\qualifier{range}{[parmin,parmax] Stepping range. Default is the
                   minimal/maximal allowed parameter value.}
\qualifier{nsteps}{[=10] Number of steps the parameter 'par' is
                   stepped from range[0] to range[1].}
\qualifier{reset}{If given after each step the initial parameter set, which was
                    valid before this function was called, is restored.}
\qualifier{fit}{[=&fit_counts] Reference to the function running the fit algorithm.}
\qualifier{fitargs}{Arguments required by the 'fit' function. See __push_list for
                    format information!}
\qualifier{rerun}{Reruns a stepping procedure based on results of a previous run,
                    i.e., before each step the according parameter set of the previous
                    run is loaded, e.g., to improve the results using another fit_method.}
\qualifier{resume}{Missing steps in the given steppar-file will be calculated based
                     on the previous steps, i.e., resuming the stepping where it was
                     stopped.}
\qualifier{stepping}{Provide user defined grid [Array_Type], e.g., for a log grid (default: linear)}
\qualifier{check}{[=0] Before each step saved steppar informations of other stepped
                        parameters are gathered and checked for a parameter set with a
                        better chi2 (using steppar_get_bestparams). In case a better
                        parameter set was found the stepping will be restarted.
                        * ATTENTION: 'save' qualifier is required !!!
                        * Gathered are steppar-files, which match the pattern
                          given with 'save',e.g., if save is "steppar_PID00001.fits"
                          all files "steppar_PID?????.fits" are globed!
                          Note that the affix "_PID???" is automaticcaly appended to
                          the save string if it does not exist (see 'save' qualifier!)
                        * The Integer 'check' is set to is the maximal number of
                          restarts.
                        * A parameter set is considered better if
                          chi2_new < chi2_init * ( 1 - dchi2 )
                        * NOTE: Parameter grouping is possible (see steppar_get_bestparams)
                        }
\qualifier{dchi2}{[=0.1] Percental limit for the chi2 of a new parameter set to be
                  considered as a better parameter set (see 'check'):
                  chi2_new < chi2_init * ( 1 - dchi2 ).}
\qualifier{save}{After each stepping the result is saved in the file given
                   (as String_Type) with this qualifier. The chi2 of undone steps
                   are set to 0.! Also note that the filename is appended with
                   the parameterindex: 'steppar.fits' -> 'steppar_PID00001.fits'}
\qualifier{force}{Forces to overwrite an existing file given with 'save'.}
\qualifier{chatty}{Prints fitting information.}
}
\description
    The given parameter is stepped through in the given parameter 'range',
    which is devided into 'nsteps' equidistant value points. If 'val1', 'val2'
    and 'step' are given 'range' = ['val1','val2'] and
    'nsteps' = ('val2'-'val1')/'step'.
    At each of this points a fit is performed. The fitting starts with the
    value point closed to the best fit value and alternatingly progresses outwards
    to each side. The 'reset' qualifier restores the initial parameter set
    after each step, otherwise the parameter set of the nearest stepping point
    is used.
    The 'rerun' qualifier allows to rerun a stepping using the steppar information
    of a previous run, i.e., at each step the according parameter set of that
    previous run is loaded before the fit algorithm is started.
    In case the steppar function was killed, the 'resume' qualifier
    can be used to resume the stepping (given that 'save' was used!).
    If the 'save' qualifier is given the results of the stepping are stored
    as a .fits file after each step! NOTE that the affix "_PID?????" is
    automatically appended to the filename (if not already included), where
    the ??? are the ID of the stepped parameter. If the specified file
    already exists an error is thrown if the 'force' qualifier for overwriting
    is not set.
    The 'save' qualifier is also requiered if the 'check' for a better
    parameter set is enabled, i.e., before each step it is checked if there
    is a parameter set amongst other stepped parameters with
    chi2 < chi2_init * ( 1 - 'dchi2') leading to a restart of the stepping
    using the better parameter set, where chi2_init is the chi2 of the parameter
    set the stepping was initialized with (which will be updated after a restart).
    The maximal number of restarts is given by 'check'.
    This qualifier is useful if several steppar processes are running at the same
    time saving their results after each step to a fits-file, as in this case
    a better fit will be automatically applied! NOTE THAT using 'check'
    can lead to inconsistent steppar resulsts in terms of inital parameter sets,
    e.g., one steppar process is already finished and afterwards in another one
    a better parameter set is found leading to a reastart in the remaining
    steppar processes!

    => After using 'check' either make sure all steppar run with the same initial
       parameter set or manually extract the best parameter set and run all steppars
       once more!    
    
    The fitting itself is performed using the 'fit' function, to which
    all qualifiers are passed, which gets the 'fitargs' as arguments (see
    __push_list).
    
    IMPORTANT:
      steppar is based on the current version of the parameter set!
       
    KEYS:
     fit_method: fit method used for fitting (see get_fit_method)
     statistic:  chisqr according to the parameter set the function was callled with
     num_variable_params:
     num_bins:
     pval:       initial value of the stepped parameter    
     pname:      exact name of the stepped parameter
     %_freeze, %_min, %_max: additional information about the parameters, where
                             % is the escapedParameterName
\seealso{}
\done

\function{steppar_get_bestparams}
\synopsis{ Extracts new best fit parameter values out of steppar information }
\usage{ Struct_Type[] params = steppar_get_bestparams( String_Type[] File );}
\altusage{ Struct_Type[] params = steppar_get_bestparams( Struct_Type[] steppar );}
\qualifiers{
\qualifier{chatty}{Information output}
\qualifier{minchi2}{Chi2 limit the new parameter set has to beat
                      to be taken into accounts!
                      Default: eval_stat_counts.statistic}
\qualifier{groups}{Allows to extract best parameter sets for defined parameter
                     groups. 'groups' has to be an Array_Type, where each entry
                     represents a group and contains an Integer/String array
                     with the according parameter indices/names.
                     Useful in combination with 'simultaneous_fit' settings, e.i.
                     groups = %.model.groups!}
}
\description
      This function searches for the minimal chisqr value within the given
      files/steppar information and returns a parameter structure with the
      according parameters (see get_params). With the groups qualifier a
      groupping can be specified, i.e., the search for a minimal chi2 is
      done for each individual groups. The returned structure array
      only contains those parameter, which lead to the better chi2 value.
      If no better chi2 was found or there was no better chi2 than the
      specified minchi2 value, the function returns Struct_Type[0].

      ATTENTION:
      It is neccessary that the model with which the steppar was executed
      is loaded as this functions uses get_params and only changes those
      values of the freeParameters as the values of the frozen ones are
      not saved!
      If another parameter set is loaded there will be a difference in
      the new chi2 value this function gives (use 'chatty') and that
      chi2 an eval_counts will return!
\seealso{get_params}
\done

\function{steppar_get_conf}
\synopsis{Obtains confidence limits out of steppar information}
\usage{ Struct_Type[] conf = steppar_get_conf( String_Type[] File );}
\altusage{ Struct_Type[] conf = steppar_get_conf( Struct_Type[] steppar );}
\qualifiers{
\qualifier{chatty}{Information output}
\qualifier{dchi2}{[=2.71] Delta Chisqr}
}
\description
      Tries to obtain confidence level related to the given 'dchi2'
      value (default: 2.71) based on the steppar information.
      To do so this function calculates interpolated intersections
      of the chi2 landscape and min(chi2)+'dchi2'.
      Only if 2 or more such intersections are found the confidence
      limits are set. In case of more then 2 solutions always the
      minimal/maximal solutions are taken.
\seealso{interpol}
\done

\function{steppar_load}
\synopsis{loads saved steppar information}
\usage{ Struct_Type[] steppar = steppar_load( String_Typep[] pat ); }
\altusage{ Struct_Type[] steppar, keys = steppar_load( String_Type[] pat ; keys ); }
\qualifiers{
\qualifier{keys}{If given also the key structure is returned (using fits_read_header).
                   If keys is a List_Type with keynames, only these keys are read using
                   fits_read_key_struct, which is faster!}
\qualifier{ext}{Only loads extension fitting 'ext' (String_Type)!}
}
\description
     Loads a *.fits file storing the steppar information saved with
     steppar_save. The given string 'pat' can be a single file
     including several extension, an array with files or used as
     a pattern for multiple files in globbing format (see glob).
     
\seealso{steppar, steppar_save, glob}
\done

\function{steppar_plot}
\synopsis{xfig plot of the chi2 landscape of (a) stepped parameter(s)}
\usage{steppar_plot( Struct_Type steppar, key );}
\altusage{steppar_plot( String_Type stepparfile );}
\qualifiers{
\qualifier{norender}{If given xfig plots are returned instead of rendered}
\qualifier{pdfunite}{If given the rendered plots are united into a single one}
\qualifier{ignorez}{Ignores Chi2 values equal to Zero}
\qualifier{size}{=[15,11] Size of the plot}
\qualifier{path}{="steppar.pdf": Path for the rendered file}
\qualifier{ext}{=".pdf": File Type, e.g., "png"}
\qualifier{yoff}{Constant offset for the y-axis. Set to the initial statistic
                   by default}
\qualifier{yrange}{Range of the y-axis. Either Double_Type[1] or [2].
                     If only one value is given it is taken as ymax! }
\qualifier{dchi2}{=2.71: Delta Chisqr}
}
\done

\function{steppar_save}
\synopsis{Saves steppar information (& keys) to a *.fits file}
\usage{steppar_save( String_Type file, Struct_Type[] steppar [, keys]);}
\description
      Saves the output of 'steppar' to a *.fits file. 
\seealso{steppar}
\done

\function{storevar}
\synopsis{use `save_slang_variable' instead}
\done

\function{stretch_hist}
\synopsis{stretch a histogram grid}
\usage{Struct_Type hist = stretch_hist(hist, scal);}
\description
    Stretch the grid of a histogram structure with fields bin_lo, 
    bin_hi, value, and err. The bin_lo and bin_hi fields are scaled 
    by the factor scal. If other fields are present, those are 
    preserved and passed on. Only presence of bin_lo and bin_hi are
    checked for. If scal is an array with [a0,a1,a2,a3...], the grid
    is stretched with a polynomial function of 
       a0 + a1*bin_lo + a2*bin_lo^2 + ... .
\seealso{add_hist, shift_hist, scale_hist}
\done

\function{strftime_MJD}
\synopsis{formats an Modified Julian Date as a string}
\usage{String_Type strftime_MJD([String_Type fmt,] Double_Type MJD)}
\description
    If \code{fmt} is not specified, \code{"%Y-%m-%d, %H:%M:%S"} is assumed.
\seealso{strftime, gmtime, MJD2UNIXtime}
\done

\function{string2sys_err_array}
\synopsis{create sys_err_array from str = sys_err_array2string}
\usage{Array_Type sys_err = string2sys_err_array(String_Type str);}
\description

   This routines converts a string created by the function
   "sys_err_array2string" back to an array, which can then be applied
   to the data by the function set_sys_err_frac.
   
\seealso{fits_save_fit, set/get_sys_err_frac, sys_err_array2string}
\done

\function{string_intersection}
\synopsis{Returns the common substrings within all strings of the given array}
\usage{String_Type string_intersection(String_Type[] strings);}
\qualifiers{
    \qualifier{minlen}{restrict the returned substrings to a minimum length}
    \qualifier{longest}{return the longest substring only}
}
\example
    a = string_intersection(["mister", "twister"]; minlen = 3);
    % -> ["ister", "ster", "ter", "ste", "iste", "ist"]
\done

\function{string_match_perl}
\synopsis{matches a string against a RE and stores the results in $1, $2, ...}
\usage{Integer_Type string_match(String_Type str, pat[, Integer_Type pos])}
\description
    The string \code{str} is matched against the regular expression \code{pat},
    starting at the position \code{pos} (numbered from 1, which is the default).
    The function returns the position of the start of the match in \code{str}.
    The entire matching string is stored in the global variable $0,
    the substrings of patterns enclosed by "\\\\( ... \\\\)" pairs
    are stored in $1, $2, ..., $9  (as in Perl).

    \code{string_match_perl} is deprecated. Use S-Lang's string_matches instead.
\example
    \code{if( string_match_perl("hello world", "\\\\([a-z]+\\\\) \\\\([a-z]+\\\\)"R) )}\n
    \code{  ()=printf("1=%s, 2=%s\n", $1, $2);}

    \code{%} Equivalent way using \code{string_matches}:\n
    \code{variable m = string_matches("hello world", `\\([a-z]+\\) \\([a-z]+\\)`, 1);}\n
    \code{if(m!=NULL)  vmessage("1=%s, 2=%s", m[1], m[2]);}
\seealso{string_matches}
\done

\function{strjoin_wrap}
\synopsis{concatenates elements of an array and inserts linebreaks}
\usage{Sting_Type strjoin_wrap(Array_Type x, String_Type delim);}
\qualifiers{
\qualifier{maxlen}{[=72]: maximum number of characters in a line}
\qualifier{newline}{[="  "]: new line string}
\qualifier{initial}{[=""]: initial delimiter}
\qualifier{final}{[=""]: final delimiter}
}
\seealso{strjoin}
\done

\function{strreplace1}
\synopsis{replaces one occurence of a substring in a string by another string}
\usage{String_Type res = strreplace1(String_Type str, search_str, repl_str);}
\description
    replaces the first occurence of \code{search_str} in \code{str} by \code{repl_str}:\n
    \code{(res, ) = strreplace(str, search_str, repl_str, 1);}
\done

\function{strreplaces}
\synopsis{Replace one or more substrings}
\usage{String_Type strreplaces(String_Type a, String_Type[] b, String_Type[] c[, Integer_Type max_n]);}
\description
    This is a modificaiton to 'strreplace', where
    multiple, different substrings (given as arrays)
    are replaced at the same time.
\seealso{strreplace}
\done

\function{strsplit}
\synopsis{split a string at a separator and return contents as an array}
\usage{Array_Type strsplit(String_Type s, String_Type separator);}
\description
This routine takes a string s and splits it into parts separated by
a separator character.
\example
variable ra="12:34:56.78";
variable rastr,hh,mm,ss;
rastr=strsplit(ra,":");
hh=atof(rastr[0]); mm=atof(rastr[1]); ss=atof(rastr[2]);
ra=hms2deg(hh,mm,ss);
\done

\function{struct_array}
\synopsis{creates an array of structures}
\usage{Struct_Type[] struct_array(Integer_Type n, Struct_Type s);}
\description
    An array of n structures is created,
    where each structure is a copy of the
    given one s. 
\example
    variable sarr = struct_array(4, struct { firstname, lastname });
    sarr[2].firstname = "Karl";
    sarr[2].lastname = "Remeis";

    print(sarr);
    % will return
    % {firstname=NULL, lastname=NULL}
    % {firstname=NULL, lastname=NULL}
    % {firstname="Karl", lastname="Remeis"}
    % {firstname=NULL, lastname=NULL}
\seealso{table_copy}
\done

\function{struct_array_2_struct_of_arrays}
\synopsis{Converts a Struct_Type[] to Struct_Type with array fields}
\usage{Struct_Type struct_array_2_struct_of_arrays(Struct_Type[] SA);}
\qualifiers{
\qualifier{depth [0]:}{ If a field is a Struct_Type itself,
                        the this function calls itself iteratively
                        'depth' times going down the rabbit hole.}
\qualifier{dim [0]:}{ If a field is multi-dimensional 'dim' is the
                       dimension to which the array is appended to.}
}
\description
   This function converts a Struct-Array 'Struct_Type[]' to
   a single Struct_Type by converting the COMMON fields of
   all structs into arrays.
   The function works iteratively in case a field of the given
   struct array SA itself is a Struct_Type. The function will
   call itself maximaly 'depth' times.
   In case a field is multi-dimensional the array will be appended
   to the 'dim'-th dimension.

   NOTE that this function runs orders faster than 'merge_struct_arrays'
   while offering the same features as, e.g., reshaping multi-dimensional
   arrays (the reshaping in this function also works if not every field
   is multidimensional!).

   The only drawback is that the function fails if an entry is NULL as
   this case cannot be catched. In that case you may use
   'merge_struct_arrays'!
\examples

   %%%%% SIMPLE example
   variable sa = struct_array( 3, struct{ a=1, b=2 } );
   sa[0] = struct_combine( struct{c}, sa[0] );
   variable s = struct_array_2_struct_of_arrays( sa );
   print(s);
   {a=Integer_Type[3],
    b=Integer_Type[3]}
    
   %%%%% COMPLEX example
   variable sa = struct_array( 30000, struct{ a=1,
                                              b=_reshape([1:120],[1,2,60]),
                                              c=struct{ d="hello" }
                                            }
                             );
   %%% only merge the parent structure
   variable s = struct_array_2_struct_of_arrays( sa ; depth=0 );
   isis>    print(s);
   %  {a=Integer_Type[30000],
   %   b=Integer_Type[30000,2,60],
   %   c=Struct_Type[30000]}

   %%% merge also the first generation     
   variable s = struct_array_2_struct_of_arrays( sa ; depth=1 );
   print(s);
   %  {a=Integer_Type[30000],
   %   b=Integer_Type[30000,2,60],
   %   c=Struct_Type with 1 fields}
   print(s.c);
   %  {d=String_Type[3]}

   %%% use the second dimension of mudlti-dimensional arrays for merging:
   variable s = struct_array_2_struct_of_arrays( sa ; dim=1 );
   print(s);
   %  {a=Integer_Type[30000],
   %   b=Integer_Type[1,60000,60],
   %   c=Struct_Type[30000]}

   %%% Runtime comparision to 'merge_struct_array'
    tic;  s = struct_array_2_struct_of_arrays( sa );
    vmessage(" sa2soa took %g seconds to call",toc);
    print(s);
    tic;  m = merge_struct_arrays( sa );
    vmessage(" msa took %g seconds to call",toc);
    print(m);
      
\seealso{merge_struct_array, array_struct_field, array_flatten, unique, intersection}
\done

\function{struct_copy}
\synopsis{makes a copy of a struct with all its fields}
\usage{Struct_Type copy = struct_copy(Struct_Type struct);}
\description
    \code{struct_copy} copies a \code{Struct_Type} with all its
    fields, which can be of any Type! If a field is an Array_Type[]
    \code{struct_copy} calls \code{array_copy} and if a field is
    \code{Struct_Type}, \code{struct_copy} calls itself.
    Further it copies Ref_Type, which allows copying XFIG-Objects.
\example
    s = Struct_Type[1]; s[0]=struct{ a=Array_Type[1,2] };
    s[0].a[[0]]=[0:9]; S = struct{ f=s };     
    C = struct_copy(S); C.f[0].a[[0]] = ["modified"];
    print(S.f[0].a);
\seealso{array_copy, list_copy, COPY, assoc_copy}
\done

\function{struct_fields_list_to_array}
\synopsis{convert all fields of a structure from lists to arrays}
\usage{struct_fields_list_to_array(Struct_Type s[, DataType_Type t]);}
\seealso{list_to_array}
\done

\function{struct_field_or_default}
\synopsis{return value of a structure field or a default value}
\usage{value=struct_field_or_default(Struct_Type s, String_Type tag, default);}
\description
    return s.tag if it exists, otherwise default
\done

\function{struct_of_arrays_2_struct_array}
\synopsis{Converts a Struct_Type with array fields to a Struct_Type[]}
\usage{Struct_Type[] struct_of_arrays_2_struct_array(Struct_Type S);}
\description
   This function converts a 'Struct_Type' containing fields
   which are arrays (all arrays must have the same dimension!)
   to a Struct-array 'Struct_Type[]' with the same fields but
   one dimensional. That means the returned Struct_Type[] has
   then the dimension (array_shape) of the fields.
   The returned Struct-Array will contain all fields the original
   'S' also had, BUT for those fields with a different
   array_shape then the first field and the value will be set to the
   first entry of its field!
\example
   variable s = struct{ a=[1:3], b=[4:6], c=[3,20] };
   variable sa = struct_of_arrays_2_struct_array( s );
   print(sa);
   {a=1, b=4, c=3}
   {a=2, b=5, c=3}
   {a=3, b=6, c=3}
\seealso{array_map}
\done

\function{struct_print_field_statistics}
\usage{struct_print_field_statistics(Struct_Type s);}
\seealso{moment}
\done

\function{struct_to_assoc_array}
\synopsis{converts a structure to an associative array}
\usage{Assoc_Type struct_to_assoc_array(Struct_Type structure[, DataType_Type data-type]);}
\description
    The given structure is converted into an associative
    array. Thereby, the keys of the returned array will
    be equal to the field names of the structure.

    If the second argument is provided, the values of the
    associative array will be of that type. Otherwise,
    the type of the first field of the structure will be
    used. Note, that all fields have to be of the same
    data-type!
\example
    c = struct_to_assoc_array(struct {
      pi = 3.141592653589793,
      c = 2.99792458e10,
      h = 6.62606876e-27
    });
    foreach key (c) using ("keys")
      vmessage("c[%s] = %e", key, c[key]); ;
\seealso{Assoc_Type, assoc_get_keys, get_struct_field_names}
\done

\function{sum_lc}
\synopsis{combines light curves from one structure}
\usage{Struct_Type sum_lc(Struct_Type lc, Array_Type inds)}
\qualifiers{
\qualifier{time}{name of the time field (default: \code{"time"})}
\qualifier{rate}{format string for the rate field (default: \code{"rate%S"})}
\qualifier{error}{format string for the error field (default: \code{"error%S"})}
}
\description
    The rates in the fields of \code{lc} (named by \code{inds} 
    via the \code{rate} format statement) are summed up.
    The corresponding errors (named by the \code{error} format statement)
    are added in quadrature.
    The returned structure has the fields \code{time, rate, error}.
\examples
    % 1. Assume you have
      lc = struct { time, rate1, error1, rate2, error2, rate3, error3 };
    % Then
      slc = sum_lc(lc, [1:3]);
    % is equivalent to
      slc = struct {
        time = lc.time,
        rate = lc.rate1 + lc.rate2 + lc.rate3,
        error = sqrt( lc.error1^2 + lc.error2^2 + lc.error3^2 )
      };

    % 2. Assume you have
      lc = struct { time, rate_a, err_a, rate_b, err_b, rate_c, err_c };
    % Then
      slc = sum_lc(lc, ["a", "b", "c"]; rate="rate_%s", error="err_%s");
    % is equivalent to
      slc = struct {
        time = lc.time,
        rate = lc.rate_a + lc.rate_b + lc.rate_c,
        error = sqrt( lc.err_a^2 + lc.err_b^2 + lc.err_c^2 )
      };
\done

\function{sunpos}
\synopsis{computes the RA and Dec of the Sun at a given date.}
\usage{(Double_Type eq, Double_Type ecl) = sunpos(Double_Type JD);}
\description
   Computes the position of the Sun at the date JD (or dates, if JD
   is an array), given in Julian Date. Position calculation takes
   into account pertubations from the planets, aberration and
   nutation.  Output value 'eq' is a structure in equatorial
   coordinates: eq = struct{ra, dec}, 'ecl' is in ecliptic plane
   coordinates: ecl = struct{lon, obl} (Longitude and true
   obliquity).
   
   Note: Function adopted from the IDL function with the same name.
   See there for accuracy tests, which should also apply here.
\qualifiers{
\qualifier{MJD}{give time in MJD instead of JD}
\qualifier{radian}{return all variables as radians rather than degrees}
\qualifier{hour}{return RA in hours rather than degrees (not possible when radian is set).}
}
\example
   (eq, ecl) = sunpos(2455055.5) ;
   ()=printf("Right ascension: %.4f deg, Declination: %.4f deg\n", eq.ra, eq.dec) ;
\done

\function{switch_tbnew_rel_abund}
\usage{switch_tbnew_rel_abund([String_Type parnames]);}
\description
    Unless one or more \code{parnames} are given, all elements are switched.
\qualifiers{
\qualifier{inst}{[=1] instance of tbnew}
\qualifier{verbose}{}
}
\done

\function{sys_err_array2string}
\synopsis{creates a string from an Array returned by "get_sys_err_frac"}
\usage{String_Type str = sys_err_array2string(Array_Type sys_err);}
\description

    This routine creates a String from the Array of systematic errors
    apllied to the data. Such an array is returned by the function
    "get_sys_err_frac".
   
\seealso{fits_save_fit, set/get_sys_err_frac, string2sys_err_array}
\done

\function{table_add_row}
\usage{table_add_row(table, ...);}
\done

\function{table_copy}
\synopsis{makes a copy of all columns of a table}
\usage{Struct_Type copy = table_copy(Struct_Type table);}
\description
    \code{copy.field = @(table.field);  %} for every \code{field}
    NOTE: This also works for XFIG-PLOT-OBJECTS
\done

\function{table_model.add_additional}
\synopsis{Add additional contribution to interpolated spectra}
\usage{table_model.add_additional(String_Type name);}
#c%{{{
\qualifiers{
  \qualifier{unit}{[=""] Set the unit of the parameter}
  \qualifier{minimum}{[=min(values)] Set hard lower limit}
  \qualifier{bottom}{[=min(values)] Set minimum parameter range}
  \qualifier{top}{[=max(values)] Set maximum parameter range}
  \qualifier{maximum}{[=max(values)] Set hard upper limit}
  \qualifier{initial}{[=mean(values)] Set initial parameter value}
  \qualifier{delta}{[=(max(values)-min(values))*1e-2] Set parameter step hint}
}
\description
  Add additional contributions to the interpolated spectra. These parameters
  are simply a normalization for the contributions, e.g., 0 means there is
  no contribution, 1 means there is full contribution. For each of this
  contributions an additional spectrum is added to the interpolated model.

\seealso{table_model.add_parameter}
\done

\function{table_model.add_parameter}
\synopsis{Add interpolation parameter to table model}
\usage{table_model.add_parameter(String_Type name, Double_Type[] values);}
#c%{{{
\qualifiers{
  \qualifier{log}{if given, parameter is interpolated logarithmically rather than linearly}
  \qualifier{unit}{[=""] Set the unit of the parameter}
  \qualifier{minimum}{[=min(values)] Set hard lower limit}
  \qualifier{bottom}{[=min(values)] Set minimum parameter range}
  \qualifier{top}{[=max(values)] Set maximum parameter range}
  \qualifier{maximum}{[=max(values)] Set hard upper limit}
  \qualifier{initial}{[=mean(values)] Set initial parameter value}
  \qualifier{delta}{[=(max(values)-min(values))*1e-2] Set parameter step hint}
}
\description
  Each call to this function adds a new parameter with the given name
  and given interpolation points for this parameter. The table model gets
  interpolated for this given points.

\seealso{table_model.add_additional}
\done

\function{table_model.close}
\synopsis{Finish table model}
\usage{table_model.close();}
#c%{{{
\description
  Write the checksums to each HDU and close the
  table model file.
\done

\function{table_model.set_additional}
\synopsis{Set interpolation model contributions}
\usage{table_model.set_additional(Ref_Type function);}
#c%{{{
\description
  Set the table model contributions function that gets stored
  in the table on the defined energy grid. The function must
  be of the form
  \code{define modelfunction (bin_lo, bin_hi, params, k)}
  where bin_lo and bin_hi are the bin boundary arrays and
  params is the parameter array set by table_model.add_parameter.
  The additional argument k is set to the number of the additional
  parameter. So k=1 for the first additonal parameter, k=2 for the
  second and so on.

  The function is expected to calculate the k-th contribution with
  the parameter for this contribution set to 1.

\seealso{table_model.set_additional}
\done

\function{table_model.set_function}
\synopsis{Set interpolation model function}
\usage{table_model.set_function(Ref_Type function);}
#c%{{{
\description
  Set the table model function that gets stored in the table
  on the defined energy grid. The function must be of the form
  \code{define modelfunction (bin_lo, bin_hi, params)}
  where bin_lo and bin_hi are the bin boundary arrays and
  params is the parameter array set by table_model.add_parameter.
  The function may use qualifiers.

\seealso{table_model.set_additional}
\done

\function{table_model.set_grid}
\synopsis{Set the energy grid for the table model}
\usage{table_model.set_grid(bin_lo, bin_hi);}
#c%{{{
\qualifiers{
  \qualifier{lolimit}{[=0] The model value if lower than lowest bin}
  \qualifier{hilimit}{[=0] The model value if higher than highest bin}
}
\description
  Set the energy grid on which the model gets interpolated.
  Boundaries of the bins have to match.

\seealso{table_model.set_function, table_model.set_additional}
\done

\function{table_model.write}
\synopsis{Write table model to file}
\usage{table_model.write();}
#c%{{{
\qualifiers{
  \qualifier{redshift}{if given, set redshift parameter flag}
  \qualifier{mult}{if given, model is multiplicative (default additive)}
  \qualifier{verbose}{show additional information}
}
\description
  Given the model function and additional contributions, this
  function evaluates the model and contributions on the given
  parameter values and writes the resulting spectra to the
  table file. Any qualifier passed to this function is passed
  on to the model functions.

  IMPORTANT: This function calls the evaluation functions
  several times and may take a long time to finish. Depending
  on the evaluation time of the model functions.

\seealso{table_model.close}
\done

\function{table_print_TeX}
\synopsis{prints certain columns of a structure as a TeX table}
\usage{table_print_TeX(Struct_Type tab[, String_Type cols[]]);}
\description
    If no columns are specified by the string-array \code{cols},
    all columns of the table tab are used.
\qualifiers{
\qualifier{align}{alignment of all columns: "l", "c", "r" [default: "l"]}
\qualifier{heading}{[=\code{cols}]: headings for TeX table}
\qualifier{output}{[=stdout]: output can be written in a file by stating
                       a File_Type or the filename (String_Type)}
}
\done

\function{tai2tt}
\synopsis{Convert a (M)JD in International Atomic Time into Terrestrial Time (TT, also known as TDT or ET)}
\usage{tt=tai2tt(JD)}
\description
  This routine converts a Julian Date that is given in International Atomic
  Time (TAI) into Terrestrial Time, which is roughly corresponding to
  Terrestrial Dynamic Time or Ephemeris Time. This is done by
  adding 32.184s to the TT. This routine is array safe.

\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TT-TAI in seconds, rather than the corrected (M)JD.}
}
\seealso{utc2tai,tai2utc,tt2tai,tt2tdb}
\done

\function{tai2utc}
\synopsis{Converts a (M)JD in International Atomic Time into civil time (UTC)}
\usage{utc=tai2utc(JD)}
\description
This routine converts a Julian Date that is given in international
atomic time into Coordinated Universal Time (UTC) by applying the
relevant correction for leap seconds. This routine is array safe.

NOTE: This function is only approximately correct. Within about a
minute of a leap second this routine can be off by as much as
one second.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the UTC-TAI in seconds, rather than the corrected (M)JD.}
\qualifier{leapfile}{Path to a file containing the leap second
          information. This file is available, e.g., from
          ftp://maia.usno.navy.mil/ser7/tai-utc.dat
          Most astronomical software systems distribute this file. The default
          is getenv("LHEASOFT")/refdata}
}
\seealso{utc2tai,tt2tai,tai2tt}
\done

\function{taylor}
\synopsis{Returns the taylor series for given coefficients}
\usage{Double_Type taylor(Double_Type[] x, coefficients)}
\description
    Returns the taylor series around x with the given
    'coefficients', which lengths determines the order.
    The sum is evaluated using the Horner (1819) schema
    for numerical accuracy and computation speed:

    c0 + c1*x + 1/2!*c2*x^2 + 1/3!*c3*x^3 + ...
    = (((... + c3)*x/3 + c2)*x/2 + c1)*x + c0
\example
    x = [0, 1];
    c = [1, 4e-3, 0, 3e-6]; % up to third order

    taylor(x, c); % returns [1.0, 1.0040005]
\done

\function{taylorcoeff_from_struct}
\synopsis{extracts taylor coefficients from the field names of a structure}
\usage{Double_Type[] taylorcoeff_from_struct(Struct_Type struct[,
                    String_Type 0th_pattern, String_Type Nth_pattern]);}
\description
    If a structure has fields which names reflect the
    different orders of a taylor series, this function
    extracts these coefficients by matching the field
    names against two patterns.

    '0th_pattern' is the field name of the zero order
    and 'Nth_pattern' this of all higher order terms.
    Both patterns have to be regular expressions. In
    particular, the expression of the Nth order must
    contain a number extraction two determine the order
    N of that field.


    By default, '0th_pattern' is set to "[a-zA-Z]0" and
    'Nth_pattern' to "[a-zA-Z]\\([0-9]*\\)dot"R. If the
    latter _pattern matches, but the extracted number
    is an empty string N=1 is assumed.

    The returned array contains the taylor coefficients
    in ascending order as used by the 'taylor'-function.
\seealso{taylor, string_matches}
\done

\function{tcg2tt}
\synopsis{Convert a (M)JD in TCG to Terrestrial Time (TT, aka TDT or ET)}
\usage{tai=tcg2tt(JD)}
\description
  This routine converts a Julian Date that is given in geocentric coordinate
  time (TCG) to Terrestrial Time. The two time scales differ by a constant
  rate(!) because TT uses the SI second at the surface of the geoid, and TCG
  uses the SI second at the potential of the center of the Earth.
  This routine is array safe.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TT-TCG in seconds, rather than the corrected (M)JD.}
}
\seealso{tcg2tt,utc2tai,tai2utc,tai2tt,tt2tdb}
\done

\function{tcp_client}
\synopsis{connects to a TCP server}
\usage{Struct_Type  tcp_client(String_Type host[, Integer_Type port]);
\altusage{Integer_Type tcp_client(String_Type host[, Integer_Type port];
                   msg_handler = Ref_Type, obj_handler = Ref_Type);}}
\qualifiers{
    \qualifier{nogreet}{disable greet messages (see below)}
    \qualifier{chatty}{chattiness (default: 1)}
    \qualifier{COMMENT:}{Further qualifiers are listed at HOOKS below.}
}
\description
    Uses the `socket' module to connect to a TCP server. After a
    connection attempt, a greet message is exchanged with the server
    by default unless the `nogreet' qualifier is set. In the latter
    case the `tcp_server' has be set up with the same qualifier.
    After the connection has been established, a structure with
    functions for communicating with the server is returned.
    Alternatively, if the `msg_handler` and `obj_handler` hooks are
    set (see the HOOK section below) the client remains in a main
    loop until the connection to the server has been closed. In this
    case, the function returns 0 if any error occured during the
    lifetime of the connection or 1 otherwise. The client structure,
    which is passed to all hooks and returned if these hooks are no
    set, has the following fields:
    
      Integer_Type &send(String_Type msg or Any_Type obj)
        Function for sending a string message or an SLang object
        to the server (see `pack_obj' for list of supported types).
        Returns the number of bytes sent.
        Warning: Currently, sending objects is slow (~40 kB/s,
                 i.e., ~20 s for 100,000 doubles).
        Qualifiers:
          receipt - wait for a confirmation by the client for the
            receipt (the server's `receive' function needs the
            same qualifier to be set).
      BString_Type or Any_Type &receive()
        Wait for and receive a message from the server. Returns
        either the message itself (BString_Type) or in case of a
        received object the object itself.
        Qualifiers:
          dontwait - do not wait for a message and return NULL in
            case no message or object is pending.
          receipt  - send a receipt to the client after having
            received a message or an object
      Integer_Type &disconnect()
        Disconnect from the server, i.e., close the communication
        socket. Returns 1 in success or 0 otherwise.
      Integer_Type connected - boolean value indicating whether
        the connection is still established (1) or not (0)
      Struct_Type config - internal configuration
      Struct_Type hook - defined hooks

 HOOKS
    The following functions are called via qualifiers of the same
    name, which need to be set to references (Ref_Type) to user-
    defined functions (see below for an example). The passed
    `client' is the client's structure as defined above.
    
    String_Type or Integer_Type &greet_hook(client, greetmsg)
      Called after having received the greet message from the server
      (if not disabled by the `nogreet' qualifier). Should return
      the greet message to be sent back to the server. Return 0 in
      case the greet message from the server should be rejected,
      which will cancel the connection.
    Void_Type &connect_hook(client)
      Called after a connection to the server is established. Can
      be used to start the communication with the server in
      combination with the `msg_handler' hook.
    Void_Type &closed_hook(client)
      Called after the connection has been closed from the server's
      side.
    Void_Type &msg_handler(client, msg)
      Called after a message (BString_Type) has been received from
      the server.
    Void_Type &obj_handler(client, obj)
      Called after an object has been received from the server.
\example
    % simple functions for receiving messages and objects
    define getmsg(c, msg) { vmessage("message from server: %s", msg); };
    define getobj(c, obj) { message("received an object:"); print(obj); };
    % start the client
    variable stat = tcp_client("localhost"; msg_handler = &getmsg, obj_handler = &getobj);
    % check its exit status
    if (stat == 0) { message("lost connection to server"); }

    % a more sophisticated example can be found on the Remeis wiki
    % www.sternwarte.uni-erlangen.de/wiki/doku.php?id=isis:socket
\seealso{tcp_server, socket, unpack_obj}
\done

\function{tcp_get_server_handle}
\synopsis{get a handle to a running TCP server}
\usage{Struct_Type tcp_get_server_handle();}
\qualifiers{
    \qualifier{kill}{shut down / kill the server}
}
\description
    In case the handle, i.e., the structure of a running
    `tcp_server' got lost by accident, this function
    returns this structure again. Alternatively, the
    `kill' qualifier tries to shut down the server. If
    this fails all communication sockets are closed,
    which basically means to kill the server.
\seealso{tcp_server}
\done

\function{tcp_server}
\synopsis{implements a basic TCP server}
\usage{Integer_Type tcp_server();
 or  Struct_Type tcp_server(; background);}
\qualifiers{
    \qualifier{port}{bind port (default: 1149)}
    \qualifier{maxclients}{maximum allowed number of clients connected
      simultaneously (default: 10)}
    \qualifier{nogreet}{disable greet messages (see below)}
    \qualifier{background}{run server in the background (see below)}
    \qualifier{ip}{bind address (default: "0.0.0.0", i.e., accept
      connections from outside; only change if you know what
      you are doing)}
    \qualifier{chatty}{chattiness (default: 1)}
    \qualifier{COMMENT:}{Further qualifiers are listed at HOOKS below.}
}
\description
    Uses the `socket' module to implement a basic TCP server. It
    listens for clients trying to connect and accepts a connection
    after a greet message has been exchanged successfully. The
    greet message can be disabled using the `nogreet' qualifier,
    which in this case needs to be set for a `tcp_client' as well.
    Furthermore, any message or object received from a client is
    read automatically. After any of these actions taken by the
    server, a user-defined function may be called in order to handle
    these events further (see the HOOK section below).

    The `background' qualifier allows the server to be started in
    background mode, such that commands can still be entered into
    the ISIS prompt. This background mode is realized by calling
    the main server function, which handles client events, each
    second scheduling an `alarm` signal (SIGALRM).
    NOTE: the alarm signals are triggered in the main program loop
          of ISIS, i.e., while it's "working". That means the
          main server function is not executed while ISIS awaits
          input from the prompt (ISIS "sleeps" here). Thus, you
          might want to do at least something, e.g, press enter.
    WARNING: further alarms should not be scheduled using `alarm'
             or `signal'. Otherwise, the server might crash.

    If not in background mode, `tcp_server' returns either 1 after 
    the server has been shut down successfully or 0 otherwise. Else
    the structure of the following form is returned. Note that this
    structure is also passed to each hook (see below):
    
      Struct_Type[] client - list of clients
      Integer_Type &shutdown()
        Function to shut down the server. After all clients are
        requested to disconnect, all sockets are closed. Returns
        whether the shut down was successful (1) or not (0).
        Qualifiers:
          trigger - shut down the server at the end of the main
            loop, which does not interrupt ongoing communication
      Void_Type &cleanclients()
        Function to remove clients no longer connected from the list.
      Integer_Type running - boolean value indicating the server's
        state (1: running; 0: shut down)
      Struct_Type config - internal configuration
      Struct_Type hook - defined hooks

    Each `client' structure has the following fields:
    
      Integer_Type &send(String_Type msg or Any_Type obj)
        Function for sending a string message or an SLang object
        to a client (see `pack_obj' for list of supported types).
        Returns the number of bytes sent.
        Warning: Currently, sending objects is slow (~40 kB/s,
                 i.e., ~20 s for 100,000 doubles).
        Qualifiers:
          receipt - wait for a confirmation by the client for the
            receipt (the client's `receive' function needs the
            same qualifier to be set).
      BString_Type or Any_Type &receive()
        Wait for and receive a message from the client. Returns
        either the message itself (BString_Type) or in case of a
        received object the object itself.
        Qualifiers:
          dontwait - do not wait for a message and return NULL in
            case no message or object is pending.
          receipt  - send a receipt to the client after having
            received a message or an object
      Integer_Type &kick()
        Kick the client, i.e., close the communication socket.
        Returns 1 in success or 0 otherwise.
      Integer_Type connected - boolean value indicating whether
        the client is still connected (1) or not (0)
      Integer_Type number - the client's number, which is a serial
        increasing identifier starting with zero
      Struct_Type config - internal configuration
      BString_Type[] unhmsg - in case the `msg_handler'-hook is not
        set, messages of the client, which have been received
        without a call to `receive` during the server's main loop
        are appended here
      List_Type unhobj - in case the `obj_handler'-hook is not set,
        objects without a call to `receive' are appended here

 HOOKS
    The following functions are called via qualifiers of the same
    name, which need to be set to references (Ref_Type) to user-
    defined functions (see below for an example). The passed
    `server' and `client' are the server's and corresponding
    client's structures as defined above.
    
    Integer_Type &connect_hook(server, client)
      Called after a client tries to connect to the server. Should
      return 1 or 0 for accepting or rejecting the connection,
      respectively.
    String_Type  &greet_hook(server, client)
 or Integer_Type &greet_hook(server, client, greetmsg)
      Called after accepting a client's connection, which should
      return the greet message to be sent to the client. Is called
      again after the re-greet message has been received from the
      client, which should return 1 or 0 for accepting or rejecting
      the greet, i.e, the connection, respectively. The hook is not
      called if the server was set up with the `nogreet' qualifier.
    Void_Type &established_hook(server, client)
      After a connection has been accepted including the greet, this
      function allows to start the communication between the client
      and the server by, e.g., sending messages to the client in
      combination with the `msg_handler' hook.
    Void_Type &disconnect_hook(server, client)
      Called after a client disconnected from the server by itself,
      i.e., the client has not been kicked.
    Void_Type &msg_handler(server, client, msg)
      Called after a message (BString_Type) has been received from
      the client. If this hook is not set, then the message is
      appended to the `unhmsg` field of the client's structure.
    Void_Type &obj_handler(server, client, obj)
      Called after an object has been received from the client. If
      this hook is not set, then the object is appended to the
      `unhobj` field of the client's structure.
\example
    % after a client's connection has been accepted,
    % send a data structure and disconnect client
    define do_client(s,c) {
      vmessage("sending data to client #%d", c.number);
      ()=c.send("welcome client! sending data...");
      ()=c.send(struct { data = [1:5]*10, err = [1:5] });
      ()=c.kick();
      % remove kicked client from the list
      s.cleanclients();
    }
    % start the server
    variable stat = tcp_server(; established_hook = &do_client);
    % check its exit status
    if (stat == 0) { message("server exited unexpectedly"); }

    % a more sophisticated example can be found on the Remeis wiki
    % www.sternwarte.uni-erlangen.de/wiki/doku.php?id=isis:socket
\seealso{tcp_client, socket, alarm, pack_obj}
\done

\function{tdb2tt}
\synopsis{Convert a (M)JD in Terrestrial Barycentric Time (TDB) into Terrestrial Time (TT, aka TDT)}
\usage{tt=tdb2tt(JD)}
\description
  This function converts a Julian Date that is given in Terrestrial Barycentric
  time (TDB) into Terrestrial Time (TT).

  By default the routine uses equation 2.6 of Kaplan (2005, USNO Circular 179),
  which is a truncated version of the series provided by Fairhead and Bretagnon,
  1990, A&A 229, 240 and which is better than +/-10mus between AD1600 and
  AD2200.

  Since that series is for TT-TDB, we formally calculate the deltat for the TT
  that equals the given JD, we make no attempt to correct for the slight
  change in deltat between JD(TT) and JD(TDB), since the correction is smaller
  than the precision of the Fairhead and Bretagnon series. Use the JPL ephemeris
  for ms pulsar work.

  If the qualifier ephemeris is given, and points to a JPL ephemeris that
  does contain TT-TDB information, then the value determined from the
  ephemeris is computed (which has a higher precision than Fairhead & Bretagnon,
  but is slower to evaluate).

  Note that for virtually all practical applications apart from barycentering
  ms radio pulsar data it is sufficient to use TT as the argument to DE405
  or VSOP1987.

  This routine is array safe.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the difference TT-TDB in seconds, rather than the corrected (M)JD.}
\qualifier{ephemeris}{if set and no argument, then correct using DE430, otherwise
                        use the ephemeris provided (see function jpl_initeph).}
}
\seealso{tdb2tt,utc2tai,tai2utc,tai2tt,jpl_initeph}
\done

\function{tdist_fit}
\synopsis{Fitting a student's t-distribution to data in energy space}
\usage{fit_fun("tdist");}
\description
    Student's t-distribution with LineE being the centroid energy,
    Width the full width at half maximum and nu the number of degrees
    of freedom. nu=1 reproduces a Lorentzian, nu->infty a Gaussian.
\seealso{tdist_pdf}
\done

\function{TeX_value_pm_error}
\synopsis{formats a \code{(value, min, max)} triple as TeX string}
\usage{String_Type TeX_value_pm_error(Double_Type value, Double_Type min, Double_Type max)}
\altusage{String_Type TeX_value_pm_error(Struct_Type output_of__round_conf)}
\qualifiers{
\qualifier{factor}{[=\code{1}]: factor to multiply \code{value, min, max}, will not be shown,
                   works only if the function is called with three arguments}
\qualifier{neg}{factorize overall minus sign}
\qualifier{sci}{[=\code{2}]: exponent to switch to scienficic notation}
\qualifier{scismall}{[=\code{-sci}]: exponent for small numbers}
\qualifier{scilarge}{[=\code{+sci}]: exponent for large numbers}
}
\description
    \code{TeX_value_pm_error} tries to produce a reasonable TeX output
    from a \code{(value, min, max)} tripel or the output of the function
    \code{round_conf}.
    Rounding and obtaining the number of digits is done with the
    function \code{round_conf}.
\seealso{round_conf,round_err}
\done

\function{thawedParameters}
\synopsis{find all parameters of the current fit-function that are not frozen}
\usage{Integer_Type[] thawedParameters()}
\description
    Note that \code{thawedParameters} may include ones that are
    tied to another one, or derived as functions of other parameters.
\seealso{freeParameters}
\done

\function{timeOfPhase}
\synopsis{Compute the time corresponding to given phase}
\usage{Double_Type[] timeOfPhase(Double_Type[] phase, Double_Type, p, dp, ddp);}
\description
  Given a phase and a period evolution by the period P \code{p},
  first, and second derivative of P, \code{dp} and \code{ddp},
  respectively, calculate the time.

  Note: This function is essentially the inverse of \code{timeOfPhase}.
  For non-zero \code{ddp} the inverse is only approximated through
  series inversion!
\seealso{phaseOfTime}
\done

\function{time_array}
\synopsis{Converts a time [s] (e.g., _time) into an array [sec,min, ...]}
\usage{Integer_Type[] = time_array( Integer_Type );}
\altusage{String_Type = time_array( Integer_Type ; str);}
\qualifiers{
\qualifier{str}{Instead of a time array a String_Type is returned!}
}
\description
   Converts a time given in seconds into a time array, where
   the first entry corresponds to the seconds, the second to
   the minutes, etc. The length of the array is variable,
   i.e., if the given time is of the order of hours, the array
   will not include days and years.

   If the qualifier 'str' is given the array will be converted
   into a string (see example).
\example
   isis> ta = format_time ( _time ); ta;
   Integer_Type[5];
   isis> print(ta);
   54
   1
   14
   302
   43

   %or
   isis> ts = format_time ( _time ;str ); ts;
   43y 302d 14h  1m 54s
   
\seealso{_time}
\done

\function{transpose_table}
\synopsis{transforms a structure of arrays into an array of structures and vice versa}
\usage{Array_Type table_by_rows    = transpose_table(Struct_Type table_by_columns);
\altusage{Struct_Type table_by_columns = transpose_table(Array_Type table_by_rows);}
}
\description
    \code{table_by_columns.field[i] = table_by_rows[i].field;}  % for each \code{field} and index \code{i}
\done

\function{try_or_timeout}
\usage{try_or_timeout(UInteger_Type seconds, Ref_Type function, ...);}
\description
    Runs the function referenced by \code{function} for at most
    \code{seconds} seconds. If \code{function} finishes before
    \code{seconds} have elapsed it returns the return values of
    \code{function}. If \code{function} does not finish in that time
    it throws a \code{TimoutError}.
    Additional arguments passed on to \code{function} may simply be
    specified as additional arguments to \code{try_or_timeout}.
    Qualifiers can be passed on as usual.
\done

\function{tt2tai}
\synopsis{Convert a (M)JD in Terrestrial Time (TT, aka TDT or ET) into TAI}
\usage{tai=tt2tai(JD)}
\description
This routine converts a Julian Date that is given in Terrestrial Time
(formerly known as Terrestrial Dynamic Time or Ephemeris Time) into
TAI. This is done by subtracting 32.184s from the TT.
 This routine is array safe.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TAI-TT in seconds, rather than the corrected (M)JD.}
}
\seealso{utc2tai,tai2utc,tai2tt,tt2tdb}
\done

\function{tt2tcg}
\synopsis{Convert a (M)JD in Terrestrial Time (TT, aka TDT or ET) into TCG}
\usage{tai=tt2tcg(JD)}
\description
  This routine converts a Julian Date that is given in Terrestrial Time
  (formerly known as Terrestrial Dynamic Time or Ephemeris Time) into
  geocentric coordinate time (TCG). The two time scales differ by a constant
  rate(!) because TT uses the SI second at the surface of the geoid, and TCG uses
  the SI second at the potential of the center of the Earth.
  This routine is array safe.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TCG-TT in seconds, rather than the corrected (M)JD.}
}
\seealso{tcg2tt,utc2tai,tai2utc,tai2tt,tt2tdb}
\done

\function{tt2tdb}
\synopsis{Convert a (M)JD in Terrestrial Time (TT, aka TDT or ET) into Terrestrial Barycentric Time (TDB)}
\usage{tdb=tt2tdb(JD)}
\description
  This function converts a Julian Date that is given in Terrestrial
  Time into Terrestrial Dynamic Barycentric Time.

  By default the routine uses equation 2.6 of Kaplan (2005, USNO Circular 179), which
  is a truncated version of the series provided by Fairhead and Bretagnon,
  1990, A&A 229, 240 and which is better than +/-10mus between AD1600 and AD2200.

  If the qualifier ephemeris is given, and points to a JPL ephemeris that
  does contain TT-TDB information, then the value determined from the
  ephemeris is computed (which has a higher precision than Fairhead & Bretagnon,
  but is slower to evaluate).

  Note that for virtually all practical applications apart from barycentering
  ms radio pulsar data it is sufficient to use TT as the argument to DE405
  or VSOP1987.

  This routine is array safe.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the difference TDB-TT in seconds, rather than the corrected (M)JD.}
\qualifier{ephemeris}{if set and no argument, then correct using DE430, otherwise
                        use the ephemeris provided (see function jpl_initeph).}
}
\seealso{tdb2tt,utc2tai,tai2utc,tai2tt,jpl_initeph}
\done

\function{tt2ut1}
\synopsis{Convert a (M)JD in TT into UT1}
\usage{ut1=tt2ut1(JD)}
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TT-UT1 in seconds, rather than the corrected (M)JD.}
\qualifier{morrison}{Perform the correction using a linear interpolation
                       to the points by Morrison and Stephenson, 2004,
                       Journal for the History of Astronomy 35, 327.
                       Not good after 2000.}
}
\description

 This function converts a (Modified) Julian Date that is given in
 TT into UT1, i.e., the timescale that defines the earth rotation angle.

 By default, for years before 2005 the function uses the polynomial
 expressions for TT-UT1 published by F. Espenak and J. Meeus, 2006,
 Five Millenium Canon of Solar Eclipses: -1999 to +3000,
 NASA TP-2006-214141, for years after 2005 the polynomials published
 by Espenak at http://www.eclipsewise.com/help/deltatpoly2014.html are
 used.

 Alternatively, a linar interpolation in the solar eclipse derived
 data by Morrison and Stephenson (2000) can be used (see morrison
 qualifier).

 The uncertainty of the result is 1s or less from 1780 until 1830,
 less than 1s from 1830-2000. It increases quadratically backwards
 from 1820, sigma ~32((year-1820)/100)^2.

 This routine is array safe.

 Note: Because of the uncertainty in TT-UT1, the same
 polynomial is used for the TT->UT1 and UT1->TT conversion,
 and therefore it generally is NOT true that JD==ut12tt(tt2ut1(JD)).

\seealso{ut12tt,utc2tai,tai2utc,tai2tt}
\done

\function{unique_n}
\synopsis{finds the indicies of unique tupels}
\usage{Integer_Type[] unique_n(Array_Type a1, ..., an)}
\description
    The arrays \code{a1}, ..., \code{an} (with \code{n>=1}) must have the same length,
    but may contain different data types.
    The return value is an array of all indices \code{i}
    of all unique tupels \code{(a1[i], ..., an[i])}.
\done

\function{UNIXtime2MJD2}
\synopsis{converts UNIX time to modified Julian date}
\usage{Double_Type UNIXtime2MJD2(Long_Type secs)}
\seealso{MJD2UNIXtime}
\done

\function{unpack_obj}
\synopsis{converts a binary string back into an SLang object}
\usage{Any_Type pack_obj(BString_Type obj);}
\description
    An input array of binary strings (BString_Type), which has
    been created by the `pack_obj' function, is converted back
    into an SLang object using the `unpack' function.

    See the documentation of `pack_obj` for more details.
\example
#v+
#p+
    s = pack_obj(PI);        % s[0] = "d\\030-DT\\373!\\011@"

    a = unpack_obj(s);       % a = 3.141592653589793;
#p-
#v-
\seealso{pack_obj, unpack, pack}
\done

\function{unset_lines_par_fun}
\synopsis{removes the par_fun's in a lines-model to speed up the model}
\usage{unset_lines_par_fun([Integer_Type id]);}
\description
    The amplitude parameters of the lines-model are reset for every line:\n
    \code{set_par_fun("lines(id).line_A", NULL);}\n
    If \code{id} is not specified, code{id=1} is used.
\seealso{gauss, lines, set_lines_par_fun}
\done

\function{unshift}
\synopsis{fit function to determine the shift of an array to a reference one}
\usage{unshift(id)}
\description
    The function 'shift' rotates an array by a given number
    of indices. The fit function described here tries to
    get this number back: the counts of the dataset 'id'
    are assumed to be a shifted version of a reference
    array. In order to do the fit, this reference array
    has to be associated to the dataset via:
    
      set_dataset_metadata(id, reference_array)
      
    It is also possible to scale the array values to match
    the reference ones or to add an offest to the array.
    Summarized, the fit parameters are as follows:
      shift  - relative shift of the counts to the
               reference array
      scale  - factor the reference is multiplied with
      offset - offset added to the model counts
 
    The model counts are calculated by
      model = scale*shift(reference_array, shift) + offset
\seealso{backshift, shift_intpol, set_dataset_metadata, shift}
\done

\function{ut12tt}
\synopsis{Convert a (M)JD in UT1 into TT}
\usage{utc=ut12tt(JD)}

\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TT-UT1 in seconds, rather than the corrected (M)JD.}
\qualifier{morrison}{Perform the correction using a linear interpolation
                       to the points by Morrison and Stephenson, 2004,
                       Journal for the History of Astronomy 35, 327.
                       Not really good after 2000}
}
\description
 This function converts a (Modified) Julian Date that is given in UT1,
 i.e., the timescale that defines the earth rotation angle,
 into a (M)JD in TT.

 By default, for years before 2005 the function uses the polynomial
 expressions for TT-UT1 published by F. Espenak and J. Meeus, 2006,
 Five Millenium Canon of Solar Eclipses: -1999 to +3000,
 NASA TP-2006-214141, for years after 2005 the polynomials published
 by Espenak at http://www.eclipsewise.com/help/deltatpoly2014.html are
 used.

 Alternatively, a linar interpolation in the solar eclipse derived
 data by Morrison and Stephenson (2000) can be used (see morrison
 qualifier).

 The uncertainty of the result is 1s or less from 1780 until 1830,
 less than 1s from 1830-2000. It increases quadratically backwards
 from 1820, sigma ~32((year-1820)/100)^2.

 This function is array safe.
\seealso{tt2ut1,utc2tai,tai2utc,tai2tt}
\done

\function{utc2tai}
\synopsis{Convert a (M)JD in civil time (UTC) into International Atomic Time}
\usage{tai=utc2tai(JD)}
\description
This routine converts a Julian Date that is in civil time (Coordinated
Universal Time, UTC) into International Atomic Time (TAI) by applying
the relevant correction for leap seconds. This routine is array safe.
\qualifiers{
\qualifier{mjd}{The argument is given in MJD, not in JD.}
\qualifier{deltat}{Return the TAI-UTC in seconds, rather than the corrected (M)JD.}
\qualifier{leapfile}{path to a file containing the leap second
          information. This file is available, e.g., from
          ftp://maia.usno.navy.mil/ser7/tai-utc.dat
          Most astronomical software systems distribute this file. The default
          is to look for it in getenv("LHEASOFT")/refdata}
}
\seealso{utc2tai,tt2tai,tai2tt}
\done

\function{UTC2UNIXtime}
\synopsis{converts Universal Time (GMT) to seconds since 1970-01-01, 0:00 UTC}
\usage{Long_Type secs = UTC2UNIXtime(Struct_Type tm);
\altusage{Long_Type secs = UTC2UNIXtime(Integer_Type Y[, m[, d[, H[, M[, S]]]]]);}
}
\description
    \code{UTC2UNIXtime} can be seen as the inverse of the \code{gmtime} function,
    operating on Coordinated Universal Time instead of dates in a local time zone,
    as \code{mktime} does, which is the inverse of \code{localtime}.

    \code{UTC2UNIXtime} can also take the components of the date \code{Y}-\code{m}-\code{d}, \code{H}:\code{M}:\code{S} UTC
    as separate arguments. The year \code{Y} is then given directly (unlike \code{tm.tm_year = Y - 1900}),
    and likewise the month \code{m} (unlike \code{tm.tm_mon = m-1}).
    For omitted arguments, \code{m=1}, \code{d=1}, \code{H=0}, \code{M=0}, and \code{S=0} are assumed.
\seealso{mktime, gmtime, localtime}
\done

\function{vacuum_to_air}
\synopsis{Convert vacuum wavelengths to air wavelengths}
\usage{Double_Type l_a[] = vacuum_to_air(Double_Type l_v[])}
\description
    Convert vacuum wavelengths l_v (in Angstroem) to air wavelengths l_a (in Angstroem)
    for dry air at 15 degrees Celsius, 101.325 Pa, and with 450 parts per million CO2
    content. Valid in the wavelength range between 2300 to 17000 Angstroem.
\notes
    According to Equation 1 in Ciddor 1996, Applied Optics, 35, 1566:
       1e8*(l_v-l_a)/l_a = n-1 = k1/(k0-l_v^(-2)) + k3/(k2-l_v^(-2))
    => l_a = l_v / ( k1*1e-8/(k0-l_v^(-2)) + k3*1e-8/(k2-l_v^(-2)) + 1 )
       with k0 = 238.0185 * 1e-8 / Angstroem^2,
            k1 = 5792105  * 1e-8 / Angstroem^2,
            k2 = 57.362   * 1e-8 / Angstroem^2,
            k3 = 167917   * 1e-8 / Angstroem^2.
\qualifiers{
\qualifier{verbose [=1]}{: Set to 0 to suppress warnings.}
}
\example
    l_a = vacuum_to_air([2000,4000,8000,16000]);
\seealso{air_to_vacuum}
\done

\function{vector_astro}
\synopsis{vector initializer that is more appropriate for astronomy}
\usage{Vector_Type vector_astro(Double_Type x, y, z)}
\altusage{Vector_Type vector_astro(Double_Type r, phi, theta; sph)}
\altusage{Vector_Type vector_astro(Double_Type r, lon, lat; astro)}
\altusage{Vector_Type vector_astro(Double_Type phi, theta; sph)}
\altusage{Vector_Type vector_astro(Double_Type lon, lat; astro)}
\qualifiers{
\qualifier{astro}{consider the given coordinates to be astronomical (r, lon, lat)
                    following the astronomical definition (i.e., lon ranges from
                    0 to 2pi, lat from -pi/2 to pi/2; default for two arguments)}
\qualifier{sph}{consider the given coordinates to be spherical (r, phi, theta)
                  following the mathematical definition (i.e., theta ranges from
                  0 to pi)}
\qualifier{deg}{interpret angular arguments in degrees}
}
\description
  The components of the vector are returned within the Vector_Type
  and are accessible like a structure with the fields x, y, and z. 

  If the astro qualifier is given, cartesian coordinates are
  calculated as
#v+
    [x, y, z] =
        r * [cos(lon)*cos(lat), sin(lon)*cos(lat), sin(lat)]
#v-
  If the sph qualifier is given, the cartesian coordinates are
  calculated as
#v+
    [x, y, z] =
        r * [cos(phi)*sin(theta), sin(phi)*sin(theta), cos(theta)]
#v-

 If the function is called with only two arguments, then these represent the
 spherical or astronomical coordinates, and we assume that r=1.

 If the initializer is called with a single argument of type Vector_Type,
 the corresponding Vector_Type is returned

\seealso{Vector_Type,Matrix33_Type,vector_to_spherical,dms2deg,hms2deg}
\done

\function{vector_to_spherical}
\synopsis{Returns the spherical coordinates corresponding to a 3D Vector}
\usage{(r,lon,lat)=vector_to_spherical(Vector_Type v)}
\qualifiers{
\qualifier{astro}{return coordinates using the astronomical convention (the default)}
\qualifier{sph}{return coordinates (r,theta,phi) using the mathematical definition
                  (i.e., theta ranges from 0 to pi)}
\qualifier{deg}{return angular arguments in degrees (default: radian)}
}

\description
   This function returns the spherical coordinates corresponding to a vector.
   See the description of function vector_astro for the relevant equations.
   

\seealso{Vector_Type,vector_astro,dms2deg,hms2deg}
\done

\function{Vector_Type=matrix33_get_diag(Matrix33_Type)}
\synopsis{return the diagonal elements of a 3x3 matrix}
\usage{ dia=matrix33_get_diag(m);}
\description
  Return the diagonal elements of a Matrix33_Type as a Vector_Type.
  Also available through the accessor function m.diag

\seealso{vector,Vector_Type, matrix33_new}
\done

\function{Vector_Type=matrix33_get_trace(Matrix33_Type)}
\synopsis{return the trace of a 3x3 matrix}
\usage{trac=matrix33_get_trace(m);}
\description
  Return the sum of the diagonal elements of a Matrix33_Type
  Also available through the accessor function m.trace

\seealso{vector,Vector_Type, matrix33_new}
\done

\function{Voigt (fit-function)}
\synopsis{implements the bare convolution of a Gaussian and a Lorentzian}
\description
    The \code{Voigt} fit-function uses the bare parameters of the
    convolved Gaussian and Lorentzian profiles G and L:

    \code{G(x; sigma) = 1/[sqrt(2 pi)*sigma] * exp(-(x/sigma)^2/2)}\n
    \code{L(x; gamma) = gamma/pi * 1 / [x^2 + gamma^2]}\n
    \code{V(x; sigma, gamma) = int dt G(t; sigma) * L(x-t; gamma)}

    Unlike ISIS' internal \code{voigt} fit-function,
    which implements a Voigt profile in astrophysical context
    associating enerigies with \code{x} in the above formula,
    \code{Voigt} uses \code{x} in wavelength units or any unit
    associated with the "spectrum" through \code{define_counts}.
\seealso{voigt, voigt_profile}
\done

\function{Voigt_complex}
\synopsis{Compute complex Voigt profile}
\usage{Complex_Type[] = Voigt_complex(z, z0, sigma, gamma);}
\done

\function{voigt_profile}
\synopsis{computes a normalized Voigt profile}
\usage{Double_Type voigt_profile(Double_Type x, [[N,] x0,] sigma, gamma)}
\description
    The Voigt profile V is the convolution
    of a Gaussian profile (with width \code{sigma>0})\n
       \code{G(x; sigma)  =  exp{-x^2/(2 sigma^2)} / [sqrt(2 pi) sigma]}\n
    and a Lorentzian profile (with width gamma>0)\n
       \code{L(x; gamma)  =  gamma/pi / (x^2 + gamma^2)}\n
    i.e.:\n
       \code{V(x; sigma, gamma)  =  int G(t; sigma) * L(x-t; gamma) dt}

    The \code{voigt_profile} function computes\n
       \code{N * V(x-x0; sigma, gamma)}\n
    i.e., \code{x0} is the center of the Voigt profile (default: 0)
    and \code{N} is its normalization (default: 1), i.e.\n
       \code{int voigt_profile(x, N, x0, sigma, gamma) dx  =  N} .

    Do not confuse this "Voigt profile" with the "Voigt function H(a, u)".
\seealso{voigt, get_cfun2}
\done

\function{vradbinary (fit-function)}
\synopsis{describes the radial velocity in a binary system}
\description
    Note that only bin_lo is taken into account.
\seealso{radial_velocity_binary}
\done

\function{vsop87}
\synopsis{calculates high precision planetary positions using the VSOP87 theory}
\usage{Struct_Type vsop87(JD,planet;qualifiers)}
\description
 This routine implements the planetary theory VSOP87 of
 Bretagnon and Francou (1988, A&A 202, 309).
 The routine can return high precision planetary positions for all planets
 for a given (array) of times (in TT).

 Depending on the qualifiers the routine can return rectangular coordinates
 (X,Y,Z) or ecliptical coordinates (longitude, latitude, distance) in a
 heliocentric coordinate system either for epoch J2000 or for the epoch
 of date. In addition, elliptical orbital elements and standard orbital
 elements can also be calculated.

 By default the routine returns rectangular heliocentric coordinates in a
 structure with elements x, y, z, vx, vy, vz.
 Here x, y, z are in an ecliptical coordinate system for the epoch J2000,
 where the x-axis is in the direction of the vernal equinox, the z-axis
 points to the ecliptical North pole, and the y-axis forms a right handed
 coordinate system with the x- and z-axes. The default units are AU and AU/day.

 For heliocentric coordinates the planet can have the values
 "Mercury", "Venus", "Earth", "Mars", "Jupiter", "Saturn",
 "Uranus", and "Neptune". For heliocentric rectangular coordinates
 the value "Earth-Moon" will return the position of the
 Earth-Moon barycenter.

 In the case of solar system barycentric data, it is not possible to
 obtain the elements of the Earth-Moon barycenter, only the coordinates
 of the Earth can be queried. In addition, for this set of
 coordinates it is also possible to request the rectangular
 position of the Sun.

 For the years 1900-2100 interval the precision of the positions
 is at the level of a few milli-arcseconds, with the exception of
 Saturn, for which a precision of "only" 0.1'' is reached.

 The precision is better than 1" for the following time intervals
 around AD2000:
 Mercury, Venus, Earth-Moon Barycenter, Mars: 4000 years
 Jupiter, Saturn: 2000 years
 Uranus, Neptune: 6000 years

 The time argument to the routine is the Julian Date in Terrestrial Time (TT),
 which corresponds to the international atomic time TAI+32.184s.

 This is a powerful routine with a large number of options. A set of helper
 functions provide simpler interfaces.

\qualifiers{
 \qualifier{mjd}{The argument is in MJD(TT), not in JD(TT)}
 \qualifier{tdb}{The argument is in TDB, not TT}
 \qualifier{barycentric}{Return coordinates with respect to the solar system barycenter rather
                    than heliocentric coordinates. This implies that coordinates are for J2000.}
 \qualifier{spherical}{Return spherical coordinates in the ecliptical coordinate system.
                       Only possible for heliocentric data.
                       In this case the structure returned contains the elements
                       lat, lon, r, latdot, londot, rdot, where lat and lon are the ecliptical
                       latitude and longitude (in rad), r is the distance in AU, and
                       latdot, londot, rdot are their time derivatives (in rad/day or
                       AU/day, respectively.)}
 \qualifier{mean_equinox}{Coordinates are given in the frame defined by the
                   mean equinox and ecliptic of the date. In VSOP87, this
                   system is derived from the J2000 one by pure precessional
                   motion. The default is to return J2000 coordinates.}
 \qualifier{elements}{Return the orbital elements. The structure contains
                   the following tags:
                      a: semi-major axis (in AU)
                      l: mean longitude (rad)
                      k=e cos(p): elliptical element k
                      h=e sin(p): elliptical element h
                      q=sin(g) cos(omega): elliptical element q
                      p=sin(g) sin(omega): elliptical element p
                      e: orbital eccentricity
                      plon: perihelion longitude (in rad)
                      g: semi inclination (rad)
                      i: inclination (rad)
                      omega: longitude of the ascending node (also called G)
                 }
 \qualifier{deg}{All angle quantities to be returned are in deg, or deg/day.}
 \qualifier{mks}{All distances returned by the function are given in meters or m/s, depending
                 on the deg qualifier, angles are returned in rad and rad/s or deg and deg/s.}
 \qualifier{cgs}{Dito, but use centimeters instead of meters.}
 \qualifier{vsopfile}{Path to file containing the VSOP quantities; only necessary if
                 this is a non-standard installation of isisscripts. This qualifier is
                 only used if the vsop87 data have not yet been read.
                 By default, the file with the coefficients is searched for in the
                 directory pointed to by the environment variable ISISSCRIPTS_REFPATH.}
 \qualifier{forcearray}{Force the tags in the returned struct to be arrays. The default is
                 that they are arrays only if JD is an array.}
}
\seealso{planetpos,jpl_eph,jpl_eph_vec}
\done

\function{v_relativistic_to_optical}
\synopsis{Convert relativistic velocity to optical velocity.}
\usage{v_relativistic_to_optical(V_rel);}
\description
    Calculate the optical velocity definition from a given relativistic velocity.
    Velocities must be given in km/s.
\done

\function{wcs_axes_plot}
\synopsis{gets WCS coordinates of axes and returns xfig plot object}
\usage{xfig_plot_object = wcs_axes_plot (String_Type image_filename); }
\altusage{xfig_plot_object = wcs_axes_plot (image, wcs); }
\qualifiers{
\qualifier{width}{set plot width}
\qualifier{height}{[=14] set plot height, unless width and height are both
                     set, the other one is calculated based on the
                     format of the image (currently CDELT1 != CDELT2
                     is not taken into account, but only #pixels)}
\qualifier{x1label}{[=WCS of major] set ticlabels of x1-axis}
\qualifier{x2label}{[=0] set ticlabels of x2-axis}
\qualifier{y1label}{[=WCS of major] set ticlabels of y1-axis}
\qualifier{y2label}{[=0] set ticlabels of y2-axis}
\qualifier{axis_color}{[="white"] set the color of the axes}
\qualifier{x1major}{set WCS values for major tics of x1-axis.
                     similar qualifiers exist for x2, y1, and y2-axes.
                     unless x1 and x2 are both set, the qualifier affects
                     both x-axes. the same holds for the y-axes and the
                     minor qualifiers.}
\qualifier{x1minor}{set WCS values for minor tics of x1-axis.
                     similar qualifiers exist for x2, y1, and y2 axes}
\qualifier{return_axes}{set qualifier to additionally return
                     data (Assoc_Type) including axes information.
                     the returned object \code{wcsaxes} can be accessed
                     with \code{wcsaxes["x1"]} (keys are ["x1","x2","y1","y2"])}
\qualifier{info}{print the WCS (RA-DEC) ranges of the axes
                     (helpful for setting minor and major by hand)}
}
\description
    Creates an xfig plot object based on a sky image with WCS coordinates
    and determines the major and minor tics as well as the tic labels.
    The underlying coordinates in the xfig plot are pixels (with integers
    corresponding to the pixel centers).
    The tic marks of each axis correspond to the "leading" (most varying)
    coordinate along this axis (see wcs.xtype). There can/will be problems
    when poles (and similar things) are within the field of view.
    It is possible (and recommended) to specify major and minor tics and
    the ticlabels (e.g., in order to use sexagesimal labels) of each axis
    using the available qualifiers.
    The alternative usage allows to specify the image and the wcs
    information separately. They can be given either as a string of the
    filename or directly as the image (array) and the WCS struct. In the
    latter case they can be modified before.

    WARNINGS:
     - check WCS coordinates (the wcsfuns module uses less keywords
       than, e.g., ds9)
     - when plotting sub-images (i.e., cutting the image before
       calling this function), make sure that the WCS structure is
       consistent (e.g., modified center pixel)
\example
    variable filename = "img.fits";
    variable p = wcs_axes_plot (filename);
    p.plot_png (log(img); cmap="ds9b");
    p.render("img.pdf");

    % if, e.g., WCS has to be modified/corrected:
    variable wcs = fitswcs_get_img_wcs(filename);
    wcs.cdelt[1] *= -1; % can be necessary if a keyword was not read
    variable p = wcs_axes_plot ( filename, wcs );

    % use sexagesimal axis labels (example for declination from 44 8' - 44 20')
    variable dec_major = [8:20:2]; % arcmin
    variable y1major = 44 + dec_major / 60.0;
    variable y1minor = 44 + [8:20:1] / 60.0; % minor in 1' steps
    variable dec_ticlabels = array_map (String_Type, &sprintf, "$44^\\circ\\,%02d'$", dec_major);
    variable p = wcs_axes_plot (filename; y1major = y1major, y1minor = y1minor, y1label = dec_ticlabels);
    
\done

\function{Weibull (fit-function)}
\description
    \code{W(x) = norm * k/lambda * ((x-x0)/lam)^(k-1) * exp(-((x-x0)/lam)^k);}\n
    \code{W( x0 + lambda * (1-1/k)^(1/k) )  =  max.}
\done

\function{weighted_mean}
\usage{Double_Type m = weighted_mean(x, w);
\altusage{Double_Type m = weighted_mean(x; err=err);  % or weighted_mean(x, err; err)}
}
\description
    \code{m = sum(w*x) / sum(w);}\n
    \code{%} for Gaussian errors:\n
    \code{w = 1/err^2;}
\done

\function{where_max}
\synopsis{finds the indices of an array where the values are maximal}
\usage{Integer_Type i[] = where_max(Array_Type a);}
\description
    \code{i = where(a==max(a));  % =>  a[i] == max(a)}
\seealso{where_min}
\done

\function{where_min}
\synopsis{finds the indices of an array where the values are minimal}
\usage{Integer_Type i[] = where_min(Array_Type a);}
\description
    \code{i = where(a==min(a));  % =>  a[i] == min(a)}
\seealso{where_max}
\done

\function{wienhump (fit-function)}
\synopsis{describes a Wien hump}
\description
    Describes a Wien hump with peaks at 3kT.
\seealso{bbody}
\done

\function{winding_number_polygon}
\usage{wn=winding_number_polygon(P,V)}
\synopsis{return the winding number for a point P in a polygon}
\description
 return the winding number for a point P in a (potentially
 complex polygon V)

 The winding number counts the number of times the polygon V winds
 around point P. The point is outside if the winding number is 0.

 The algorithm used here is as efficient as the determination
 of the crossing number.

 The point P is a struct{x,y}, while the polygon is a
 struct{x[],y[]} where the arrays are the x- and y-coordinates.
 The polygon has to be closed, i.e. V.x[n]==V.x[0] and
 V.y[n]==V.y[0] where n is the number of polygon points.

 See the URL below for more explanations.

 Based on code by Dan Sunday,
 http://geomalgorithms.com/a03-_inclusion.html

\seealso{crossing_number_polygon,point_in_polygon,simplify_polygon}
\done

\function{WorldCoastLine}
\usage{Struct_Type WCL[] = WorldCoastLine();}
\description
    \code{WCL} is an array of structures with \code{x} and \code{y} fields
    which specify the coordinates (longitude east and lattitude north)
    in degrees of the World's coast lines.
\example
    xrange(-2, 2); yrange(-1, 1);
    variable cl;
    foreach cl (WorldCoastLine())
      oplot( Hammer_projection(cl.x, cl.y; deg, normalized) );
\done

\function{write_plot}
\synopsis{Write the data from the last plot made with the isis_fancy_plots package }
\usage{[pd =] write_plot(root; head=0, data);}
\qualifiers{
\qualifier{head}{==0 to suppress header in output file}
\qualifier{data}{if set, return data as a structure instead}
}
\description

 [pd =] write_plot(root; head=0, data);

 Creates ASCII files with the data from the last plot made (with any
 plot device) using plot_counts, plot_data, plot_residuals, or
 plot_unfold.  Data will be stored in columns in the following order-
 bin_lo, bin_hi, data_value, data_error (single column), model,
 {mean residual, residual-1sigma, residual+1sigma}.  There can be
 multiple sets of columns for the groups of residuals if the data are
 combined, but the residuals are not.

 Important notes:

   The X-axis and Y-axis units will reflect those of the plot. The
   binning and noticed data ranges will reflect data filters applied
   within ISIS, but not the (usually more limited) plot range filters.

   Each uncombined data set, or group of combined data sets, will be
   be written to a separate ASCII file.  A file for combined data sets
   can have multiple columns for residuals if they are left uncombined.

   Combined data or residuals will only write out the chosen
   combinations, not the individual pieces used to create them.

   Files created from plot_residuals will only output the bin_lo/hi
   grid and the residual/-/+ columns, not the data or model.

   If the model or residuals were not plotted, they will not be
   written to the ASCII file.

   Plots made from plot_unfold will create separate files for the
   data and the unfolded model, as they can be on different grids.
   The model file will begin with the appropriate bin_lo, bin_hi.

 Inputs:

   root- A string with the base file name.  Outputs will be--
         root_#.dat, root_#.res (for plot residuals), and root_#.mod (for
         plot_unfold, if the data and model are on separate grids). # will
         will cycle from 0 to the number of input data groups-1, assigning
         data sets in the order that they were input to the plot command.
         ***Old files will be overwritten if 'root' is not a unique name.***

   head- Set as a qualifier only.  If !=0 (default=1), the ASCII data
         files will have useful header information appended, otherwise,
         the files will just contain columns of data.

   data- If the data qualifier exists, instead of writing an ASCII file, the 
         write_plot function will return a structure with the data values from 
         the last plot generated.
\seealso{pg_info}
\done

\function{write_simput_dummy_bkg}
\synopsis{plots a sky image, including an optional scale and/or grid}
\usage{write_simput_dummy_bkg(fsimput,fsrc,fpar);}
\description
    fsimput:  Name of the ouput simput-File
    fsrc:     Name of a SRC-FITS-Image (with FK5 WCS Header)
    fpar:     ISIS-par File (with proper normalization) of the BKG
\qualifiers{
\qualifier{src_name}{["BKG"] name of the source in the SIMPUT file}
\qualifier{overwrite}{if given, already exsiting simput files are overwritten}
\qualifier{arcmin_area}{=[1.0] area in arcmin specifying the flux for the given spectrum }
}
\seealso{xfig_plot_new,fitswcs_get_img_wcs}
\done

\function{write_slurm_script}
\synopsis{writes a slurm job script, which can be submitted with squeue}
\usage{write_slurm_script(file,jobname,walltime,cmds);}
\qualifiers{
\qualifier{queue}{set the queue (remeis by default)}
\qualifier{silent}{do not show any output}
\qualifier{serial}{instead of parallel, execute commands one after the other}
\qualifier{option}{setting commands or environment variables before executing the actual commands}
\qualifier{ntaks}{[=1] number of tasks per CPU}
\qualifier{memory}{[=1000] MByte in memory allocated (--mem-per-cpu in slurm)}
\qualifier{output}{output logfile name}
\qualifier{error}{error logfile name}
\qualifier{account}{if the selected queue needs a specific account set this qualifier accordingly}
\qualifier{dir}{[=getcwd()] absolut path where the script should be run (default is cwd)}
}
\description
    Writing a slurm job script, which can be submitted with squeue.
    The function might yet not include all options and works best for
    simple cases. The cmds variable is a String_Type array, with each
    field being one command line. The 
\example
    % define your commands
    variable cmds = 
      [ "echo 123" , "echo 456" ]; 
    % call the function
    variable file = "slurmscript.slurm";
    variable jobname = "script";
    variable walltime = "00:30:00";
    write_slurm_script(file,jobname,walltime,cmds);
\done

\function{write_spix}
\synopsis{writes the structure provided by make_spix in a FITS file}
\usage{write_spix(Stuct_Type \code{spix}, String_Type \code{filename});}
\seealso{make_spix, read_spix, plot_spix}
\done

\function{x1axis}
\synopsis{(re)draws a first x-axis}
\usage{x1axis(Double_Type min_value, Double_Type max_value[, Double_Type step]);}
\qualifiers{
\qualifier{nsub}{[=0] number of minor tick marks within each major divison (nsub=0 => no minor ticks)}
\qualifier{maji}{[=0.5] length of major tick marks, drawn inwards (in units of the character height)}
\qualifier{majo}{[=0] length of major tick marks, drawn outwards (in units of the character height)}
\qualifier{fmin}{[=0.5] length of minor tick marks (as fraction of the major tick marks)}
\qualifier{disp}{[=0.5] displacement of baseline of tick labels to the axis (in units of the character height)}
\qualifier{angle}{[=0] orientation of the text (in degrees)}
\qualifier{color}{[=1] color of the axis}
\qualifier{linewidth}{[=1] line width of the axis}
\qualifier{opt}{[="N"/"LN"] pgaxis-options: "L" (log), "N" (numbers), "1" (force decimal), "2" (force EE)}
\qualifier{yrel}{[=0] relative position of the x1axis (0=bottom, 1=top)}
\qualifier{xmin}{[=xmin] absolute x-position of the axis}
\qualifier{xmax}{[=xmax] absolute x-position of the axis}
}
\seealso{x2axis (and references therein)}
\done

\function{x2axis}
\synopsis{draws a second x-axis}
\usage{x2axis(Double_Type min_value, Double_Type max_value[, Double_Type step]);}
\qualifiers{
\qualifier{nsub}{[=0] number of minor tick marks within each major divison (nsub=0 => no minor ticks)}
\qualifier{maji}{[=0.5] length of major tick marks, drawn inwards (in units of the character height)}
\qualifier{majo}{[=0] length of major tick marks, drawn outwards (in units of the character height)}
\qualifier{fmin}{[=0.5] length of minor tick marks (as fraction of the major tick marks)}
\qualifier{disp}{[=0.5] displacement of baseline of tick labels to the axis (in units of the character height)}
\qualifier{angle}{[=0] orientation of the text (in degrees)}
\qualifier{color}{[=1] color of the axis}
\qualifier{linewidth}{[=1] line width of the axis}
\qualifier{opt}{[="N"/"LN"] pgaxis-options: "L" (log), "N" (numbers), "1" (force decimal), "2" (force EE)}
\qualifier{yrel}{[=1] relative position of the x2axis (0=bottom, 1=top)}
\qualifier{xmin}{[=xmin] absolute x-position of the axis}
\qualifier{xmax}{[=xmax] absolute x-position of the axis}
}
\description
    \code{x2axis} has to be called after the plot window is drawn.
    It relies on the world coordinates set by \code{xrange}, \code{yrange}.
    The automatical x2axis can be switched off with \code{change_plot_options}.
\example
    ()=change_plot_options(; xopt="BNST");  % default: "BCNST", see _pgbox
    xrange(0, 360); yrange(-1.1, 1.1);
    plot([0:360], sin([0:2*PI:#361]));
    x2axis(0, 2);
\seealso{_pgaxis, change_plot_options, coords_in_box, y2axis, x1axis, y1axis}
\done

\function{x2axis_with_tics}
\usage{x2axis_with_tics(Double_Type X[][, Label]);}
\qualifiers{
\qualifier{ftick}{[=0.08]: fraction of box-height that tick marks are below axis}
\qualifier{flabel}{[=0.05]: fraction of box-height that labels are above axis}
}
\done

\function{x2label}
\synopsis{labels the second x-axis}
\usage{x2label(String_Type s);}
\qualifiers{
\qualifier{color}{[=1] textcolor}
\qualifier{f}{[=0.03] fraction of the plot-area's height that the label is above}
}
\seealso{xylabel}
\done

\function{xfigplot_hardnessratio_grid}
\synopsis{plots hardness-grid and/or hardness-datapoints}
\usage{xfigplot_hardnessratio_grid(Struct_Type tracks);
 or xfigplot_hardnessratio_grid(Struct_Type tracks, String_Type fits_hr_table
    );}
\qualifiers{
\qualifier{outputdir}{String_Type, name of output directory,
          default: 'testplot.pdf'}
\qualifier{W}{Integer_Type, width of new xfig-plot}
\qualifier{H}{Integer_Type, height of new xfig-plot}
\qualifier{xrange}{Double_Type[2], range on x-axis}
\qualifier{yrange}{Double_Type[2], range on y-axis}
\qualifier{padx}{Double_Type, percentage of padding of whole plot in x direction,
          default: 0.05}
\qualifier{pady}{Double_Type, percentage of padding of whole plot in y direction,
          default: 0.05}
\qualifier{xlabel}{String_Type, label on x-axis}
\qualifier{ylabel}{String_Type, label on y-axis}
\qualifier{par1label}{String_Type, label within plot of par1}
\qualifier{par1label_x}{Double_Type, x-position of 'par1label' as percentage 
          (needs to be set, if 'par1label' is given)}
\qualifier{par1label_y}{Double_Type, y-position of 'par1label' as percentage
          (needs to be set, if 'par1label' is given)}
\qualifier{par1label_c}{Integer_Type/String_Type, color of 'par1label' as RGB,
          default: 0x000000 (black)}
\qualifier{par2label}{String_Type, label within graph of par2}
\qualifier{par2label_x}{Double_Type, x-position of 'par2label' as percentage
          (needs to be set, if 'par2label' is given)}
\qualifier{par2label_y}{Double_Type, y-position of 'par2label' as percentage
          (needs to be set, if 'par2label' is given)}
\qualifier{par2label_c}{Integer_Type/String_Type, color of 'par2label' as RGB,
          default: 0x000000 (black)}
\qualifier{extralabel}{String_Type, extra label within graph}
\qualifier{extralabel_x}{Double_Type, x-position of 'extralabel' as percentage
          (needs to be set, if 'extralabel' is given)}
\qualifier{extralabel_y}{Double_Type, y-position of 'extralabel' as percentage
          (needs to be set, if 'extralabel' is given)}
\qualifier{extralabel_c}{Integer_Type/String_Type, color of 'extralabel' as RGB,
          default: 0x000000 (black)}
\qualifier{extralabel2}{String_Type, extra label within graph}
\qualifier{extralabel2_x}{Double_Type, x-position of 'extralabel2' as percentage
          (needs to be set, if 'extralabel' is given)}
\qualifier{extralabel2_y}{Double_Type, y-position of 'extralabel2' as percentage
          (needs to be set, if 'extralabel' is given)}
\qualifier{extralabel2_c}{Integer_Type/String_Type, color of 'extralabel2' as RGB,
          default: 0x000000 (black)}
\qualifier{cstart}{Integer_Type/String_Type, start color of grid as RGB,
          default: 0xC1CDCD (gray)}
\qualifier{cstop}{Integer_Type/String_Type, stop color of grid as RGB}
          default: 0xC1CDCD (gray)}
\qualifier{lend1}{if exists, switch end (or begin) of tracks1 
          (where each tracks' value will be assigned)}
\qualifier{lend2}{if exists, switch end (or begin) of tracks2
          (where each tracks' value will be assigned)}
\qualifier{shift1x}{Double_Type, x-position of par1-values at end of track, default: 0.}
\qualifier{shift1y}{Double_Type, y-position of par1-values at end of track, default: 0.}
\qualifier{shift2x}{Double_Type, x-position of par2-values at end of track, default: 0.}
\qualifier{shift2y}{Double_Type, y-position of par2-values at end of track, default: 0.}
\qualifier{hr_x_col}{String_Type, name of hr_x-column ('fits_hr_table') for plotting datapoints}
\qualifier{hr_y_col}{String_Type, name of hr_y-column ('fits_hr_table') for plotting datapoints}
\qualifier{err_x_col}{String_Type, name of error_x-column ('fits_hr_table') for plotting datapoints}
\qualifier{err_y_col}{String_Type, name of error_y-column  ('fits_hr_table)' for plotting datapoints}
\qualifier{err_c}{Integer_Type/String_Type, color of errorbars as RGB
          default: 0x000000 (black)}
\qualifier{symbol_shape}{String_Type, shape of datapoints}
\qualifier{symbol_size}{Double_Type, size of datapoints, default: 1.}
\qualifier{symbol_cstart}{Integer_Type/String_Type, (start) color of datapoints as RGB}
\qualifier{symbol_cstop}{Integer_Type/String_Type, stop color of datapoints as RGB,
          if not given 'symbol_cstart' is color of all}
\qualifier{source_col}{String_Type, name of source-column ('fits_hr_table') for plotting datapoints,
          needs to be given only if different colors for different sources}
\qualifier{extralabel_sources_x}{Double_Type, x-position of source_names as percentage
          (if given, source names are displayed in their respective color)}
\qualifier{extralabel_sources_y}{Double_Type, y-position of source_names as percentage
\qualifier{tracks1}{Integer_Type, if given, only tracks1 are plottet}
\qualifier{tracks2}{Integer_Type, if given, only tracks2 are plottet}
\qualifier{notracks}{Integer_Type, if given, no tracks are plottet, 'fits_hr_table' must be
          given for datapoints}
}
\description
	- for only grid: give only tracks
	- for grid with datapoints: give tracks and FITS-table with hardnessratios (and errors)
       - for only datapoints: give tracks and FITS-table, but use 'notracks'-qualifier
\seealso{hardnessratio_from_dataset}
\done

\function{xfigplot_pulseprofile_energy_map}
\synopsis{returnes an Xfig plot object containing the pulse-profile-energy-map}
\usage{Xfig_Object xfigplot_pulseprofile_energy_map(Struct_Type map);}
\description
  Struct_Type map = struct{
     pgrid = struct{ bin_lo = Double_Type[np],
                     bin_hi = Double_Type[np]
                   },
     egrid = struct{ bin_lo = Double_Type[ne],
                     bin_hi = Double_Type[ne]
                   },
     map = Double_Type[ne,np]
  };
     
\example
\seealso{pulseprofile_energy_map}
\done

\function{xfig_3d_orbit_on_cube}
\synopsis{Visualize 3d orbits by projecting them onto a cube}
\usage{xfig = xfig_3d_orbit_on_cube(Struct_Type orbits [, Struct_Type text]; qualifiers)}
\description
    The orbits have to be given in cartesian coordinates (x,y,z) and are passed
    to the function via a structure. Each individual orbit is itself a structure
    with fields 'x', 'y', and 'z'. The orbits are then visualized by plotting the
    projections to the xy-, xz-, and yz-plane on the surfaces of a cube. Use the
    qualifiers to change the viewing angle, labels, or properties of the cube and
    the background grid. The function's optional second argument can be used to
    add text or symbols to the plot (see example section below).
\notes
    If one of the three coordinates is missing, the projection on the plane defined
    by the remaining two coordinates is still plotted. This feature can be used to
    add lines or text to one specific plane only.

    The properties of the orbits (line style, color, ...) can be changed by adding the
    field 'qualies' to the structure that decribes the orbit. The field 'qualies' is
    again a structure whose fields are qualifiers of the function 'xfig_new_polyline'.
\qualifiers{
\qualifier{phi [=-45]}{Azimuthal angle in the x-y-plane in degrees (-90 < phi < 0).}
\qualifier{theta [=60]}{Polar angle from the z-axis in degrees (0 < theta < 90).}
\qualifier{dist [=1e6]}{Distance of the eye from the focus.}
\qualifier{scale [=4]}{Factor to scale dimensions with respect to the character size (scale > 0).}
\qualifier{x_label [="$x$\\,(kpc)"]}{x-label.}
\qualifier{y_label [="$y$\\,(kpc)"]}{y-label.}
\qualifier{z_label [="$z$\\,(kpc)"]}{z-label.}
\qualifier{digits [=1]}{Digits for ticmarks (a non-negative integer).}
\qualifier{cube}{Modify the cube by providing a structure whose fields are qualifiers of the function 'xfig_new_polyline'.}
\qualifier{grid}{Modify the grid by providing a structure whose fields are qualifiers of the function 'xfig_new_polyline'.}
\qualifier{center}{Center the cube at (0,0,0).}
\qualifier{z_adjust}{Use separate scale for z-axis.}
}
\example
   % basic example:
   o = struct{ orbit1, orbit2, orbit3 };
   o.orbit1 = struct{ x=[0,1], y=[0,1], z=[0,1] };
   o.orbit2 = struct{ x=[1,1], y=[4,1], z=[-3,0], qualies=struct{color="blue"} };
   o.orbit3 = struct{ x=[-1,0,1], y=[-1,1,-1], qualies=struct{closed, fillcolor="tomato", depth=5} }; % filled triangle in the xy-plane
   xfig = xfig_3d_orbit_on_cube(o);
   xfig.render("test.pdf");
   xfig_set_eye(1e6,0,0,0); % restore the default position of the eye

   % adding text to xy-plane:
   t = struct{ triangle=struct{text="triangle", x0=0, y0=0, depth=4} };
   xfig_3d_orbit_on_cube(o,t).render("test.pdf");
   xfig_set_eye(1e6,0,0,0); % restore the default position of the eye

   % example involving the function 'orbit_calculator':
   o = orbit_calculator([12,12],[22,22],[29.6,29.6],[40,40],[49,49],[36,36],[3.078,4],[262,262],[-13.52,-13.52],[16.34,16.34],-1000; r_disk=50, set);
   set_struct_field(o.tr, "o0", struct_combine(o.tr.o0, "qualies")); set_struct_field(o.tr.o0, "qualies", struct{ color="red", depth=2, backward_arrow });
   set_struct_field(o.tr, "o1", struct_combine(o.tr.o1, "qualies")); set_struct_field(o.tr.o1, "qualies", struct{ color="blue", depth=1, backward_arrow });
   xfig_3d_orbit_on_cube(o.tr).render("test.pdf");
   xfig_set_eye(1e6,0,0,0); % restore the default position of the eye

   % changing the appearance of the cube and grid:
   xfig_3d_orbit_on_cube(o.tr; cube=struct{color="red",width=2}, grid=struct{line=0}).render("test.pdf");
   xfig_set_eye(1e6,0,0,0); % restore the default position of the eye

   % adding symbols:
   t = struct{ sun, gc };
   t.sun = struct{ text="$\\odot$", x0=-8.4, y0=0, z0=0, depth=3 };
   t.gc = struct{ text="+", x0=0, y0=0, z0=0 };
   xfig_3d_orbit_on_cube(o.tr, t).render("test.pdf");
   xfig_set_eye(1e6,0,0,0); % restore the default position of the eye
\seealso{orbit_calculator}
\done

\function{xfig_compress_eps}
\synopsis{Compresses an EPS-File via "pdftops -level3 -eps"}
\usage{int = xfig_compress_eps(String_Type file)}
\description
    PS and EPS-Files created with xfig are usually much too large.
    Using this function the size can be reduced by a factor of few. Simply
    call this function after creating a PS or EPS with ".render(file)".
\done

\function{xfig_draw_orbit}
\synopsis{draws the orbit of a binary system into an exsiting xfig-object and
    allow also to highlight specific phases}
\usage{xfig_draw_orbit(Xfig_Object xf, Double_Type asini, i, ecc, omega, rstar)
}
\qualifiers{
\qualifier{line_color}{[\code{="gray"}] color of the lines showing the
           coordinate main axes and the orientation of the semi-major axis of
           the orbit}
\qualifier{phases}{[\code{=\{\}}] list of Double_Type[2] arrays indicating
           the phases which should be highlighted}
\qualifier{scale}{[\code{=1.05}]  compression/streching for the highlighted
           phases over/under the orbit}
\qualifier{phase_color}{[\code{="red"}] color of the highlighted phases,
           can be a single string or an array of strings, with the same length
           as phases so that every highlighted phase can be drawn in
           a different color.}
\qualifier{phase_width}{[\code{=2}] width of the highlighted phases, can
           also be an array, just as phase_color.}
\qualifier{star_color}{[\code{=lightblue}]  fill-color of the companion star}
\qualifier{trueanom}{if set, phases are given in true anomaly, i.e., equal
           delta-phi values will correspond to equal delta-t on the
           orbit.}
\qualifier{zerolines}{[\code{=1}] plot x- and y-axis lines through [0,0]}
\qualifier{phaselines}{draw a lightgray line every phase n*0.1 and label it}
\qualifier{label_phase_switch}{[\code{=0}] : phase at witch phase labels
           are switched from before the line to after the line.}
\qualifier{label_pos}{[\code{=0.85}] distance of the phase labels from the 
           orbit line, in fraction of distance to center.}
\qualifier{nolabel} { : removes the coordinate system }
}
           
\description
    This function plots the orbit of a binary system as seen from
    above into a given xfig-object. The orbital parameters asini
    (semi-major axis, in lt-sec), i (inclination, in deg), ecc (eccentricity),
    omega (argument of periastron, in deg) and the radius rstar (in r_sun)
    of the companion star need to be supplied.
    Qualifiers are passed through the xfig-plotting routine of the
    orbit.
        
\examples
     %orbit parameters of GX 301-2 according to Koh et al (1997).
     variable asini = 368.3 ; % lt-sec (semi major axis)
      variable i = 66; % degree to rad
      variable ecc = 0.462 ;  %
      variable omega = 310.4 ; % degrees
      variable rstar = 43 ; % solar radii  in lt-sec

      variable xfgx = xfig_plot_new(12,12) ;
      xfgx.world(-600,450,-450, 600) ;
      xfig_draw_orbit(xfgx, asini, i, ecc, omega, rstar ;
        phases={[0.9184,0.9315]}, phase_color="red", phase_width=4, scale=1) ;
      xfgx.plot([0,0],[-100,-370] ; line=1, forward_arrow, depth = 10,
        arrow_thickness =2) ;
      xfgx.xylabel(0,-400, "To Earth");
      xfgx.render("plots/gx301_skizze2.eps") ;

  \seealso{ellipse}
\done

\function{xfig_draw_orbit_3D}
\synopsis{draws the orbit of a binary system in 3D}
\usage{Xfig_Object xfig_draw_orbit([Xfig_Object xf,] Double_Type asini, i, ecc, omega);}
\qualifiers{
    \qualifier{caminc}{the camera inclination to the line of sight
              (in degrees; default: 90-i)}
    \qualifier{camroll}{the camera roll angle around the z-axis (along the
              line of sight; in degress; default: 0)}
}
\description
    Similar to 'xfig_draw_orbit' this function plots the orbit of a
    binary, but seen from a user-defined direction and, thus, in 3D.
    In order to draw the orbit the projected semi-major axis (asini),
    inclination (i), eccentricity (ecc), and longitude of periastron
    (omega) have to be provided.
    
    Additionally to the orbital plane the tangent plane of the sky
    is drawn as well. This plane is defined to be perpendicular to
    the line of sight and through the center of mass, i.e. the focal
    point of the eccentric orbit nearest to the periastron (P).
    Thereby, the observer is located below the tangent plane of the
    sky, indicated by the arrow pointing to Earth.
    All dashed lines are within the orbital plane, which are the
    intersection of the orbital and tangent plane of the sky, the
    semi-major and -minor axis (in red), and the line connecting the
    orbit's center and the periastron. The projected semi-major axis
    (asini) is drawn as dotted dashed line.
    
    The camera is controlled via qualifiers and by default the binary
    is seen face-on.

    Note that so far only basic functionallity have been implemented.
\seealso{xfig_draw_orbit, ellipse}
\done

\function{xfig_get_normalized position}
\synopsis{Compute normalized (world0) x,y-position from world1 x,y-position}
\usage{xnorm=xfig_get_normalized_position(p,xval,yval)}
\description
For a given xfig plot p and coordinate values xval,yval given in the world1
coordinate system, compute the corresponding normalized world0 x,y-position.

This function is necessary since the xfig module does not export
its world normalization functions. It is heavily based on these.

\seealso{xfig_get_normalized_x_position,xfig_get_normalized_y_position}
\done

\function{xfig_get_normalized_x_position}
\synopsis{Compute normalized (world0) x-position from world1 x-position}
\usage{xnorm=xfig_get_normalized_x_position(p,xval)}
\description
For a given xfig plot p and x-coordinate value xval given in the world1
coordinate system, compute the corresponding normalized world0 x-value.

This function is necessary since the xfig module does not export
its world normalization functions. It is heavily based on these.

\seealso{xfig_get_normalized_position,xfig_get_normalized_y_position}
\done

\function{xfig_get_normalized_y_position}
\synopsis{Compute normalized (world0) y-position from world1 y-position}
\usage{ynorm=xfig_get_normalized_y_position(p,yval)}
\description
For a given xfig plot p and y-coordinate value yval given in the world1
coordinate system, compute the corresponding normalized world0 y-value.

This function is necessary since the xfig module does not export
its world normalization functions. It is heavily based on these.

\seealso{xfig_get_normalized_position,xfig_get_normalized_x_position}
\done

\function{xfig_mix_colors}
\synopsis{mix two named xfig colors}
\usage{xfig_mix_colors(color1,color2,fraction)}
\qualifiers{
\qualifier{name}{xfig name of the color mix}
}
\description
 This function mixes two named xfig colors in rgb space. The function looks up
 the rgb values of color1 and color2, and mixes their rgb values according
 to 
 new color=color1*fraction+color2*(1-fraction)
 The operations are analoguous to the color mixing performed by the xcolor
 package of LaTeX (the operation is similar to LaTeX's color1!fraction!color2
 syntax). 
 The function returns the xfig name of the new color, or nothing if the
 name qualifier is given (recommended).
\seealso{mix_rgb_colors}
\done

\function{xfig_multibox}
\synopsis{combines M x N xfig-objects in one box}
\usage{pl = xfig_multibox(XFig_Plot_Type pl[M,N] [, Double_Type spacing])}
\qualifiers{
\qualifier{rotate}{switch the meaning of M and N}
}
\seealso{xfig_new_vbox_compound, xfig_new_hbox_compound}
\done

\function{xfig_plot_allbezier}
\synopsis{adds the plot of all Bezier curves from the bezier function to a xfig plot object}
\usage{Xfig_Struct pl_new = xfig_plot_allbezier(Xfig_struct pl, bez [, conlines ]);
}
\description
   Adds the plot of all Bezier curves to a previously defined
   xfig-object 'pl'. 'conlines' and 'bez' are the values returned from
   the bezier-function called with both qualifiers "allbez" and
   "conlines".
   Colors of each Bezier line collection is changed automatically.
   If conlines are plotted, they will be plotted in "gray".
   
\seealso{bezier}
\done

\function{xfig_plot_colormap}
\synopsis{Plots a colormap/colorscale}
\usage{Struct_Type xfig_plot_new( Double_Type[n,m] IMG );}
\qualifiers{
\qualifier{cmap}{[="ds9b"] Either a name (String_Type) of a colormap
                     (see png_get_colormap_names), or a self defined
                      colormap (Integer_Type[]).}
\qualifier{gmin}{[=min(IMG)] Minimal value of image to be plotted}
\qualifier{gmax}{[=max(IMG)] Maximal value of image to be plotted}
\qualifier{W}{[=14] Plot width [cm]. 'ratio' is overwritten if W & H are given!}
\qualifier{H}{[=W/ratio] Plot height [cm]. 'ratio' is overwritten if W & H are given!}
\qualifier{orientation}{[=1] Horizontal if 1, vertical if 0. Set Horizontal if W/H > 1,
             otherwise set to vertical.}
\qualifier{label}{Label for the colormap}
\qualifier{format}{Format for colormap axis ticlabels, see 'help xfig_plot.axis'}
\qualifier{depth}{[=80] Depth of the colormap}
\qualifier{fontsize}{[="scriptsize"] Font size of tic & axis label}
\qualifier{ticlen}{[=.15] Length of major tic marks (minor tic are .5*ticlen).}
\qualifier{maxtics}{[=5] Maximal number of labeled tics}
\qualifier{box}{Draws a box behind the colormap}
\qualifier{border}{[=.01] Relative size of box excess length}
\qualifier{boxcolor}{[="#FFFFFF"] Color of the box}
}
\description
   This function uses xfig_plot_image to create a colormap based on the given
   IMaGe (with dimension n and m).
   
\example
   variable x = [PI:10*PI:#100];
   variable y = [PI:5*PI:#80];
   variable IMG = sin(x) # transpose(cos(y));
   variable xf = xfig_plot_colormap( IMG );
   xf.render("/tmp/test.pdf");

\seealso{xfig_plot_new, %.shade_region, xfig_plot_image}

\done

\function{xfig_plot_conf}
\synopsis{plots contour levels stored with save_conf}
\usage{XFig_Plot_Type pl = xfig_plot_conf(String_Type file[, XFig_Plot_Type pl])}
\qualifiers{
\qualifier{lvls}{specify the Delta chi-sqr values for the contour levels,
            default: [2.30, 4.61, 9.21], i.e., 1 sigma, 90% and 99% CL}
\qualifier{color}{specify the colors of the contour lines
           The array's last value is used for the best fit's cross (if
           length of array > number of levels).}
\qualifier{line}{[=0] specify the line styles of the contour lines}
\qualifier{width}{[=1] specify the width of the contour lines
                  The array's last value is used for the best fit's cross.}
\qualifier{smooth}{[=1] this integer>=1 specifies the interpolation factor
                   for the image used to calculate the contours}
\qualifier{W}{width of the plot if created by \code{xfig_plot_contours}}
\qualifier{H}{height of the plot if created by \code{xfig_plot_contours}}
\qualifier{worldXY}{any world qualifier to the xfig-plot will be passed,
                    which requires an existing SLxfig plot object to be given}
\qualifier{transpose}{transposes the countour map (exchange x- and y-values)}
\qualifier{nocross}{do not plot best fit cross}
\qualifier{noclip}{if used with a sltikz plot object, do not clip}
}
\description
    If no SLxfig plot object \code{pl} is given, a new one is created
    and its world coordinate system is defined according to the
    ranges used to calculate the confidence map. x- and y-labels
    are initialized with the parameter names, but can be changed
    afterwards.
\seealso{get_confmap, xfig_plot_new, enlarge_image}
\done

\function{xfig_plot_confmap}
\synopsis{Create an xfig plot of a confidence map}
\usage{xfig_plot_new fig = xfig_plot_confmap(String_Type cm; qualifiers)}
\description
    Create an xfig plot from a pre-calculated confidence map `cm' computed and saved with
    the function `get_confmap'.
\qualifiers{
\qualifier{best_fit}{Mark the best fit with a cross.}
\qualifier{CDF}{If set to a positive integer, the chi^2 values are converted to a cumulative
      distribution function with the degree of freedom given by this qualifier. For
      instance, CDF=1 implies that chi^2 values of 1, 2.71, and 6.63 are converted
      to 0.68, 0.9, and 0.99 (this qualifier requires the gsl-module).}
\qualifier{colormap [="haxby"]}{Choose, e.g., "ds9sls", "seis", "rainbow", "haxby", "topo",
      "globe" (see the function `png_get_colormap' for more information).}
\qualifier{colormap_gmin, colormap_gmax}{Minimum and maximum grayscale value for the colormap,
      see `png_gray_to_rgb'.}
\qualifier{chi2 [=Double_Type[0]]}{Chi^2 values for which contour lines should be drawn (this
      qualifier requires the gcontour-module).}
\qualifier{chi2_color [="magenta"]}{Color of the contour lines and the best fit cross.}
\qualifier{factor [=1]}{Factor by which the x- and y-sampling of the confidence map is increased.}
\qualifier{field [="chisqr"]}{Which field of the confidence map shall be plotted.}
\qualifier{width [=8], height [=8]}{Width and height of the confidence map.}
\qualifier{latex_package [="txfonts"]}{Load a package in the preamble of LaTeX documents.}
\qualifier{reverse_axes}{If present, both axes are reversed.}
\qualifier{[xyz]label}{Add a x-, y-, or z-label to the confidence map.}
\qualifier{[xyz]axis [=struct{ticlabels_confine=0, ticlabels2=0}]}{Modify the x-, y-, or z-axis
      by providing a structure whose fields are qualifiers of the function `xfig_plot.axis'.
      Note that the `log' qualifier (set logarithmic axis scale) does not work.}
\qualifier{nomultiplot}{Return the confidence map and its corresponding colormap in two separate
      xfig objects instead of one multiplot object.}
}
\example
    fig = xfig_plot_confmap("confmap.fits"; chi2=[1, 2.71, 6.63], zlabel="$\\chi^2$"R, zaxis=struct{format="%g", ticlabels1=0});
    fig.render("test.pdf");
\seealso{get_confmap}
\done

\function{xfig_plot_data}
\synopsis{plots data which was loaded by "read_data_from_write_plot"}
\usage{XFig_Plot_Type pl = xfig_plot_data(XFig_Plot_Type pl, Struct_Type dat)}
\qualifiers{
\qualifier{color}{specify the color}
\qualifier{width}{line width}
\qualifier{sym}{symbol}
\qualifier{size}{size of the symbol}
\qualifier{depth}{depth in plot}
\qualifier{pp}{data are plotted twice (pulse profile)}
}
\seealso{xfig_plot_unfold,read_data_from_write_plot,read_col,write_plot}
\done

\function{xfig_plot_distribution_matrix}
#c%{{{
\synopsis{Plot distribution matrix}
\usage{xfig_plot_distribution_matrix(Struct_Type dm[, String_Type file]);}
\qualifiers{
\qualifier{names}{string array containting the names for all dimensions}
\qualifier{nameX}{add label for dimension X}
}
\description
    Plot the distribution matrix to file \code{file} using xfig. If 
    \code{file} is not given or is \code{NULL} return instead the xfig
    object.

\example
    variable dm = distribution_matrix({v0, v1, v2}); % vX are the state values in the dimensions
    variable names = ["dim0", "dim1", "dim2"];
    xfig_plot_distribution_matrix(dm, "distribution.pdf"; names=names, name1="other label");

\seealso{distribution_matrix}
\done

\function{xfig_plot_epfpd}
\synopsis{plot the results of epfoldpdot with Xfig, including the
pulse profile of the stronges P/P-dot values}
\usage{xfig_plot_epfpd(Struct_Type eppd, Struct_Type lc)
}
\qualifiers{
\qualifier{W}{width of resulting plot (default=12)}
\qualifier{H}{height of resulting plot (default=W)}
\qualifier{pdstart}{start of P-dot axis (default read from eppd)}
\qualifier{pdstop}{end of P-dot axis (default read from eppd)}
\qualifier{pstart}{start of P axis (default read from eppd)}
\qualifier{pstop}{end of P axis (default read from eppd)}
\qualifier{p0}{center of P  axis (default mean of periods in eppd)}
\qualifier{cmap}{[\code{="iceandfire"}] Color-map of Chi^2 landscape }
\qualifier{cont}{switch to plot FWHM contour on map }
\qualifier{pp}{switch to plot pulse profile below map }
\qualifier{t0}{[\code{="0"}] t0 for pulse profile }
\qualifier{nbins}{[\code{="12"}] number of phase bins for pulse profile }
}
           
\description
    This function plots the results for "epfoldpdot" into a nice
    colorful xfig image, with the cuts in P and P-dot over the most
    significant period as well as (if desired) the pulse profile of
    that period. This style of plot is inspired by the results plots
    of PRESTO.

    This code is still under development and not very flexible yet.
    Feel free to improve it.       

  \seealso{epfoldpdot}
\done

\function{xfig_plot_image}
\synopsis{Plots an image (with x/y-grid)}
\usage{Struct_Type xfig_plot_new( Double_Type[n,m] IMG, [Double/Struct_Type x, y] );}
\altusage{xfig_plot_new( Double_Type[n,m] IMG, [Double/Struct_Type x, y [, Struct_Type xf]] );}
\qualifiers{
\qualifier{cmap}{[="ds9b"] Either a name (String_Type) of a colormap
                     (see png_get_colormap_names), or a self defined
                      colormap (Integer_Type[]).}
\qualifier{gmin}{[=min(IMG)] Minimal value of image to be plotted}
\qualifier{gmax}{[=max(IMG)] Maximal value of image to be plotted}
\qualifier{xmin}{[=min(x)] Minimal value of x-axis}
\qualifier{xmax}{[=max(x)] Maximal value of x-axis}
\qualifier{ymin}{[=min(y)] Minimal value of y-axis}
\qualifier{ymax}{[=max(y)] Maximal value of y-axis}
\qualifier{dx}{[=0.] Relative justification of pixels in x-direction. Used if 'x' is no grid.}
\qualifier{dy}{[=dx] Relative justification of pixels in y-direction. Used if 'y' is no grid.}
\qualifier{fill}{[=20] global filling method (see %.shade_region) }
\qualifier{fmap}{[=Integer_Type[n,m]+fill] individual filling method for each pixel}
%\qualifier{nancol}{[="black"] Extra color for IMG pixel values, which are NAN.}
\qualifier{ratio}{[=1] Desired ratio of x and y-axis scale}
\qualifier{W}{[=14] Plot width [cm]. 'ratio' is overwritten if W & H are given!}
\qualifier{H}{[=W/ratio] Plot height [cm]. 'ratio' is overwritten if W & H are given!}
\qualifier{colorscale}{If given an inlay colorscale is plotted using xfig_plot_colorscale}
\qualifier{format}{Format for colormap axis ticlabels, see 'help xfig_plot.axis'}
\qualifier{just}{[=[.95,.05,-.5,.5]] Position (world00) & relative justification
         of the colorscale.}
\qualifier{scale}{[=[.33,.05]] : Relative size of the colorscale in respect to W & H.}
\qualifier{xlabel}{Label of x-axis}
\qualifier{ylabel}{Label of y-axis}
\qualifier{depth}{[=150] xfig depth}
}
\description
   Other than the %plot_png function of the xfig_plot structure this function
   aims to plot images with a (given) ARBITRARY x/y-grid in a correct way
   (NOTE that .plot_png does not care for x/y values!). This also allows to
   plot images with logarithmic scales without any problems! In addition
   it is easy to plot gabbed images by just giving the x and y grid as bin_lo
   and bin_hi.

   ARGUMENTS
    IMG:
    In any case a IMaGe has to be given, which can arbitrary dimensions n and m
    (it even can be 1-dimensional).

    x, y:
    x and y-grids are optional arguments but cannot be given individually.
    If no grid is given the grids are set to the pixel number assuming a linear
    grid! The grids can be Array_Types either of length n and m, respectively, in
    which case the bin_lo and bin_hi is created by assuming a linear grid.
    Otherwise their length must be n+1, m+1!
    It is also possible that x and y are Struct_Types already containing the bin_lo
    and bin_hi (it is only required that the bin_lo filed has 'lo' in its name and
    the bin_hi field 'hi', respectively). In that last case it is also possible to
    plot images with gabs!
    
    xf:
    Must be and xfig_plot structure. If given the image is plotted into xf and there
    will be no return value. This way all automatic plot settings of this function will
    be disregarded. Otherwise a new xfig_plot is created (qualifiers are
    passed through).
    
\example
   % Example 1: logscale
   variable x = [PI:10*PI:#100];
   variable y = [PI:5*PI:#80];
   variable IMG = sin(x) # transpose(cos(y));
   variable xf = xfig_plot_image( IMG, x*180/PI, y*180/PI;
                                         padx=0.025, pady=0.025,
      		                          dx=-.5,
                                         xlog, ylog,
				          fill=20
				        );
   xf.render("/tmp/test.pdf");

   % Example 2: image of Example 1 but with gaps
   variable tmp = [[1:30],[35:65],[75:115]];
   variable x = struct{ lo = tmp/11.5*PI, hi = (tmp+1)/11.5*PI };
   tmp = [[1:40],[50:90]];
   variable y = struct{ lo = tmp/18.*PI, hi = (tmp+1)/18.*PI };
   variable IMG = sin(x.lo) # transpose(cos(y.lo));
   variable xf = xfig_plot_image( IMG, x, y ; cmap="drywet");
   xf.render("/tmp/test.pdf");

\seealso{xfig_plot_new, %.shade_region, xfig_plot_colormap}

\done

\function{xfig_plot_ionization_structure}
\synopsis{plots the ionization structure from an abundance file produced by xstar}
\usage{plot_ionization_structure(String_Type abundace_file, String_Type[] elements, String_Type ouput_file)}
\qualifiers{
 \qualifier{xpow}{plot radius in units of 10^xpow (default: 10)}
 \qualifier{xaxis}{use 'logxi' or 'radius' to set the abscissa (default: radius)}
 \qualifier{temp}{add temperature panel}
 \qualifier{press}{add pressure panel}
 \qualifier{xee}{add electron fraction panel}
 \qualifier{dens}{add density panel}
 \qualifier{athresh}{only ionization states are plotted which exceed a certain threshold (default: 0.3)}
 \qualifier{ion_states}{string array of ionization states in roman numbers}
 \qualifier{width}{plot witdh}
 \qualifier{height}{plot height}

 All other qualifiers are passed to xfig_plot.plot()
}
\description
 This function produces and xfig plot of the ion abundance structure of
 an xstar calculation. The input parameters are the xstar abundance FITS
 file, a string array of the elements of interest and an output file name.

\examples

  xfig_plot_ionization_structure ("xout_abund1.fits", ["si", "FE", "Mg", "O"], "ion_abundance.pdf"; ion_states = ["iii", "v"], temp );

 plots the ion abundance of the ionization states III and V for the elements Silicon, Iron, Magnesium and Oxygen
 and the temperature as a function of radius.
\done

\function{xfig_plot_model}
\synopsis{plots the model which was loaded by "read_data_from_write_plot"}
\usage{XFig_Plot_Type pl = xfig_plot_model(XFig_Plot_Type pl, Struct_Type dat)}
\qualifiers{
\qualifier{color}{specify the color}
\qualifier{width}{line width}
\qualifier{depth}{depth in plot}
\qualifier{hplot}{make it a histogram plot}
\qualifier{pp}{model is plotted twice (pulse profile)}
}
\seealso{xfig_plot_data,read_data_from_write_plot,read_col,write_plot}
\done

\function{xfig_plot_params}
\synopsis{Returns a xfig-plot of the given parameter (value, limits, [conf, chi2])}
\usage{ Struct_Type xplot = xfig_plot_params( Struct_Type params1, params2, ... );}
\qualifiers{
\qualifier{tmpdir}{[='/tmp/'] Directory for temporar files}
\qualifier{size}{[=[12,9]]: Dimension in cm of the (main) plot}
\qualifier{space}{[=.05*size[0]]: Space between parameter columns.
                    Dimension in cm of the (main) plot}
\qualifier{xwidth}{[=.5]: Width of the parameter symbols (same units as x values).}
\qualifier{x}{Alternative values for the x-axis (e.g., time in MJD)}
\qualifier{xlabel}{Label for the alternative x-axis.}
\qualifier{connect}{Connects the individual parameter values with a line.}
\qualifier{conf}{Confidence level for the given parameters are plotted, if given.
                 conf must be a Struct_Type[] with fields: the 'index' of the
                 parameter and the corresponding confidence (absolut) limits
                 'conf_min' and 'conf_max'.}
\qualifier{steppar}{Steppar information (see 'steppar'). Either steppar filename
                      which is loaded with steppar_load or a List_Type with two
                      elemtents 1. stepparinformation and 2. the keys.}
\qualifier{chi2log}{Logarithmic scales for chi2 plots}
\qualifier{grid}{Enables gridlines on the chi2 plots}
\qualifier{chi2max}{Custom max value for chi2 range}
\qualifier{chi2min}{Custom min value for chi2 range}
\qualifier{cmap}{[='ds9b'] Colormap for the colorcoded chi2 landscapes}
\qualifier{ticmap}{[='rainbow'] Colormap for the ticlabels/chi2 landscapes}
}
\description
     This function creates and returns a xfig-plot showing a compact overview
     of the given parameter information.
     The function takes a arbitrary number of arguments, which however are
     expected to be all parameter struct arrays (see get_params)! For each
     argument/parameter-array a individual xfig-plot will be created, which
     in the end will be combined in an xfig_new_vbox_combound spaced by 'space'.

     The main plot shows the parameter values
     within its limits (y-axis). Its dimension are determined by the 'size'
     qualifier. By default the x-axis is either the parameter index or the
     number/dataset of the parameter, if the names of all given parameter are
     the same.
     The values are represented by horizontal lines, where their width
     can be specified by the 'xwidth' qualifier (measured in x units). If
     'xwidth' exceeds the minimal distant of adjacent x-values, 'xwidth' is set
     to that value.
     The 'x' qualifier allows to use custom x-values, e.g., the time at which
     the parameters are valid. The corresponding label can be set with 'xlabel'.
     If the 'x' qualifier is used, a second x-axis will be added to the plot,
     which will be used to plot the parameter information, i.e., the spacing
     is given be those 'x' values!
     It is possible to give additional information about the parameters. Firstly
     the 'conf' qualifier can be used to give confidence levels of the parameters.
     'conf' must be a Struct_Type[] with fields 'index' (index of the parameter),
     'conf_min' and 'conf_max' corresponding to the absolut values of the confidence
     levels. The 'index' field is needed to find the corresponding parameter! Only
     matches will be plotted!
     Further information about the chi2 landscape can be given with the 'steppar'
     qualifier, which expect filename(s) or filepattern(s) to load steppar
     information produced with 'steppar'. The files are loaded using the
     'steppar_load' function. Using 'steppar' qualifier creates a multiplot
     with two additional plots besides main plot (caution, the return xfig-plot
     will then be a xfit-multiplot!) showing the individual chi2 landscales
     of the parameters, if existent and a mean chi2 of those landscapes.
     With the 'chi2log' qualifier the chi2 landscapes are plotted logarithmic.
     'grid' will extent the tics to a grid. To set custom limits for the chi2 range
     use 'chi2min' and 'chi2max'! To easily visualize which chi2 landscape
     belongs to which parameter, the parameter x-labels are colorized
     accordingly using the colormap given by 'ticmap'.
     Furthermore, color-coded chi2 landscapes are added to the main plot using
     the colormap given with 'cmap'. The colormaps can be given by either a
     name (String_Type) of an existing colormap (png_get_colormap_names) or
     a colormap itself.
\seealso{steppar, steppar_load, png_get_colormap_names}

\done

\function{xfig_plot_res}
\synopsis{plots the residuals which were loaded by "read_data_from_write_plot"}
\usage{XFig_Plot_Type pl = xfig_plot_model(XFig_Plot_Type pl, Struct_Type dat)}
\qualifiers{
\qualifier{color}{specify the color}
\qualifier{width}{line width}
\qualifier{depth}{depth in plot}
\qualifier{chi}{specifies that the residuals are in chi}
\qualifier{ratio}{specifies that the residuals are a ratio}
\qualifier{pp}{residuals are plotted twice (pulse profile)}
}
\seealso{xfig_plot_data,read_data_from_write_plot,read_col,write_plot}
\done

\function{xfig_plot_sky_img}
\synopsis{plots a sky image, including an optional scale and/or grid}
\usage{Xfig_Plot_object pl = xfig_plot_sky_img(String_type fits_img);}
\description
    This function uses xfig to plot a sky image, with a given color
    scale ("hot" by default). With a separate qualifier, a scale can
    be switched on. The 
\qualifiers{
\qualifier{cmap}{["hot"] color map}
\qualifier{scale}{switch on the scale}
\qualifier{arcmin}{[=5] length of the scale in arcmin}
\qualifier{lin}{linear image scale instead of logarithmic}
\qualifier{func}{giva a reference to an arbitrary function to be applied to the image}
\qualifier{size}{[10,10] size of the image in cm}
\qualifier{ref_img}{[fits_img] if not given as direct argument, a reference FITS image has to be provided for the scale (filename!)}
\qualifier{grid}{draw a grid and corresponding labels}
\qualifier{step_ra}{[=1] step size of the RA-grid (given in min)}
\qualifier{step_ra}{[=5] step size of the Dec-grid (given in arcmin)}
\qualifier{grid_color}{[#BBBBBB] color of the grid}
\qualifier{scale_color}{[white] color of the scale}
}
\seealso{xfig_plot_new,fitswcs_get_img_wcs}
\done

\function{xfig_plot_spectrum}
\synopsis{Makes an array of slxfig objects from a set of plots saved with write_plot}
\usage{Struct_Type[] pl = xfig_plot_spectrum(Struct_Type plt[],[...])}
\description
   Generates an array of SLxfig plot objects from a given set of plot data
   structs (as generated by write_plot).

   Arguments should be structs as returned by write_plot(;data). The first
   argument should be the "best-fit" model - i.e., the spectrum and model you
   want to plot in the top pane. Subsequent arguments should be structs
   returned by write_plot(;data) for "intermediate" residuals (e.g. with other
   spectral features turned on or off).

   The function returns an array of SLxfig plot objects, which can then be
   rendered using, e.g., xfig_multiplot().render. The colors, symbols,
   linestyles, and other parameters can be adjusted using qualifiers (see
   below). Since the xfig objects themselves are returned, you can also modify
   them yourself after the fact (in order to, e.g., add labels or do more
   plotting).

   EXAMPLE: consider a scenario where we fit some model containing a Gaussian
   emission line (using an "egauss" component) to a dataset of three spectra.
   We want to create a plot that has a top pane (occupying 50% of the total
   plot area) showing the spectrum and best-fit model, a middle pane showing
   the residuals without the emission line, and a bottom panel showing the
   best-fit residuals. The three spectra will be plotted in black, red, and
   blue, with the model overplotted in green, purple, and orange, and the
   output will be a PDF called "spectrum.pdf".

   \code{
   plot_data({1,2,3};res=1);                 % plot data and best-fit model with residuals
   variable best = write_plot(;data);        % write plot to a structure
   set_par("egauss(1).area",0.0,1);          % Turn off the emission line
   () = fit_counts;                          % re-fit without the line
   plot_data({1,2,3};res=1);                 % plot data without the line
   variable noline = write_plot(;data);      % write the last plot to another structure
   pl = xfig_plot_spectrum(best,noline;      % call xfig_plot_spectrum to create the xfig objects
     colors=["black","red","blue"],          % data colors
     mcolors=["green","purple","orange"],    % model colors
     topPanelFrac=0.5);                      % top panel (data+model) occupies 50% of total plot area
   xfig_multiplot(pl).render("spectrum.pdf");% render the final plot as a PDF
   }

   More qualifiers (see below) are available to fine-tune how the plot
   appears. The colors, style, and size modifiers (dsym, symsize, mstyle,
   mscale, dataDepth, modelDepth) can be given as arrays, with one element for
   each spectrum plotted, or as single values, in which case each spectrum is
   plotted with the same symbol/style/color/size.

   By default, the x- and y-ranges for the resulting plot will depend on the
   energy (or wavelength) ranges which were _noticed_ when you called
   write_plot(). The simplest way to change the range of the resulting plot is
   to only notice the wavelength/energy range you want to plot with xnotice()
   or xnotice_en() - this will ensure that the y-axis and residuals scale
   properly with what is plotted. There are also "xrange" and "yrange"
   qualifiers to set the range of the top panel; however, the residual ranges
   are still determined based on the full range of data.

   Labels for the individual panels can be enabled via the "labels" qualifier.
   If labels is present, but not set (or if its first element is NULL), then
   the panels will be automatically labeled with letters starting from 'a'.
   The "firstPanelLabel" qualifier can be used to change the starting label;
   it should be a single letter. If the "labels" qualifier is not present at
   all, the panels will be unlabeled. The x/y positions of the labels can be
   set via the label_x and label_y qualifiers, which should be arrays of
   Double_Type values indicating the positions of the lower-right corners of
   the labels for each panel in 'world0' coordinates for that panel (see
   'xfig_plot--wcs'). By default these are [0.94,0.90] for the data+model
   panel and [0.94,0.78] for the residuals.

   The "gaps" qualifier lets you specify where the model should not be plotted
   (e.g., between 1.7 and 2.4 keV in the Suzaku-XIS). It should be given as a
   List_Type list of Array_Type arrays, one array for each spectrum plotted.
   The arrays should be a list of start and stop energies for the gaps (this
   means they need to have even length); for instruments with no gaps, pass
   \code{Double_Type[0]}.  E.g., for a Suzaku observation with XIS0, XIS1,
   XIS3, and the HXD/PIN where you don't want to plot the model between 1.6
   and 2.3 keV in the XIS spectra, you would pass
   \code{gaps={[1.6,2.3],[1.6,2.3],[1.6,2.3],Double_Type[0]}}.

   This script is mostly feature-complete, but please contact Paul Hemphill
   (pbh@space.mit.edu) regarding bugs or missing features (please do not
   report missing bugs; any bugs that are not present are missing
   intentionally).

\qualifiers{
\qualifier{topPanelFrac}{Double_Type, fraction of plot taken up by top panel, default 0.5}
\qualifier{height}{Double_Type, height of plot in cm, default 14.2}
\qualifier{width}{Double_Type, width of plot in cm, default 12.6}
\qualifier{colors}{String_Type[] or Integer_Type[], xfig colors for each spectrum plotted, by default uses get_sron_colors() to get a set of colors}
\qualifier{mcolors}{String_Type[] or Integer_Type[], xfig colors for each model plotted. By default each model is plotted with the same color as its associated data.}
\qualifier{dsym}{String_Type[] or Integer_Type[], xfig symbols for data for each spectrum plotted, defaults to "point"}
\qualifier{symsize}{Integer_Type[], size of symbols for each spectrum plotted. Can be a single value or an array (one element for each spectrum).}
\qualifier{mstyle}{Integer_Type[], xfig linestyle for overplotted best-fit model for each spectrum. mstyle = -1 plots the model as a histogram. As with symsize, can be single value or array. Default -1}
\qualifier{mwidth}{Integer_Type[], xfig line width for overplotted best-fit model. As with symsize, can be single value or array. Default 1.}
\qualifier{mscale}{Double_Type[], multiplicative scale for model (e.g., if you want to plot your model above your data points). Single value or array.}
\qualifier{dataDepth}{Integer_Type[], xfig depth for data points, either a single value or an array specifying the depth for each spectrum. Default 1}
\qualifier{modelDepth}{Integer_Type[], xfig depth for model lines. See dataDepth. Default 0 (this means models are plotted over data by default)}
\qualifier{ratio}{If present, residuals are ratio residuals, otherwise residuals are assumed to be some variant of data-model.}
\qualifier{no_residuals}{If set, no residuals are plotted.}
\qualifier{gaps}{List_Type, indicates where gaps in an instrument's spectrum mean the model should not be plotted.}
\qualifier{xrange}{Double_Type[2], range for x-axis (default autoscaled)}
\qualifier{yrange}{Double_Type[2], range for y-axis of top panel (default autoscaled)}
\qualifier{rrange}{List_Type[]. Should be a list of two-element arrays containing the minimum and maximum for the y-axis of each residual plot, in the order that they appear on the plot.}
\qualifier{xlog}{Axes are linear by default. If "xlog" is set, or if it is set to a nonzero number, the x-axis will be logarithmic. If "xlog" is not set, or is set to zero, the x-axis will be linear.}
\qualifier{ylog}{See xlog; sets log/linear scale for y-axis on top panel. Residual plots always have linear y-axes.}
\qualifier{unit}{String_Type, either "keV" (energy, default) or "A" (wavelength).}
\qualifier{xlabel}{String_Type, label for x-axis, default depends on units. If unit="keV", "Energy (keV)"; if unit="A", "Wavelength (\\AA)". Set to NULL for no label.}
\qualifier{ylabel}{String_Type, label for y-axis of spectrum, default depends on units. If unit="keV", "Counts s$^-1$ keV$^-1$"; if unit="A", "Counts s$^{-1}$ \\AA$^{-1}$".}
\qualifier{rlabel}{String_Type, label for y-axis of residuals. If the "ratio" qualifier is set, default is "Ratio", otherwise default is "$\\chi$".}
\qualifier{labels}{String_Type[], labels for individual panels.}
\qualifier{firstPanelLabel}{String_Type or Integer_Type, label for first panel (further panels will increment this value, so if you give 'b' the next panels will be 'c', 'd', etc).}
\qualifier{label_x}{Double_Type[], x-positions (in world0 coordinates) of panel labels.}
\qualifier{label_y}{Double_Type[], y-positions (in world0 coordinates) of panel labels.}
\qualifier{arrowY}{Double_Type[2], lower and upper device coordinates for arrows for bins extending off the screen, default [0.01,0.1].}
\qualifier{version}{Display version information and exit.}
}
\seealso{write_plot, xfig_multiplot, xfig_plot_unfold, xfig_plot--wcs}
\done

\function{xfig_plot_unfold}
\synopsis{tries to plot the complete plot loaded by "read_data_from_write_plot"}
\usage{XFig_Plot_Type pl = xfig_plot_unfold(Struct_Type dat)}
\qualifiers{
\qualifier{size}{[dx,dy,dr] size of the xfig-plot}
\qualifier{dcol}{color of the data}
\qualifier{mcol}{color of the model}
\qualifier{xrng}{[xmin,xmax]}
\qualifier{yrng}{[ymin,ymax]}
\qualifier{rrng}{[rmin,rmax]}
\qualifier{ranges}{[xmin,xmax,ymin,ymax,rmin,rmax]}
\qualifier{chi}{specifies that the residuals are chi}
\qualifier{ratio}{specifies that the residuals are ratio}
\qualifier{y_label}{set a y-label for the data/model plot}
\qualifier{keV2erg_fac}{if given, the y-label is given in units of
                          ergs/s/cm^2 x 10^keV2erg_fac }
\qualifier{pp}{data is plotted twice (pulse profile)}
}
\description
    With "read_data_from_write_plot", previously stored data can be
    loaded into a structure. This structure ("dat" in the example
    above) can now be plotted with "xfig_plot_unfold".

    The routines xfig_plot_data, xfig_plot_model, and
    xfig_plot_res do the single steps on its own.
\seealso{xfig_plot_data,xfig_plot_model,xfig_plot_res,read_data_from_write_plot,write_plot}
\done

\function{xfig_polarplot_new}
\synopsis{creates a xfig_plot with polar axis}
\usage{Struct_Type xfig_plot_new( Double_Type Size )}
\altusage{Struct_Type xfig_plot_new()}
\qualifiers{
\qualifier{S}{[=10] Size of quadratic plot in width and height [cm].
                      Is overwritten by argument size!}
\qualifier{min}{[=0] Minimal angle of polar plot [degree]}
\qualifier{max}{[=180] Maximal angle of polar plot [degree]}
\qualifier{origin}{[=90] Origin of angular axis measured from x-axis [degree]}
\qualifier{dir}{[=-1] Clockwise direction of polar plot axis (-1/1)}
\qualifier{ticlabels}{[=.6] If 0 ticlabels are turned of. If not 0 it is used
                       as relative justification}
\qualifier{ticlabelsize}{[="small"] Size of ticlabels}
\qualifier{ticlabelrotate}{[=1] Rotate polar ticlabels corresponding to their position (0/1)}
\qualifier{ticthickness}{[=2] Thickness of tics}
\qualifier{smallticinc}{[=2] Increment for small tics [degree]}
\qualifier{medticinc}{[=10] Increment for medium tics [degree]}
\qualifier{bigticinc}{[=20] Increment for big tics [degrees]}
\qualifier{aticlables}{[=[min:max:bigticinc]] Ticlabels of angular axis}
\qualifier{aticformat}{["$%d^\\circ$"] Format for the angular ticlabels}
\qualifier{rlabel}{[=NULL] Label for the radial axis}
\qualifier{rtics}{[=11] Number of radial tics}
\qualifier{rticlabel}{[ =[0:1:#rtics/2] ] Radial tic labels. The 0th rtic label will not be drawn!}
\qualifier{norticlabel}{If given, disabels radial tic labels.}
\qualifier{rticlabelrotate}{[=0] Rotate polar ticlabels corresponding to their position (0/1)}
\qualifier{padr}{[=0.05] Padding of radial axis}
\qualifier{grid}{If given grid lines are plotted}
\qualifier{gridcolor}{[="#BBBBBB"] Color of the grid lines}
\qualifier{debug}{If given the x-y-axis of the underlying xfig_plot are shown}
}
\description
   This function returns a xfig_plot_new structure but imprinted with polar axis.
   This way one can use the functionality of xfig_plot_new. The polar axis
   can be modified with the qualifiers above.

   NOTE that when using %.(h)plot still requries 'x' and 'y' values, not 'radius'
   and 'angle'. That means the user has to do the transformation manually accounting
   for the 'origin', 'direction' and 'minimal'/'maximal' values (see examples).

   TO BE IMPLEMENTED:
   * functionality to us 'radius' and 'angle' for plotting!

\example
   % Plot coordinate transformation
   variable min = 0,
            max = 180,
            dir = 1,
            org = 45;
   variable ang = [min:max:#100];
   variable rad = 0.5+0.5*sin(ang*PI/180);
   variable xf = xfig_polarplot_new(; min=min, max=max, origin=org, dir=dir, grid );
   xf.plot( rad*cos( dir*(ang+org)*PI/180 ),
            rad*sin( dir*(ang+org)*PI/180 ) ; color="red" );
   xf.render("/tmp/test.pdf");

\seealso{xfig_plot_new}

\done

\function{xnotice_atime}
\synopsis{notices a range of defined arrival times}
\usage{xnotice_atime(Integer_Type index[, Double_Type time_lo, Double_Type time_hi]);
 or xnotice_atime(Integer_Type index, Double_Type time);
 or xnotice_atime(Double_Type time);}
\description
    There are three different ways to notice the time
    range of arrival times defined in a dataset:
    1. Like 'xnotice' set the time range of dataset
       'index' explicitly from 'time_lo' to 'time_hi'.
    2. Find the border of the time range of dataset
       'index' nearest to the given time. Then set
       the found border to this time. The border of
       a possible adjacent dataset is changed respect-
       ively.
    3. Find that dataset, which time range has the
       nearest border to the given time and proceed
       like option two.
    If the dataset only is given, then the full time
    range is used.
\seealso{define_atime, xnotice}
\done

\function{xspec_to_isis}
\synopsis{Parse xspec parameter file to isis parameter file}
\usage{Integer_Type t = xspec_to_isis(String_Type xspec_par)}
\qualifiers{
\qualifier{set}{set loadad parameters}
\qualifier{save}{ [=String_Type filename] save parameters as isis parameter file
	if filename is not specified it is saved as the .xcm file but with .par suffix}
\qualifier{nonverbose}{suppress any output not produced by errors}
}
\description
	Parse an xspec parameter file \code{.xcm} to an isis parameter file.
	If desired parameters can be set after parsing.
	*** Warning:
	Xspec tying expressions are only supported in the form of
	\code{= p1}
	*** Warning:
	Convolution models are not supported!
	If tied parameters is out of range of the respective parameter
	the limits are set to fit and a warning is given.
\example
	isis> t=xspec_to_isis ("test.xcm"; save);
	Running Xspec.................
	parameters saved to test.par as:
	diskbb(1) + nthComp(2)
	 idx  param            tie-to  freeze         value         min         max
	  1  diskbb(1).norm        0     0          21.7378           0       1e+10  
	  2  diskbb(1).Tin         0     0         0.230239       0.001          10  keV
	  3  nthComp(2).norm       0     0       0.00019705           0       1e+10  
	  4  nthComp(2).Gamma      0     0           1.9752       1.001           5  
	  5  nthComp(2).kT_e       0     0          1.68497     1.68497        1000  keV
	  6  nthComp(2).kT_bb      2     0         0.230239       0.001          10  keV
	  7  nthComp(2).inp_type   0     1                1           0           1  0/1
	  8  nthComp(2).Redshift   0     1                0      -0.999          10  
\seealso{set_par, load_par}
\done

\function{XSTAR_read_pops}
\synopsis{reads the ionization balances from an XSTAR population file}
\usage{Struct_Type XSTAR_read_pops(String_Type filename)}
\qualifiers{
\qualifier{verbose}{}
\qualifier{Z}{array of Z values of elements to include}
\qualifier{ions}{array of ions to include (overrides Z qualifier)}
}
\done

\function{xyfit_fun}
\synopsis{define xy-function to fit data defined via \code{define_xydata}}
#c%{{{
\usage{xyfit_fun(String_Type function_expression);}
\description
    Setting up an xy-fit-function with \code{fitfun} with \code{xyfit_fun("fitfun");}
    is done by interpreting the string "fitfun" and creates a fit function that can
    be understood by the isis routines. The string is searched for known functions
    and special symbols. Every thing else is interpreted as a new fit parameter belonging
    to a generic function \code{F}.

    The known functions can either be a registered isis function with name \code{fun_xyfit}
    or an slang intrinsic (see examples). If a symbol starts with '#' it is replaced
    with the x-axis of the current data set.

    If the xy-fit-function describes the graph a function,
    it usually computes \code{@yref} in terms of \code{@xref} and the \code{par}-array.
    This is the only option for xy-data without \code{xerr}, see \code{define_xydata}.

    If the xy-fit-function describes a parameterized curve,
    it usually computes \code{(@xref, @yref)} in terms of \code{par}
    and probably further (constant) parameters passed to \code{fitfun_xyfit}
    as qualifiers, which have been defined by \code{set_xyfit_qualifier}

    If a function of the form \code{fitfun_xyfit} is loaded and a function
    \code{fitfun_xyfit_default} exists, this will be passed to
    \code{set_param_default_hook}.

    Importand Notes:
    The routine will first search for functions with the appendix _xyfit
    and afterwards for intrinsic functions. This means if a function \code{fun} exists
    and there is also a function \code{fun_xyfit} registered the routine will use the
    latter.
    
    Calling a parameterized function together with functions of the form f(x) = y or
    two parameterized functions redults in undefined behavior. Calling a parameterized
    alone behaves as expected.

    The internal structure of the routine requires it that a call to \code{list_par},
    \code{save_par} or \code{load_par} do not work as expected. Equivalent _xypar functions
    exist.
\examples
    xyfit_fun("linear"); % or
    xyfit_fun("linear(1)"); % sets up xyfit-function linear_xyfit
                             % in this form it is the xy-equivalent to fit_fun

    % example for defining a proper _xyfit function:
    % (example for y = x*a+b; as available already with: xyfit_fun("linear");)
    define another_linear_regression_xyfit ()
    {
        variable xref, yref, par;
        switch(_NARGS)
        { case 0: return ["a [unit_a]", "b [unit_b]"];}
        { case 3: (xref, yref, par) = (); }

        @yref = par[0] * @xref + par[1];
    }
    define another_linear_regression_xyfit_default(i)
    {
    switch(i)
        { case 0: return (1, 0, -10, 10); }
        { case 1: return (0, 0,  -5, 10); }
    }
    xyfit_fun("another_linear_regression");
    list_xypar;

    % example for specifying normalization parameters
    % (see 'norm_indexes' parameter of 'add_slang_function')
    define fun_with_norm_xyfit ()
    {
        variable xref, yref, par;
        switch(_NARGS)
        { case 0: return struct { pars = ["a [unit_a]", "b [unit_b]"], norm = [0] }; }
        { case 3: (xref, yref, par) = (); }

        @yref = par[0] * (@xref ^ par[1]);
    }
    xyfit_fun("fun_with_norm");
    
    % example for a simple function call
    xyfit_fun("Norm*sin(#x)/exp(#x-xoff)"); % results in fit-function of the form
                                             % Norm*sin(x)/exp(x-xoff)
                                             % with two parameters: Norm, xoff

    % example of a combination of the previous cases
    xyfit_fun("linear(1)+tan(#x^2-xoff)/linear(2)");
    list_xypar; % output:
                %   linear(1)+tan(#x^2-xoff)/linear(2)
                %   idx  param    tie-to  freeze         value         min         max
                %   1  linear(1).a   0     0                1     -100000      100000  coefficient of x
                %   2  linear(1).b   0     0                0     -100000      100000  additive constant
                %   3  linear(2).a   0     0                1     -100000      100000  coefficient of x
                %   4  linear(2).b   0     0                0     -100000      100000  additive constant
                %   5  F(1).xoff     0     0                0           0           0
\seealso{define_xydata, set_xyfit_qualifier, list_xypar, save_xypar, load_xypar, plot_xyfit}
\done

\function{xyfit_residuals}
\synopsis{calculates the difference between xy-data and xy-model}
#c%{{{
\usage{Double_Type res[] = xyfit_residuals(Integer_Type data_id);}
\description
    The residuals \code{res[i]} are determined differently for xy-data with
    uncertainties in y only or in both dimensions.
    
    If the xy-data have no x-uncertainties \code{xerr}:\n
       \code{res[i] = (data.y[i] - model.y[i]) / data.yerr[i];}
       
    If the xy-data have uncertainties \code{xerr} as well:\n
       \code{res[i] = min( relative_distance_from_curve[i] );}\n
    where \code{relative_distance_from_curve[i]} is composed of 
       \code{(model.x - data.x[i]) / data.xerr[i]} and
       \code{(model.y - data.y[i]) / data.yerr[i]}.
\seealso{define_xydata, xyfit_fun}
\done

\function{xylabel_in_box}
\synopsis{places a text label at a relative position in the plot box}
\usage{xylabel_in_box(Double_Type x_rel, y_rel, String_Type label[, angle[, justify]]);}
\description
    \code{xylabel_in_box} uses relative coordinates \code{x_rel} and \code{y_rel}:
    the plot box corresponds to \code{0<=x_rel<=1} and \code{0<=y_rel<=1},
    but the \code{label} can also be placed outside of this area.
    In order to calculate the world coordinates (with \code{coords_in_box}),
    the \code{x}- and \code{yrange} has to be set before the first plot command.

    The world coordinates and the optional parameters are passed to
    \code{xylabel}, so see \code{xylabel} for a description of \code{angle} and \code{justify}.
\seealso{coords_in_box, xylabel}
\done

\function{y1axis}
\synopsis{(re)draws a first y-axis}
\usage{y1axis(Double_Type min_value, Double_Type max_value[, Double_Type step]);}
\qualifiers{
\qualifier{nsub}{[=0] number of minor tick marks within each major divison (nsub=0 => no minor ticks)}
\qualifier{maji}{[=0.5] length of major tick marks, drawn inwards (in units of the character height)}
\qualifier{majo}{[=0] length of major tick marks, drawn outwards (in units of the character height)}
\qualifier{fmin}{[=0.5] length of minor tick marks (as fraction of the major tick marks)}
\qualifier{disp}{[=0.5] displacement of baseline of tick labels to the axis (in units of the character height)}
\qualifier{angle}{[=0] orientation of the text (in degrees)}
\qualifier{color}{[=1] color of the axis}
\qualifier{linewidth}{[=1] line width of the axis}
\qualifier{opt}{[="N"/"LN"] pgaxis-options: "L" (log), "N" (numbers), "1" (force decimal), "2" (force EE)}
\qualifier{xrel}{[=0] relative position of the y1axis (0=left, 1=right)}
\qualifier{ymin}{[=ymin] absolute y-position of the axis}
\qualifier{ymax}{[=ymax] absolute y-position of the axis}
}
\seealso{x2axis (and references therein)}
\done

\function{y2axis}
\synopsis{draws a second y-axis}
\usage{y2axis(Double_Type min_value, Double_Type max_value[, Double_Type step]);}
\qualifiers{
\qualifier{nsub}{[=0] number of minor tick marks within each major divison (nsub=0 => no minor ticks)}
\qualifier{maji}{[=0.5] length of major tick marks, drawn inwards (in units of the character height)}
\qualifier{majo}{[=0] length of major tick marks, drawn outwards (in units of the character height)}
\qualifier{fmin}{[=0.5] length of minor tick marks (as fraction of the major tick marks)}
\qualifier{disp}{[=0.5] displacement of baseline of tick labels to the axis (in units of the character height)}
\qualifier{angle}{[=0] orientation of the text (in degrees)}
\qualifier{color}{[=1] color of the axis}
\qualifier{linewidth}{[=1] line width of the axis}
\qualifier{opt}{[="N"/"LN"] pgaxis-options: "L" (log), "N" (numbers), "1" (force decimal), "2" (force EE)}
\qualifier{xrel}{[=1] relative position of the y2axis (0=left, 1=right)}
\qualifier{ymin}{[=ymin] absolute y-position of the axis}
\qualifier{ymax}{[=ymax] absolute y-position of the axis}
}
\seealso{x2axis (and references therein)}
\done

\function{y2label}
\synopsis{labels the second y-axis}
\usage{y2label(String_Type s);}
\qualifiers{
\qualifier{color}{[=1] textcolor}
\qualifier{f}{[=0.03] fraction of the plot-area's width that the label is to the right}
}
\seealso{xylabel}
\done

\function{yearOfMJD}
\synopsis{transforms a date given as MJD into the year (with fractional parts)}
\usage{Double_Type year = yearOfMJD(Double_Type MJD);}
\seealso{jd2year}
\done

\function{z2fold}
\synopsis{peforms Z^2_m search on a lightcurve or event data in a given period
interval}
\usage{(Struct_Type res) = z2fold(Double_Type t, r, pstart, pstop);
or (Struct_Type res) = z2fold(Double_Type t, pstart, pstop) ; (event data)}

\qualifiers{
\qualifier{sampling} {how many periods per peak to use (default=10)}
\qualifier{nsrch} {how many periods to search in a linear grid (default not set)}
\qualifier{dp} {delta period of linear period grid (default  not set)}
\qualifier{m} {number of harmonics used (default =2)}
\qualifier{chatty} {set the verbosity, (default=0)}
\qualifier{gti}{GTIs for event data, given as struct{start=Double_Type, stop=Double_Type}} 
}

\description
   Performs Z^2_m test  on a given lightcurve or event data between the periods
   pstart and pstop. The function is based on epfold.sl.
   GTI correction is implemented only  for event-data.

   By default, periods are sampled according to the triangular rule
   for estimating the period error, using "sampling" periods per peak.
   If a linear grid is to be used, either "dp" for a given distance
   between two consecutive periods or "nsrch" for a given number of
   periods can be given. These qualifiers are mutually exclusive. 

   The returned structure "res" contains four fields: "p" for the
   evaluated period and "stat" for the value of the statistic used.

   Please see Buccheri et al., 1983, Astron. Astrophys. 128, 245 for
   more information on the Z-square statistics. 
   
\seealso{zsquare, epfold, pfold}   
\done

\function{zams}
\synopsis{return properties of the ZAMS per Tout et al., 1996, MNRAS 281, 257}
\usage{(radius,luminosity,temperature)=zams(mass;z=metallicity);}
\qualifiers{
    \qualifier{z}{metallicity (between 0.0001 and 0.03; default: 0.02 (solar))}
}
\description
  This function uses the rational function fits of Tout et al. (1996, MNRAS 281, 257)
  and calculates the radius, luminosity, and temperature for zero age main sequence stars
  of a given mass (in solar units;  can be an array, must be between 0.1 and 100 Msun).
  The radius and luminosity are returned in solar units, the temperature is in K.
  Tout et al. state that in general the fits are better than 7.5per cent in luminosity and
  5 per cent in radius, for solar metallicity they are good to 3 per cent in L and
  1.2 per cent in R.
\done

\function{zang}
\synopsis{Determine the angular size of an object as a function of redshift}
\usage{Double_Type angsize = zang (Double_Type size, Double_Type redshift);}
\description
    Requires an input size in kpc and returns an angular size
    in arc seconds. (Analog to the function \code{zsize},
    which converts angular size to projected size size given the
    redshift.)
    The default cosmology according is used. The function passes
    qualifiers to the functions \code{lumdist} and \code{cosmo_param} allowing
    to change the cosmology.
\example
    variable size = 50.; % kpc
    variable    z = 1.5; % redshift
    zang(50,1.5; omega_m = 0.3, omega_lambda = 0.0, silent);
    % ---> 6.58 arc seconds
\seealso{lumdist, cosmo_param}
\done

\function{zpc_tbnew_simple (fit-function)}
\synopsis{partial covering absorption}
\description
    This function describes two partial coverers with
    covering fraction f, according to the formula:
    \code{ (1-f) +  f*exp(-nH*sigma) }
    This definition is equal to how, e.g., zpcfabs is defined.
\seealso{tbnew_simple,tbnew,zpcfabs}
\done

\function{zsize}
\synopsis{Determine the projected size of an object as a function of redshift}
\usage{Double_Type proj_size = zsize (Double_Type angle, Double_Type redshift);}
\description
    Requires an input angular size in arc seconds and returns
    the projected size in kpc. (Analog to the function \code{zang},
    which converts projected size to angular size given the
    redshift.)
    The default cosmology according is used. The function passes
    qualifiers to the functions \code{lumdist} and \code{cosmo_param} allowing
    to change the cosmology.
\example
    variable angsize = 1.; % arc seconds
    variable    z = 1.5; % redshift
    zsize(angsize,z);
    % ---> 8.46 kpc
\seealso{zang, lumdist, cosmo_param}
\done

\function{zsquare}
\synopsis{calculate Z^2 statistics for pulse period search}
\usage{Double_Type Z2 = zsquare(Double_Type times, Double_Type testperiod}

\description
    Calculates the Z^2 statistics of an event list or light-curve for
    a given period. By itself not very useful, but is used in z2fold
    to search for periodicities using the Z^2 statistics.

\qualifiers{
\qualifier{lc}{=Double_Type rate; allows the use of a lightcurve instead of event files}
\qualifier{m}{= Integer_Type; determines the highest order of summations (default 2)}
}
\done

